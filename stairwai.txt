Work programme topic: H2020-ICT-49-2020: Artificial Intelligence on demand platform
Innovation Action
Project Number: 101017142
Project Acronym: StairwAI
Project title: Stairwai to AI: Ease the Engagement of Low-Tech users to the AI-on-Demand platform through AI
Duration in months: 36
Start date: January 1, 2021
End date: December 31, 2023
Call identifier: H2020-ICT-2018-20 
Topic: ICT-49-2020
Total cost: € 5 384 853,75
EU contribution: € 5 116 631,25
Coordinated by: ALMA MATER STUDIORUM - UNIVERSITA DI BOLOGNA
Fixed EC keywords: Artificial intelligence, intelligent systems, multi agent systems
Free keywords: Natural multi-language processing, matchmaking, hardware dimensioning and provisioning, Low-tech SMEs

Abstract
The most fundamental need of the AI4EU on-demand Platform at this stage in its development is the introduction of improved functionality to allow for the easy engagements of its core targets stakeholders, namely SMEs. While additional resources and strong domain-specific solutions are also needed, it will be a wasted enterprise without a simple mechanism to match users to these assets. What is needed is a Stairway to AI, a linking bridge between users in a low-tech level to the higher-level AI resources that have the potential to transform both their business.

The StairwAI project targets low-tech users with the goal of facilitating their engagement on the AI on-demand Platform. This will be achieved through a new service layer enriching the functionalities of the on-demand platform and containing: (1) a multi-lingual interaction layer enabling conversations with the Platform in the user’s own language, (2) a horizontal matchmaking service for the automatic discovery of AI assets (tools, data sets, AI experts, consultants, papers, courses etc.) meeting the user business needs and, (3) a vertical matchmaking service that will dimension and provision hardware resources through a proper hardware provider (HPC, Cloud and Edge infrastructures).

These services are designed and implemented by using techniques in an AI for AI fashion. The AI techniques deployed in the development of the services are natural language processing for the multi-lingual interaction, constraint solving, optimization and machine learning for horizontal and vertical matchmaking, knowledge representation for organizing the platform AI assets, reputation and fairness mechanisms to improve the matching results.
StairwAI will have a tremendous impact on the sustainability, collaboration opportunities, accessibility and fairness of the AI on-demand Platform, enabling the definition of proper business models for the uptake of AI bringing new value for EU industry.

1.1 List of beneficiaries
ALMA MATER STUDIORUM – Università di Bologna
University College Cork
Universitat Politècnica de Catalunya
Huawei Technologies Duesseldorf GmbH
Technische Universiteit Eindhoven
Bonseyes Community Association 
Fundingbox Accelerator SP ZOO
Istituto Nazionale di Fisica Nucleare
Tilde SIA
Stichting EGI
Thales

WP1: Project management
The objective of the WP1 is the smooth implementation of the project, and in particular:
* To coordinate and supervise activities to be carried out
* To carry out the overall administrative and financial management of the project
* To manage the contract with the European Commission and the Consortium Agreement
* To manage contacts with the European Commission
* To monitor quality and timing of project deliverables
* To assess potential risks and mitigate their impact
* To establish effective communication procedures
* To ensure Trustworthy AI components

UNIBO as the Coordinator of the project, will direct the project management, dealing with the general management of activities, to ensure the straight development of the project and effective communication both with the EC and between the members of the Consortium. UNIBO will be the reference contact for the European Commission, having meetings with EC officer when required, and reporting him/her the technical and financial progress of the project. UNIBO will be assisted in the Project Management by all partners, based on their specific field of expertise, always keeping the leadership of the WP.

Task 1.1: Technical Management and trustworthy AI 
UNIBO will be in charge of the general management of the StairwAI project and will autonomously lead the technical and industrial management of the project. 
The specific tasks performed in this phase will concern:
* monitoring of quality, timing and results of the general development of the project and of the specific WPs identified;
* management of communication within and between the WPs;
* overall coordination and supervision of all activities;
* conflict resolution and coordination of the partners concerning industrial relations and the complementary deployment of skills and capabilities;

* reporting activities, at regular times, estimating the impact and the efficiency of each activity;
* periodic control of the quality of project deliverables;
* analysis of the potential risks associated to the project and elaboration of contingency plans to guarantee the smooth development of the project even under adverse conditions; BCA will provide the members with contingency plans based on a selection of potential risks previously identified.
* ensuring all AI components are design and implemented following the Trustworthy AI Guidelines. 

Task 1.2: Administrative and Financial Management 
UNIBO will take care of the overall financial-related aspects of the project, taking care personally of the distribution of EC funds among all partners of the Consortium, as well as the collection and check of financial statements and related information (when needed, certificates on financial statements) to be included in the management report. UNIBO will monitor the use of resources from all partners and perform financial accounting. 

UNIBO will take final decisions on all financial matters concerning
* the management of funds and the allocation of resources among participants;
* the management of contacts with EU Commission: organization of the activity reports to be delivered to the European Commission, including the collection of contributions from the partners;
* implementation of EC contract and Consortium Agreement;
* general progress of work and WPs related activities;

* definition and management of meeting sessions (kick off meeting at month 1, at least one general meeting per year plus technical meetings when required), with all the related tasks: agendas, invitations and action lists;
* overall legal and contractual management, including the preparation and maintenance of the Consortium Agreement, and the maintenance of the EC Contract (e.g. amendments). Support in all tasks related to legal and contractual issues.  

Before the start of the project partners will sign a Consortium Agreement to set specific rules about IPR handling and property issues, both related to preexistent and newly created knowledge. The Consortium Agreement will define rules on IPR issues throughout the whole project’s life. 

Task 1.3: Data management 
The StairwAI consortium while not participating to the open access research data pilot will craft a data management strategy. See WP8 for details on open access of scientific publications. The Data management plan is conceived to support the data management life cycle for all data that will be collected, processed or generated by the project. More specifically, the DMP will detail what data the project will generate, whether and how it will be exploited or made accessible for verification and re-use, and how it will be curated and preserved. The DMP will be provided at M6 of the project. 

Task 1.4 Financial Management of FSTP 
The consortium has reserved €2M for FSTP which has been allocated to UNIBO, as project Coordinator. FBA, expert in managing FSTP, will support UNIBO in the operative process for managing FSTP. The funds will be given to the FSTP Sub-grantees as a Fixed Lump Sum. This task includes the necessary coordination procedures in order to guarantee that the FSTP-subgrantees receive timely payments according with milestone established in FSTP Agreement.
Task 1.5 Innovation knowledge transfer and IP management 
This activity will be focused on knowledge transfer and IP protection in line with the Consortium and Grant Agreements. The project Executive Board shall discuss all matters related to IP Rights in collaboration with the IP and innovation manager.

 The Innovation Manager will liaise with partners concerning publications and patents, and will be in charge of archiving documents, discerning results generated during the course of the project, identifying their potential use and a proposed protection policy.. The Innovation Manager must reply within 2 weeks of a request. This task will cover also the monitoring of foreground and IPR issues; and informing the partners of potential IP strategies.

WP2: Technical Requirements, service layer design and integration with the Platform
Objectives of WP2 are mainly devoted to the collection of requirements for the services that will be developed in StairwAI namely the multi-lingual interaction, the vertical and horizontal matchmaking layers. Following the requirements, the design of the service layer along with its integration in the AI-on-demand platform will be performed. It is essential for the requirement collection to interact with low-tech companies through the DIH (involved in the project by task 8.2) and with hardware resource providers.

The requirement collection in Task 2.1-2.4 will start from M1, the first set of requirements aims to be delivered by M6, and updated/refined in M18 to be aligned with the open call activities in WP7. With the support from WP8, we will reach out to AI researchers and low-tech companies. Qualitative interviews and an EU-wide questionnaire will be used to elicit current and future user needs. 

Requirements tracking tools will be used, and interaction with the technical team will ensure the requirements are taken into account in the design and implementation of the services. Co-design workshops will be organised, to define the suitable service setup for the selected pilots. Starting from the requirements Task 2.4 will design the service layer and interface interactions, and its integration in the platform will be performed in Task 2.5. The service implementation will be performed in WP3-6.

Task 2.1 Requirements for the horizontal matchmaking layer and reputation mechanisms
This task focuses on collecting requirements for horizontal matching. The requirement information will serve as input for the service development in WP5. In particular, we need to collect the requirements for the three matchmaking services that are developed in tasks 5.1, 5.2 and 5.3 (lead by UNIBO), for the reputation and fairness mechanisms regulating the matchmaking developed in task 5.4 (lead by UNIBO) and also for the collaborative AI tools for matchmaking developed in task 5.5. This will be developed by interviewing mainly low-tech users, SMEs and companies researchers and practitioners for putting the basis for a usable and intuitive matchmaking service linking user needs with AI-on-demand platform resources. 
Task 2.2 Requirements for the vertical matchmaking layer      

This task interacts with StariwAI hardware resource providers including, edge diverse, cloud resources, HPC infrastructure, in order to develop an understanding of used standards, interface protocols, pay models, deployment mechanism, performance, QoS, any concerned issues and providers’ interests in supporting AI experts. The analysis will be delivered as input to WP6 to serve as the base for design and develop the vertical matchmaking and hardware marketplace. 

This task includes the definition of architecture/software requirements for hardware providers to provide a heterogeneous computing lab to provide resources “on-demand”. EGI.eu will lead this task as it coordinates HTC and cloud resources provisions at Europe level and beyond. The current resource federation across over 300 computer centers, with more than 1 million CPU cores, 1,000 PB storage, is one of the world's biggest infrastructure of such kind. BCA, INFN and HUA will complement with Edge and HPC resources.

Task 2.3 Integration requirements coming from the AI-on-demand 

This task will gather detailed technical information from AI4EU about interface API, protocols, data schema, etc., that are used for Frontend UI, Process & Content and PaaS layers. The requirement information will be the inputs for the service development in WP6, to ensure the seamless integration and interoperability. The task is led by THA, who is the coordinator of AI4EU and be complemented by EGI, UNIBO and BCA for the integration of the services. This can guarantee communication and keep StairwAI well informed about any updates from the AI4EU development.     
Task 2.4 Design of the service layer 

This task will focus on the design and development from a software architecture point of view, of the different parts of the Stairway service and its interaction. It will be used tools such as UML – Unified modelling language – to represent and diagram the user interactions, interfaces to ease the integration. WP leaders (UNIBO, UPC, BCA, TIL, EGI) will be responsible of incorporating the design of their corresponding blocks. This design will be modular and with clear defined APIs and technologies so it is possible to expand/modify its architecture in the future with new services and technologies, making the StairwAI sustainable by its own. 
Task 2.5 Integration with the AI-on-demand platform 

The objective of this task is to ensure alignment between its effort and the AI4EU roadmap. This activity will be drawing from the outcomes of the Working Group on ‘Interoperability and External Partners’ set up by AI4EU project, in which BCA participates. Thus, BCA will lead this task complemented by THA. The key expected collaboration points that have been discussed so far and will be on the roadmap for StairwAI include: 
? Identify services to be integrated and required integration activities with the seven AI4EU layers 
? Review integration options, from non-integrated service access and exchange to smart integration including security layer, front-end UI, PaaS and infrastructure layer;
? Develop an interoperability architecture based on proven interoperability levels;
? Analysing possible further strong integration / merger between the two platforms AI4EU, the Bonseyes marketplace, and the new StairwAI services.

On a technical side, the partners UNIBO, EGI, TIL, TUE will implement the integration using the technologies identified in order to expose the StaiwAI services layer to the AI4EU community.

Within this task, as the architecture of the target AIoD platform is being refined and the related technologies have been identified, the project will implement the following activities with partner BCA, in close coordination with the WP6 activities (Vertical Matchmaking):
* Transform the current tooling into Benchmark as a service (BaaS)
- Adapt Bonseyes benchmark tool to be able to incorporate external AI Models (any)
- Transform Bonseyes benchmark workflow into BaaS (containing LPDNN)
- Add communication protocols (GUI/Server need to talk to the BaaS)

* Integrate the BaaS services into AI-on-demand
- Setup of communication protocols (Call Bonseyes API)
- Setup of BaaS container into Stairwai platform
- Make BaaA available through Stairwai interface to AI-on-demand 
- Post benchmarks on AI-on-demand

WP3: Platform knowledge and community organisation
The AI-assets on the AI-on-demand platform should be clustered and organized in an ontology of AI techniques that covers all areas of AI. The definition of such an ontology as a sort of knowledge graph is the main objective of WP3. Each class of the ontology will then link AI tools, documents and papers, courses, data-sets and benchmarks. Similarly, the AI community on the platform should be as well linked to the classes so that experts, IT companies and consultants can expose their skills.

Also, the WP will build the data sets needed to train the services will classify the user’s need with the target ontology categories that will be developed in WP5 and the data sets needed to train the algorithm to hardware matching that will be trained in WP6.

Task 3.1 - Knowledge representation design 
This task will focus on investigating and identifying AI-related vocabularies, ontologies and other potential knowledge representations towards the definition of a dynamic and interoperable AI Asset Management System that deals with different AI4EU assets (libraries, models,  tools, datasets, hardware, experts, academic resources, etc).  This task will take as starting point the ontology created in AI4EU and extend it to cover the wider array of assets targeted in StairwAI by UPC with the support of UNIBO that will lead the horizontal matchmaking. BCA that will lead the vertical matchmaking and TIL that will lead the multi-lingual NLP interaction. Result of this task is reported in the first version of deliverable D3.1. 

Task 3.2 - AI4EU knowledge representation community 
This task will focus on the support of the data and knowledge representation for the AI on demand platform, synchronising the effort with the community of projects willing to cooperate with the AI_on_demand platform. The task will organize a series of meetings with representatives of other ICT48, ICT-49 projects to explore the best ways to support the generation, publication and exploitation of their produced assets by means of a shared ontology, a set of aligned ontologies or other similar knowledge representation. 

This effort will be also synchronised with the community engagement activities in Task 8.1 to build over the existing AI4EU structures if adequate, and following an integrative approach with other ontologies, knowledge models for the different assets managed by the platform (libraries, tools, algorithms, professionals, academic resources, hardware and cloud services, job positions, experts, etc). 
The outcomes of the collaborative approach will be reported in deliverable D3.5. Furthermore we will explore the production of a whitepaper on decisions taken and options analysed available to the community through the AI on demand platform for future extensions if required. 

Task 3.3 - Data collection and datasets generation for Multi-lingual NLP 
This task will focus on supporting the creation of new datasets, by collecting data from different sources:
* Data for user stories and dialogue scenarios reflecting content of the eventual conversations with the end users of the solution e.g. topical questions, themes, issues etc.
* Data for development and domain adaptation of the multilingual Natural Language Understanding (NLU) modules and other Natural Language Processing (NLP) tools e.g. specific vocabularies, terminology, named entities etc.
* 3 Parallel data and respective terminology for development and domain adaptation of the Machine Translation (MT) engines for the respective language combinations.

An initial version of these datasets will be collected by TIL and reported in deliverable D3.6 and they will be used to train models in WP4 and will impact the knowledge modelling in Task 3.6 thus the participation of UPC. The final version of all data collected for the multilingual NPL will be reported in Deliverable D3.8

Task 3.4 - Data collection and datasets generation for Horizontal and Vertical Matchmaking 
This task is devoted to the generation and collection of (real or realistic) data sets of AI assets (tools, data sets, experts, papers, courses) and AI stakeholders (researchers, developers, domain experts, citizens, large companies, SMEs, public bodies). 

This data will be extracted not only from the AI4EU platform (thanks to UCC) but from other sources such as job offers (possibly with a template form on the AI-on-demand platform), paper repositories. Also, profiling of algorithms on different hardware platforms will create a dataset for the vertical matchmaking. 
An initial collection of datasets (reported in deliverable 3.7) will be used to train the machine learning model for the matchmaking in WP5, and WP6 by UNIBO and this dataset collection may also impact the knowledge modelling refinement in Task 3.6, thus the participation of UPC. Deliverable 3.9 Will report final version of all the data collected for the Matchmaking components. 

Task 3.5 - Data Management of data produced in the Open Calls 
This task is devoted to the collection of the data produced by the applicants in the open calls (Task 7.2). This data will be collected in deliverable D3.10, D3.11 and D3.12 by FBA and it will be then used in Task 3.6 to refine the knowledge models, thus the participation of UPC. The collected data might be used also to enhance the models and components in WP4, WP5 and WP6.  

Task 3.6 - Knowledge creation and refinement 
All the data collected in Task 3.3, 3.4 and 3.5 will be used to create, extend and refine the StairwAI AI Asset Management System based on the data produced in Tasks 3.3, 3.4 and 3.5. The results of this work will be deliverables D3.3 and D3.4. This task will also update  the knowledge design in deliverable D3.2.

WP4: Multi-lingual interaction with the platform
To ease the access to AI assets and people, the platform should enable users interacting with the platform using chat bots speaking user’s native language. To do so, a service implementing natural language processing is needed to extract from the interactions (in form of use cases, job offers, profile description) the users needs. 

Task 4.1 – User interaction design and management 
This task will cover development of dialogue flow and scenarios for selected use-cases (developed in WP5 by UNIBO and UCC) as well as integration with AI services platform and representation of the information extracted from the respective knowledge base(s) (developed in WP3 by UPC) to the end users as part of the dialogue scenarios. We will also develop user interface and chatbot visuals as part of this task.

Task 4.2 – Multilingual interaction management 
This task will be jointly performed by TIL and UPC that will collaborate in the development of the following actions:
1. Researching the best methods and tools for enabling multi-lingual interaction with the platform.
2. Development and domain adaptation of the NLU (i.e. intent detection, Named Entity Recognition - NER) and other NLP tools (i.e. key-word extraction, content generation etc.), in line with the selected use-cases developed in WP5.
3. Development and domain adaptation of the MT engines for the selected languages.
4. Development of the multi-lingual user interface.

Task 4.3 – Development of the multi-lingual virtual assistant 
This task led by TIL will cover development and testing of the multi-lingual virtual assistant for the selected use-cases and scenarios as well as evaluation and elaboration of different elements of the solution developed as part of Task 2 and Task 3. For this task, Thales will develop a multilingual Q/A system which will enable the user to request information from within a text document. After the virtual assistant system has identified one or several documents of interest, the user can use this Q/A system by asking any question to find specific information of his or her choice within this document.

Task 4.4 – Validation of the NLP on mobile devices 
This task will compile a variety of AI neural network, such as convolution, pooling, activation, and fully connected layers, into dedicated AI instruction sequences for the HUA NPU/CPU/GPU in an offline setting, with data and weight rearrangement for optimized performance. HUA will support the creation of NLP inside terminal devices and MT engines will be combined to accelerate low-tech users input requests running on mobile devices. For this task TIL will support as it is responsible for the multi-lingual virtual assistant.

WP5: Horizontal Matchmaking, reputation and fairness
WP5 is intended to produce the horizontal matchmaking services that start from the users need and map them on the AI assets needed. Use case to assets, job offers to experts and consultants, people to courses matchmaking will be provided. Fairness for the matching should be taken into account. In addition, reputation mechanism and recommendation system proactively recommending matchings will be developed. 

Task 5.1 – Matchmaking between use cases and AI assets 
Starting from the requirements identified in WP2 and data collected in WP3, this task will design and implement different matchmaking algorithms that map use cases (already processed in WP4 to extract user needs) and AI assets namely AI experts and developers, consultants, tools, data sets, courses and papers. Starting from data collected in Task 3.4, we will experiment a pure ML approach developed by TUE that performs classification to the classes of the ontology defined in WP3, and an optimization approach developed by UNIBO and UCC that uses ranks extracted from an ML approach and performs optimal matching. 

Task 5.2 – Matchmaking between job offers and experts/consultants 
In this task we will design and implement different matchmaking algorithms that map job offers (inserted in the platform in natural language following a template form) and people belonging to the European AI community that are registered on the AI-on-demand platform. As for task 5.2, starting from data collected in Task 3.4, we will experiment a pure ML approach, developed by UNIBO that performs classification to the experts profiled in WP3, and an optimization approach developed by UCC that uses ranks extracted from an ML approach and performs optimal matching. 

Task 5.3 – Matchmaking between requests and papers/courses 
In this task, particularly designed to cope with researchers need (coming from the AI4EU platform and mapped by UCC), we will design and implement different matchmaking algorithms (UNIBO) that map requests (inserted in the platform in natural language following a template form) and education sources, meaning papers and courses on specific topics. Again, this task will rely on machine learning and will use data collected in Task 3.4.

 Task 5.4 – Mechanisms for reputation and fairness 
The matchmaking that will be implemented in a first version without relying on reputation mechanisms and fairness, will be extended to cope with both aspects. For this reason, the feedback coming from users of the matchmaking service will be elaborated to rank AI assets, people and educational material by BCA. The rank will be used to improve the matchmaking algorithm and make it more robust and effective. In addition, fairness mechanisms developed by UNIBO should be taken into account to improve the ML and optimization algorithms to avoid bias in the matchmaking. 

Task 5.5 – Collaborative AI tools for matchmaking 
The matchmaking implemented as a first version is a one-shot recommender without relying on user feedback or dialogue. Feedback coming from users of the matchmaking service will be elaborated in the form of a closed-form dialogue by UPC; hence, initial solutions can be fine-tuned according to user preferences that can be obtained along the performed ranking. The provided user preferences will be used to tune the matchmaking algorithm and make it more robust and effective. 

WP6: Horizontal Matchmaking, reputation and fairness
The vertical matchmaking service design and development is the main objective of this WP. The project will set out a framework for Benchmarking of AI Applications and Solutions on various optimized hardware platforms for Deployment. The WP will:
1) Select a set of representative machine learning  (ML) models from open research that will be used to estimate the running time and the cost of commonly used AI algorithms on a range of commercially available hardware platforms, and take user’s constraints (memory, latency, and energy) into account. 

2) Develop benchmarking framework/service suitable for selected machine learning models and hardware including CPU (Intel x86, Arm Cortex-A5x, Risc-V), GPGPU (NVIDIA, etc), and NPU (HUA, etc) accelerated platforms.
3) Integrate and execute benchmarking framework on the selected hardware platforms.

4) Develop an optimization engine for matching target AI applications with the HW platforms. The optimization engine will exploit the ML models for optimizing the hardware dimensioning and selection, and provide the recommendation of the most appropriate AI framework and hardware infrastructure to the users, while respecting the user's constraints. Cloud resources, embedded systems , HPC machines and off-the-shelf general-purpose hardware will be addressed.

Task 6.1 Development of cross-platform benchmarking for AI applications on heterogeneous hardware
This task involves creating an open source cross-platform benchmarking framework of AI-based applications (ML models, optimization models, simulation models) for heterogeneous computing platforms developed by UNIBO, using the LPDNN framework. LPDNN, which stands for Low Power Deep Neural Network, is an enabling deployment framework which provides the tools and capabilities to generate portable and efficient implementations of DNNs for constrained and autonomous applications such as Healthcare Auxiliary, Consumer Emotional Agent, and Automotive Safety and Assistant. 

The benchmarks will be run under a wide range of different hardware configurations and exploring a sufficiently large space of hyperparameter and input instance, as to thoroughly describe the targeted AI applications. A set of metrics characterizing each application run will be stored, ranging from runtime, number of parameters, number of operations, latency, thoughput,  maximum memory usage, memory profile, and CPU/GPU/NPU usage and solution quality (w.r.t. a baseline). The output of this process will be the creation of multiple data sets representing the AI benchmarks on different HW platforms.

It aims to measure and provide a vendor neutral algorithm benchmarking on a wide range of hardware platforms including CPU (Intel x86, Arm Cortex-A5x, Risc-V), GPGPU (NVIDIA, AMD, Intel, etc), NPU (NVIDIA DLA, Qualcomm, Huawei, etc) for edge and HPC platforms. 

Evaluation metrics for common machine learning algorithms ranging from classification and regression problems will be used including a range of deployment performance metrics consisting of latency, throughput(fps), peak memory usage, storage usage, as well as the evaluation metrics obtained from reference datasets. The cross-platform framework will target a multi-architecture (x86, ARM, Risc-V) Linux environment supporting vendor optimized drivers for GPU and NPU acceleration. Benchmarking will target  threecategories of hardware platforms: HPC, Cloud, Embedded Computing, and off-the-shelf general-purpose platforms.

The data sets created will then be used to build, train and validate ML models -- in particular from the Deep Learning (DL) area – with the purpose of learning the relationships between AI algorithms, input instances, algorithms hyperparameters, hardware platform features and configuration.

BCAwill provide Risk-V platform and integration of HUA NPU (HUA to provide API for AI running on ARM and x86; their contribution will include integrating the API and SDKs into the benchmarking framework) BCA will provide integration to following platforms: Intel x86, Arm Cortex-A5x, GPGPU (NVIDIA, AMD,), and NPU (NVIDIA DLA, Qualcomm).

Task 6.2 Deployment and verification of LPDNN benchmarking framework on hardware platforms and generation of benchmarking results 
This task involves:
* Deployment and verification of LPDNN benchmarking framework for HPC, , and Embedded Computing Platforms;
* Generating benchmarks results for all ML models on selected hardware platforms
* Enabling a hardware marketplace.

In this work, we integrate LPDNN into the StaiwAI approach and present its lightweight architecture and deployment capabilities for embedded devices. Further, we show the deployment of KWS, image classification, and object detection applications on a set of embedded platforms while comparing to other deployment frameworks. In this task the partners will deploy and verify LPDNN benchmarking on specific platforms and generate the corresponding benchmarking results. The run of benchmarking will be performed by BCA on embedded systems hardware CPU (Arm-Intel), HUA on Huawei hardware, UNIBO on Risk V and EGI and INFN on GPGPU.

T6.3 Vertical matchmaking optimization engine 
Based on the benchmarking results of T6.3, this task aims at developing  and build an optimization engine for matching target AI applications with the HW platforms, or suggesting the set of most suitable HW architectures (and their configuration parameters) for a variety of AI algorithms. The proposed matching depends on the users’ requirements, defined in terms of time-to-solution, solution quality, and available hardware resources, which are described using a declarative approach.

The task will develop the vertical matchmaking component combining techniques from the optimization area (e.g., Constraint Programming, Mixed-Integer Linear Programming, Mathematical Programming, Recommender Systems) to solve the service mapping problem under users’ constraints. The optimization engine will exploit the ML models characterizing the AI benchmarks developed in T6.2, in order to guide the matchmaking process. The ML models will be used to identify the best HW platform capable of providing the desired performance (e.g., in terms of solution quality of budget consumption) under the constraints imposed by the available hardware resources.

BCA will provide its AI pipeline framework and  LPDNN benchmarking tool developed for the Bonseyes platform. HUA will support the creation of a lab environment by providing its Ascend NPU-based Atlas hardware platforms and software (Mindspore).

WP7: Open Call Management
The main objectives of WP7 are
* To develop business model/s for the use of the services developed and the work plan of the post-project market penetration of the extended AI-on-demand platform; 
* To network with the stakeholders reached in WP8 and maximize the project exploitation; 
* To cluster with other projects financed both under this topic and with other relevant projects, funded by Horizon 2020 and European initiatives related to AI; 
* To develop and implement a strategy for knowledge management that also proposes valorisation routes for the project findings and ideas generated.
* To launch  and manage 3 Open Calls (2 Pilots Calls and 1 Call for Adopters) along the project to select top 60 low tech SMEs and to support them in the implementation phase. 

The objectives targeted are: to reach out a critical mass of applicants; to support applicants in submitting a quality proposal; to manage the full process of open calls following EC standards, to execute and scale up all the bottom-up projects selected with a full provision of services during the 2 to 6 months of support program.

FBA will be responsible for guaranteeing the fulfilment of the EC requirements and adhere to Horizon 2020 standards with respect to transparency, equal treatment, conflict of interest and confidentiality, and the management of FSTP based on the indications established in the Annex K of the Horizon 2020 Work Programme ‘K. Actions involving financial support to third parties’, in the ‘Guidance note on financial support to third parties under H2020’ an in the ‘Art.15.1 and 15.3 of the Model Grant Agreement’. 

Task 7.1. Open Calls Preparatory Tasks 
This task will start 2 months before launching the Open Calls and will include the following tasks: 1). Challenges Definition]. Before each Open Call, the Selection Committee , lead by FBA, will define the challenges, to be explicitly addressed in the final text of the Call.  
2). Open Call Package of Documents. The Call Announcement and Guide for Applicants will be sent to the Project Officer 30 days prior of its foreseen date of publication and then the rest of documents will be prepared including Guide for Evaluators, Frequently Asked Questions and Application Form template; 
3). The Open call management tool will be setting up and customized in FundingBox Platform .

Task 7.2. Open Calls launch, management and dissemination 
StarwAI will launch 3 Open Calls, 1 Call for Adopters and 2  Pilot Calls. Once the Open Call is launched, and during the 2 months until the deadline, FBA will coordinate the following tasks: 1). Help desk to support applicants regarding the formal aspects of the application process and IT support for the Open Call Tool. 

2). Open Call dissemination: FBA will be in charge of coordinating the on-line dissemination of the Open Calls, based on ‘Growth Hacking Strategies’ and organising some face-to-face dissemination activities such as  Info Days and participation in 2 world-class events at EU Level and 6 Webinars  with the collaboration of all involved partners.  
3). Open Call monitoring. FBA will analyse, on a regular basis, the number of applications registered in order to find out if additional support actions are needed. 

Task 7.3 Evaluation of proposals & FSTP Agreements The evaluation of the proposals will be done in the following steps:  1). Proposals reception exclusively through FundingBox Platform. 2). Eligibility checks and pre-scoring, done by FBA, and pre-scoring criteria agreed among partners. 3) External evaluation done by 2 ‘Independent Evaluators’ which must be independent from the FSTP Project proposers and can not be Consortium partners employees nor board members. Once all the individual evaluations are done   the final score will be calculated as an average of the individual assessments provided by the Evaluators. As a result the ‘Ranking List’ will be elaborated. 

4) Consensus Meeting. The ‘Selection Committee’ will check the best proposals scored 
in the previous phase and will decide, by consensus, the ‘List of finalists’ to be invited to the Jury Day. In the call for adopters the beneficiaries will be selected in the Consensus meeting.  5) Jury Day (only for call for pilots) FBA will organise an online Jury Day where the companies selected will pitch their projects and the Selection Committee  , by consensus (or ? votes)  and based on the ‘awarding criteria’ will select the proposal candidate as winners that is the ‘Provisional List of FSTP recipients’ and ‘Reserve List’. Those lists will be sent to the Project Officer for his/her review. 6). Communication of results. Once the selection process is finished, without delay, FBA will publish the results in the Project Web site, including a description of the 3rd party action, the date, the duration and the legal name and country (Open Call Outcome [D3.3]). All applicants eliminated will be informed of evaluation results too. Once the selection has been done, 

7). Legal Validation. FBA will request to the ‘potential FSTP recipients’ all documentation needed to prove their compliance with the Eligibility Criteria and for the Legal validation. In case of any requirement/info requested is not provided by the beneficiary, this will directly end the Agreement setup process and projects inside the ‘Reserve List’ will replace the failing applicants. (8). FSTP Agreement signature. Each entity will sign a FSTP/Sub Grant Agreement with UNIBO, after validation and before starting any activity. And UNIBO with support from FBA will make the Financial follow up of all sub-grantees along the support program. (9). Ethical Review: All FSTP finalists proposed are carefully verified by the ‘Ethics Experts’, coordinated by FBA, to see if there are any ethics issues raised in the proposal. If so, an ‘Ethics Summary Report’ will be produced and attached to the FSTP agreement.  
FBA will send to EC an Open Call Evaluation Report [D3.4], within 30 days of the end of evaluation, describing the tasks done in the whole process. 
 
Task 7.4 Call to identify Experts and HW Resources providers 

Definition of the Validation Process. EGI will coordinate the definition of the validation process (profile required for experts and HW Resources Providers, criteria to select them and procedure for the validation) together with all partners involved in this task

Experts Open Call and HW Call and Validation. FBA will launch an open call to identify experts and HW resources providers in those domains through FundingBox Platform. All partners involved in the task will be in charge of evaluating the proposals received and selecting the best ones to be included in the Experts and HW Resources Providers. The Experts and HW  Resources Providers Catalogue will be made available on the StairwAI websiteand updated every 6 months.

Vouchers Management. SMEs needing specialized support on these domains will be matched with the Experts and HW Resources Providers in task 7.5 
Lessons learned and upgrade [EGI] The Voucher service will be continuously improved and updated (if needed) at least 2 times within the project, once after each round of experiments to incorporate lessons learned.

Task 7.5. Support Program  
UNIBO will be responsible for coordinating the FSTP Projects execution and will responsible of FSTP Sub-grantees financial follow-up. The process described below will be implemented fully 2 times within the project timeframe, one per each batch of Pilot Call projects and one time until Stage 1 and without Welcome Event for the Call for Adopters: 

? Overall Management & coordination of FSTP Projects [FBA]. During this task, the organisation and reporting of the services to be provided will be coordinated (and clearly communicated to all FSTP Projects). The mentors will be appointed and trained  to properly organize the work to be done along the support program and the coordination among the mentors; Then, the FSTP Project -together with its Mentors - will define an ‘Individual Mentoring Plan’ which establish the KPIs and Deliverables that will be taken into account when evaluating the FSTP Projects’ performance. It will be annexed to the FSTP/SubGrant Agreement signed with each FSTP Project. Periodic follow-up meetings between Mentors and Bottom-up Projects will take place to plan and track their evolution along the support program and, at the end of each stage, a deep analysis (Milestone Review) will be done to evaluate the FSTP  projects performance as described in Section 4.3; 

? A Welcome event [FBA] will be organised to introduce to FSTP Projects the full program, the Mentors and resources available to support them and the procedures for internal organisation. 
? Stage 1 [UNIBO and FBA] During this stage, the Mentors will work with the FSTP Projects, together as a team, and will provide the technical and business services for defining their Feasibility study for the adoption of AI. Including an horizontal matchmaking with the AI and resources and a vertical matchmaking with the HW resources validated in task 7.3

? Stage 2 [UNIBO]: In the following months the mentors will support the FSTP projects developing the Pilot and a first outline of Market Analysis will be done.. 

WP8: Outreach activities and long-term sustainability
WP8 is intended as a transversal and integrated range of activities ensuring the communication and the visibility of the project, the promotion of its results and the long-term sustainability actions. The overall objective is to define and implement a strategy that will lead to the successful dissemination and communication of project goals and activities and fosters its impact even after the end of the project.

It will include both internal and external communication activities and a set of dissemination actions aimed at:  
* involving and informing the research and industrial communities; 
* attract and engage SMEs in the industrial and service sector, in order to allow them to access the AI-on-demand platform to improve their business; 
* raise awareness and make visible amongst the general public on EU’s support for improving of industrial communities.

The aim of this WP is to embed StairwAI within the landscape of relevant European and national initiatives. This includes, as primary concerns, the interaction with the AI on demand Platform (AI4EU), Digital Innovation Hubs and ICT 48 networks. However, given the interdisciplinary nature of our work, we believe that StairwAI is important for a wide range of initiatives around AI such as broader European AI networks (EurAI, CLAIRE, ELLIS), the AI-Data and Robotics PPP, but also to low tech-SMEs and DIH networks. 

The WP is lead by Barry O’Sullivan from University College Cork who is the President of EurAI, the main European AI association and well connected within all relevant European AI communities and the consortium, as described in the proposal groups together members covering crucial roles in AI4EU and European initiatives.

Task 8.1. Synergy and coordination with European AI on demand Platform 
Collaboration with AI4EU has a special role within StairwAI as the AI4EU platform will be the key channel to bring assets, community AI experts, datasets and publications to the European AI community. In order to synchronise and enrich the AI4EU community, this task will be responsible for creating the bridge between the potential StairwAI consumers and AI4EU bodies. 

A clear example is to use the AI4EU channels to disseminate the challenges of WP7 and to engage with companies and SMEs that will be participants in the Open Calls of StairwAI with the community of AI4EU. To ensure the synchronisation, StairwAI will contribute to the bodies in AI4EU responsible for community outreach including the AI4EU Editorial Board, the Strategic Events Committee, and the European Observatory on Society and Artificial Intelligence (OSAI). In addition, it will also build bridges with the Architectural Committee, the Industrial AI Alliance and the future AI4EU Foundation, providing seamless integration of the services developed in the project, and supporting T2.4 and T2.5 within the AI4EU architecture.. 
The StairwAI website will host information about the project, with the majority of communication, and dissemination outputs and the developed services within the AI on-demand Platform. Dedicated effort will be allocated to supporting the Content Management System of the on-demand Platform.

Task 8.2 DIH Connection, Regional Interactions and Industry Clustering 
A detailed mapping of the stakeholders will be done based on targets indicated in Section 2.2. This list will be regularly updated with relevant industry and ecosystem contacts. A specific action will be done to connect DIHs to the project as they are one of the main stakeholders with the capacity to disseminate the StairwAI project at regional level, reaching regional SMEs and engaging with regional entities for coordinating the project activities with Structural Funds. 
DIH connection consists of the following tasks: DIHs scouting (filtering the most relevant DIHs, utilizing the network of 105 DIHs in the FBA ecosystem and partners networks and from the DIH Catalogue), DIH activation (invitation become the Supportive Partners), 2 DIH Roadshows (visiting the DIHs and organizing joint events to access target SMEs/large companies). 

All of these actors, organizations relevant to the area of remote monitoring, will be invited to become Supportive Partners, who will with the project in a quid pro quo relationship. The Supportive Partners will commit to spread the word about the project’s activities and raise both its visibility by displaying the StairwAI logo in their portal and engagement with the project by promoting the StairwAI consortium in their ecosystem. In return, the StairwAI consortium will offer them visibility (including logo and link to their site) and opportunities to participate in events in a preferential role such as speaker, success story, or best practice.

 Furthermore, partners commit to widely disseminate the project activities and results within Industry Networks and European networks/projects in which they are directly represented such as members or experts etc.. Additionally, to ensure a closer relationship with the more strategic ones, StairwAI will participate in activities promoted by other projects and networks to ensure the exploitation of synergies/technical concertation and will perform joint activities for information exchange, general dissemination and training.
 StairwAI will also contribute to EU-wide repositories of reference and dissemination portals identified within the project’s dissemination strategy.  This task will create a network that will ensure the project sustainability.

Task 8.3 Synchronization with AI Data and Robotics PPP, and joint efforts with ICT48 and ICT 49 
The members of StairwAI has been active participants in the roots of AI4EU and the Community Building activities initiative to establish an European AI, Robotics and Big Data with several of the consortium members being part of the consultations (Barry O’Sullivan, Michela Milano). 
We will build on this connection to actively engage and cooperate with the initiatives. In addition, this task will target to synchronise with the ICT 48 and the ICT 49 projects that succeed. This will avoid duplicity of work and will construct with all of them a common vision to support all work already developed by European Initiatives, projects and other stakeholders, and useful to boost StairwAI.

Task 8.4 Coordination with national and regional initiatives 
The national and regional strategies are key for StairwAI to reach local communities and are instrumental inserting their respective countries’ AI strategies. UCC coordinated the creation of an AI National Contact Point Network of volunteer experts on AI on each Member State that as part of AI4EU and in which UPC and UCC has two of these representatives. 

The aim of this task is to make use of the activities within this network through AI4EU It will support the national dissemination with appropriate materials. Finally, it will ensure that all relevant national/regional activities in all countries are engaging with the benefits of the project and the AI4EU community. This task will create a network that will ensure the project sustainability.

Task 8.5 Global outreach for SMEs and Public bodies 
This task will coordinate the efforts towards ensuring global visibility of StairwAI as an accelerator for SMEs development, and public bodies and low-tech SMEs digitalisation, independently on the segment – manufacturing, education, technological, telecommunications, healthcare, etc. - and special focus will be given to low tech ones. 

The participants in the project (UCC, UNIBO, THA, FBA) are deeply involved in AI4EU but also in the ICT-48 projects and other emergent actions can favour the growth of SMEs within the European panorama of AI and the AI4EU ecosystem. In addition, the effort will not be restricted to SMEs, but will include broader outreach to for example Public Institutions and bodies – hospitals, city councils – and NGOs and other international organizations (e.g. within UNO or WHO).


The AI-on-demand platform is growing as a common one-stop-shop to guarantee access to AI assets (tools, data sets, experts, papers, courses etc.) and to unite the European community of AI stakeholders: researchers, developers, large companies, SMEs, start-ups, public bodies and society at large. However, it is largely recognized that the existing platforms, tools and AI assets, while bringing substantial advantages for developers, data scientists, researchers and, in general, AI experts, are still out of reach for low-tech users, like SMEs. In many cases, SMEs and domain experts do not even know whether AI can help them in their business, which area of AI could potentially benefit them and where to find consultants, employees, tools, material and required expertise.

The European AI-on-demand platform could have a tremendous impact on the European industrial ecosystem and on European welfare in general if it becomes a Stairway to AI for low-tech users. Enabling an easy and intuitive interaction with the platform, guiding the low-tech users in the discovery of the relevant tools, data-sets, experts and employees, will boost the adoption of the AI-on-demand platform and guarantee its long term sustainability as it becomes an indispensable access point to AI-based business.

In addition, to enable the adoption and the scale up of existing AI techniques in low-tech industries and SMEs, it is essential to identify and dimension the proper hardware resources that are needed in terms of edge devices, cloud resources and HPC infrastructures. There are currently no existing automatic tools at the moment that are able to manage such identification and dimensioning activity, and even AI experts have to perform practical experimentation to customize a given solution running on the hardware resources that are available. 


In this way, StairwAI will have a huge impact on providers of hardware resources – big companies, SMEs and public institutions - that will find on the AI-on-demand platform a marketplace for monetizing their resources and expand their business models.

The first objective of StairwAI is to provide a service layer for the AI-on-demand platform composed of: 
* Multi-lingual interaction: the interaction with the platform is provided in each user own language.
* AI asset discovery: the platform enables automatic mapping of use cases to proper AI assets.
* Community profiling: the platform will profile user needs and provide recommendations on content and AI assets in a proactive way.
* People-to-people matching: the platform enforces the European community building strategy by fostering useful connections with experts, consultants and developers.
* Job offer-demand search: the platform improves the matchmaking between job offers and demand.
* Hardware dimensioning: the platform provides tools for predicting the hardware resources needed for running a given AI program on specific data and on specific problem instances.
* Physical resource provider marketplace: the platform enables a marketplace for supporting SMEs, Companies and Academia that can act as hardware resource providers in Europe, to support SMEs, companies and academia, building a pool of AI resources and to strengthen the IT sector.
* Trustworthy AI and fairness: As one of the main requirements of the Trustworthy AI Guidelines provided by the High Level Expert group appointed by the European Commission1, all the principles will be ensured, but specifically the requirement of fairness applied to the recommendations provided on community, assets, and hardware resource providers.

All these functionalities can be obtained through the use of AI algorithms, techniques and methodologies. The second objective of StairwAI is to use Artificial Intelligence to enhance the AI-on-demand platform. In particular, 
* natural language processing and automatic translation will be employed for the multi-lingual interaction; 
* matchmaking and constrained optimization will be used for the discovery of AI assets, people-to-people matching, and job offer demand search; 
* knowledge representation and ontologies for AI assets categorization;
* recommendation systems for community profiling;
* machine learning for AI assets categorization and hardware dimensioning. 

The StairwAI project brings together a very strong consortium that covers all the required competences, composed of experts in natural language processing (TIL), constrained optimization and recommendation systems (UNIBO and UCC), competences / skills and knowledge representation (UPC) and machine learning (TUE). In addition, strong consortium members on hardware platforms (HUA) are involved for the edge and embedded system part (BCA), for cloud (EGI) and HPC (INFN) infrastructures. Finally, THA will guarantee integration and sustainability with the existing AI4EU platform, while FBA will manage the cascade funding that is devoted to the involvement of low-tech users through use cases that will test and validate all the implemented functionalities. 
1.1 Objectives
The objectives of StairwAI can be grouped in three subcategories: Technological, Commercial and Societal. We outline the related work packages that contribute to achieve each objective, the corresponding KPIs and target values
Technological objectives
TO1 – To ease the access to the AI-on-demand platform through natural language interaction
StairwAI seeks to ease the access to the benefits of the AI-on-demand platform. Specifically, the NLP engine will provide a chatbot that will ease the interaction with the system in the native language of the user. The StairwAI project will make available a set of five highest impact languages in EU. The ambition is to extend the service beyond the duration of the project in the attempt to cover all the 23 official European languages.
Related WP: WP4. KPIs: Number of custom NLP models available (5), Platform chatbot users (in 6 months from the delivery) (500).
TO2 – To satisfy the end user needs through horizontal matchmaking services
The objective of the matchmaking services is to act as a link between low-tech users to AI experts and consultants, training and education activities, assets/software/services/tools in the repository of AI4EU, and physical resources/technologies registered to the AI4EU platform. 
Related WPs: WP3, WP5, WP7. KPIs: Number of use cases correctly mapped to AI assets (20), Number of people connected (20), Number of job offers mapped to experts (20). Number of pilots/technology demonstrators (28).
TO3 – To dimension the physical resources of the AI on demand platform with end user requirements
StairwAI’s vertical matchmaking has the objective to provide an automatic tool to dimension hardware resources given the algorithm used and the application to be solved. This will help non expert users not only to choose the proper technique (which is achieved in the horizontal matchmaking) but also to find hardware resource providers and the proper hardware dimensioning for running their code.
Related WPs: WP6, WP7. KPIs: Number of hardware providers mapped and validated (10), Number of pilots properly dimensioned on hardware (28). 
TO4 – To demonstrate the feasibility of the service layer developed as a cornerstone for the uptake of AI in low-tech SMEs
The three services developed, namely the multi-lingual interaction, the vertical matchmaking, and the horizontal matchmaking will be integrated into the AI-on-demand platform with the primary objective of easing the access to AI assets for low-tech users, in particular SMEs. The effectiveness of this integration will be mainly demonstrated through pilots and feasibility studies.
Related WPs: WP2, WP7, WP8. KPIs: Service layer integrated in the AI4EU platform.  
Economic objectives
EO1 – To contribute to strengthen the European SMEs – including low tech SMEs – and DIH 
StairwAI seeks to provide mechanisms to strengthen SMEs (particularly low-tech SMEs) and other stakeholders that are looking to use of AI services in their own businesses, thus impacting the uptake of AI technologies to create added value. The FSTP Program further pursues this objective by involving, SMEs, AI experts and hardware providers in calls. 
Related WP: WP7, WP8. KPIs: Number of open calls (3). Number of SMEs reached (2,500). Number of SMEs participating in the open calls (300). Number of feasibility studies for the adoption of AI (60). Number of pilots/technology demonstrators with SMEs (28).  
EO2 – To promote the sustainability of the AI on-demand platform 
The reduction in barriers for European low-tech SMEs in the adoption of AI technologies through (1) the multi-lingual interaction with platform, (2) the horizontal matchmaking automating the discovery of AI assets and (3) the hardware resource dimensioning and provisioning will promote the spontaneous use and uptake of the AI-on-demand platform after the project end. 
The pilots covered by the cascade funding will be assessed considering the effective use of the AI on demand platform in terms of sustainability and exploitation plans, integration with other services of the AI4EU ecosystem and connection with the DIHs and other European networks and Networks of Excellence (NoEs) contributing to boosting StairwAI results close to the market. 
Related WP: WP7, WP8. KPIs: Number of DIHs reached (100).  Number of hardware providers mapped and validated (10) . Number of new users in the AI4EU platform (5,000).
EO3 – To reduce fragmentation of AI research and development
Europe’s major commercial weakness is caused by the fragmentation and misalignment of multiple initiatives, associations, and foundations, among others, and the resulting fragmentation of the AI ecosystem in Europe. StairwAI seeks is to align with the European vision stated in the White Paper on Artificial Intelligence, with European initiatives (AI4EU, the ICT 48, ICT 49 projects, AI PPPs, EurAI) to avoid duplication of efforts and making better use of the resources of the project in the impact, as described in Section 2. 
Related WP: WP8. KPIs: Number of events connected with European initiatives (10).
Societal objectives
SO1 – To provide a framework compliant with the principles of ethical AI use
StairwAI aims is to provide a fair selection of AI experts and assets independently of gender, sex, religion of experts and be sure that its decisions are made according to fair principles as driven by the Trustworthy AI Guidelines provided by the EC. StairwAI will not bias hardware providers under any circumstance that is not functional, such as: public or private, location, or size. Furthermore, StairwAI will promote and ensure that all partners and SMEs interacting with AI components, as developers or consumers of the open calls, are aware of these Guidelines and develop AI applications that adhere to these Guidelines. 
Related WPs: WP5, WP7. KPIs: Number of development compliant with Trustworthy AI Guidelines (20), Number of SMEs informed on Trustworthy AI Guidelines (300)

1.2  Relation to the work programme
StairwAI will address the following key aspects of the work programme with the following:
From the work programme: “This activity aims at consolidating the eco-system by bringing in a larger user community, especially from the non-tech sector, and by reinforcing the service layer of the platform.”
These are the two main outcomes of StairwAI. On the one hand StairwAI will foster the involvement of low-tech users, in particular SMEs in all vertical sectors by easing the use and interaction with the platform. Along this line, the reinforcement of the AI-on-demand platform service layer is achieved through the development of a multi-lingual interface with the platform, an AI asset discovery of AI assets and helping dimensioning the hardware resource needs and the proper hardware resource providers.
From the work programme: “Reinforce the service layer of the AI-on-demand-platform funded in ICT26-2018-20 to facilitate the use and uptake of the platform resources.”
In the AI-on-demand platform development, a task is envisioned to foster cooperation between industry and academia. While technology companies and AI researchers can easily access platform resources, the services provided by the current version of the AI-on-demand platform lack capabilities to incorporate low-tech stakeholders. StairwAI will act as an enabler for low-tech users. It will enrich the AI-on-demand platform service layer enabling sophisticated matchmaking bridging use cases to assets, job offers to experts, and providing hardware dimensioning. Fair recommendation and reputation mechanisms will also be implemented in StairwAI, thus increasing the trust in the services developed.
From the work programme: “Reaching out to new user domains and boosting the use of the platform through use cases and small-scale experiments. The task will involve financial support to third parties”
StairwAI will target mainly low-tech users working in domains like manufacturing, automotive, agriculture, health, governance, etc. that do not have immediate access nor knowledge on AI techniques. Part of the open calls with cascade funding will be devoted to use-cases in these domains. A second community that will be targeted are SMEs providing computing resources that could benefit from the hardware dimensioning services. A part of the open calls will address these stakeholders.
From the work programme: “Proposals will ensure continuity with the project selected under ICT26-2018-20, having access to all the knowledge and offer needed to fully exploit it and be able to build on it.”
The connection with the ongoing AI4EU project is guaranteed by five partners having key roles in the AI4EU project, namely its coordinator, the leader of the WP4 on community building, the leader of the management of the open call and the leader of the Strategic Research and Innovation agenda. In addition, the partner managing one pilot, has a leading role in AI4EU, chairing the AI4EU Education Committee. The strong and deep knowledge of the platform structure and content enables the StairwAI consortium to closely collaborate with the AI-on-demand platform and to effectively integrate results. 
From the work programme: “The improvements resulting from the selected projects should be made available and open to the community via the platform, to allow full exploitation, and also further developments by entities outside the consortia, building on these results”
The developments of StairwAI will be made available in the existing AI-on-demand platform, and will also be used as stand-alone components, fully open and available for further developments. All pilots and technology demonstrators coming from the open calls will be made available on the AI-on-demand platform thus creating a repository of use cases, properly documented and communicated, that can represent best practices for other developers and for other low-tech SMEs. 





StairwAI aims to enrich the service layer of the AI-on-demand platform to ease its access and uptake from low-tech users. The service layer of StairwAI will be tightly integrated with the AI-on-demand platform 
StairwAI proposes to create a service layer on the AI-on-demand platform that makes the platform more accessible, intuitive, easy to use and makes it a reference access point for AI assets even from low-tech users. Three services will be designed, developed and integrated into the platform: the natural language interaction service supported by a data knowledge representation system, the horizontal matchmaking and the vertical matchmaking.

The initial interaction of these end users with the StairwAI service and software stack will take place from the Frontend UI. The integration will be performed by UCC as they are currently leading the matchmaking activities within the AI4EU project as part of the community building, therefore facilitating a seamless integration. These activities devoted to matchmaking, is concerned with the definition of the ontology, and will start from the existing development, and extend it in the direction described in StairwAI. Furthermore, this block in the UI will provide the control to a virtual assistant / chat-bot system that will provide to the end user a frame in which the concerns to find solutions can be described. 
The virtual assistant will be supported by a Natural Language Processing (NLP), Understanding and Translation engine. This – initially giving support for some of the most widely spoken languages in EU but target to be extended to support all European languages – will be analysing and identifying the relevant aspects described by the end users. The key elements will be used as connectors between the case study provided by the companies, SMEs, public bodies, to provide a solution that can help their business or research activities to develop.
This development will be in the form of AI experts, software services or tools (also algorithms), training courses, papers, information or any other information in the AI on demand platform that can satisfy their needs. Furthermore, in case of having any software asset, the end user will require to sandbox, test or deploy the solution and, therefore, will have a need for hardware in terms of physical resources and technology: commodity or specialised hardware, and containerised, virtualised resources or bare metal, depending on the asset. Due to the high number of expected end users and assets - and the unstructured databases used by AI on demand platform – to get the maximum benefit of all this information a data knowledge representation will be added linking concepts, individuals, and relations (Ontology) supporting the NLP engine. 
The Horizontal matchmaking engine will receive as input the outcome of the NLP engine that provides user needs. The horizonal matchmaking service aims to find the combination of techniques that satisfy the case study. 

To enable matchmaking, it is essential to construct a common language that, from the NLP input, produces a set of key components that are used to identify into the Ontology the AI4EU components as AI-assets: tools, people, data sets, benchmarks, courses that are then proposed to the user as alternative results for the matchmaking.

Fairness is an essential property of the matchmaking and it should be guaranteed for every result. Therefore, StairwAI will provide a reputation mechanism that will rank resources available on the platform. This mechanism can be enforced and fuelled via a feedback loop from users toward the platform. 
In instances where the development of the case study need to deploy AI assets on physical resources, a Vertical Matchmaking process will be triggered. The process consists on given a set of requirements from the end user – time constraints, costs, privacy, or any other – to find the appropriate set of resources that satisfy a given algorithm and tool in the hardware resource provider marketplace. Thus, building on top of the Bonseyes platform, initially targetting cyberphysical systems, StairwAI will extend the work to enable the seamless participation of heterogeneous resource providers – SMEs, large companies, public bodies including universities or computing centres – towards supporting HPC, Cloud and Edge resources, and heterogeneity also at the level of the technology – Conteinarisation, Virtualisation, and BareMetal services and microservices if required.

This is expected to have a great impact as this marketplace will provide a sustainability plan for the AI on-demand platform, extending the inftrastructure support – as there is only one current resource provider that is Teralab, and it will also provide an opportunity to research centres, SMEs working in the resource provider area, and telecommunication companies to monetise their resources through this space. 

The workflow of the Vertical matchmaking:  as inputs the Optimisation system for the user request receives as input as set of selected algorithms or services that need to be deployed for the success of the case study. Then, the engine also receives as inputs the user constraints – time and cost, but can be extended to any other paramters that can be identified during the development of StairwAI. The combination of both – as well as the underlying technology – conteinarisation, virtualisation, baremetal – and the implementation available linked to the architecture will prune the multiple options and, by using Machine Learning techniqies will select the best resources. 
Next, the integration process and requirements with the AI on -demand Platform is described. This process targets to provide an enriched semantic AI resources supported by an enhanced information system developed in StairwAI and to implement and assist existing and new stakeholders by allowing the system to evolve on understanding people’s utilisation. For the latter aspect, ML techniques will be used based on the utilisation of StairwAI improving the QoE of end users.
Technology readiness level
As already introduced, StairwAI will support different type of SMEs with various TRL levels, resulting from their role in the project: 
?      Pilots Call - the AI experts will put their AI resources at the disposal of the low tech sector SMEs. Those resources will start in TRL 5/6 and can move up to TRL 7-9, depending on the work scope with the low-tech Sector SMEs.  On the low-tech SMEs side it is expected to develop new products varying from TRL 5 to TRL 7 and beyond thanks to their interrelationship with AI experts and mentors. 

?      Call for Adopters – low tech SMEs will focus on the preparation of the feasibility plan for the adoption of AI and test the services offered by StairwA so it is expected that the initial TRL level will be TRL5 amd beyond. 


The methodology is divided in 4 main phases:
* Development of the service layer that includes the multi-lingual interaction with the platform, the horizontal and vertical matchmaking and their integration in the platform; within this phase is also an initial subphase of requirements, data collections, plans for communication, dissemination, data management, and trustworthy AI evaluation on components design. 
* Activation and selection of low-tech  SMEs.
* FSTP Projects execution for the adoption of AI by low tech  SMEs (Support Program) and test of the service layer
* Sustainability of the project including the long-term uptake of the AI-on-demand platform that is fostered by StairwAI services and the adoption of the StairwAI model by low-tech SMEs and DIH.

Phase I. Development of the service Layer
The services provided to the end users rely on the natural language interaction in a form of a chat-bot developed in WP4. This engine, that identifies specific elements of interest for the end user, is supported by WP3 – Data knowledge representation – where the information coming from Community Profiles and AI resource assets allocated in the Platform is structured to easily find relations of best support for service consumers. 
By using results obtained in WP3 and WP4, the WP5 – Horizontal Matchmaking – provides the end user the AI assets that satisfies their needs, if possible, and following fair principles and using matchmaking techniques. The decision is supported by a combination of AI experts, AI assets and, for those solutions where software assets are chosen, a requirement of hardware resources to perform the deployment if required. This last aspect on physical resources is developed in WP6, where mechanisms to select the most adequate hardware type and provider in between a layer of infrastructure supporters for AI on-demand Platform – Physical Resource Providers marketplace – takes place. 
WP7 and WP8 will contribute by: (i) providing case studies for StairwAI (75 in total in multiple segments of the EU market); (ii) identifying and supporting the enrichment and enlargement of infrastructure providers – also for the AI on demand sustainability objective; and (iii) supporting the expansion of new case studies by an efficient dissemination through multiple communities with specific emphasis on DIH and European national activities that, for our understanding is where the low-tech SMEs are having the vast part of their businesses. 
The project starts with the activities of WP2 and WP3 that are devoted to the collection of requirements for service development by involving stakeholders (low-tech SMEs through DIHs and associations) The knowledge representation design from WP3 also begins. Then data collection for training the machine learning models of the three services are collected in WP3: they concern multi-lingual data sets describing use-cases, job offers, people and data from hardware profiling and application benchmarking. At Month 10, the development of the Asset Management system, WP4, WP5 and WP6 services start ,based on the requirements and the data collected before. Other activities in WP4, WP5 and WP6 follow to refine the developments. It is important to understand that requirements are collected at the beginning, and then are refined after the first open call, which that might underline some missing requirement or technology gap that has to be filled. All services are then intergated in the platform in the last part of the project

Phase II. Activation and selection of low tech  SMEs
The project will make use of Financial Support to Third Parties [FSTP] scheme to identify disruptive bottom-up projects contributing to the adoption of AI by low tech SMEs. To this end, a robust process has been defined to choose up to 60 SMEs in low tech sectors (stated as initial objective) by launching Three Open Calls, along the project. There will be two types of open calls, Pilot Calls in which 28 low tech SMEs will be selected in two open calls; and one Call for Adopters in which 32 low tech sectors SMEs will be selected. In the first type of open call StairwAI will support the preparation of the feasibility plan for the adoption of AI by the low tech SMEs and delivering a pilot for the adoption of AI, while in the Call for adopters StairwAI will support just the feasibility plan for the adoption of AI by low tech sectors SMEs. 

The goals of the open calls are:
1) To contribute to the adoption of AI by low tech sector SMEs, both supporting them in developing a feasibility plan for the adoption of AI and doing a pilot.
2) To improve the service layer, in the project we will support 60 low-tech SMEs, 28 in 2 pilot calls, the goal of having two calls is to get insights in each of them to improve the service layer. The final 32 in the Adopters call will increase the sample for the improvement of the service layer.
3) To foster the adoption of AI by low tech sectors SMEs not supported by the project, the replicability of the use of AI should be made clear. StairwAI achieves this objective by having two type of calls: one on the development of pilots and one on feasibility studies. In this way we have a greater sample of SMEs and a longer list of showcases and good practices.

The full process (including selection and implementation of the bottom-up projects) will be repeated twice for the Pilot call (M13-M25 and M22-M35) lasting approx. 14 months each, and once for the call for adopters (M27-M36) as detailed in Figure 7. The call for adopters will support just the preparation of the feasibility plan while the Pilot will include a Demonstration after the feasibility plan, following only the steps indicated in the figure below.

The type of projects that will be sought are those addressing the adoption of AI by low tech Sectors SMEs using AI resources previously developed and published in the AI4EU platform. Projects will comply with the criteria set for the program. The matchmaking service layer developed in WP4 and WP5 will be tested in both type of Open Calls at the beginning of the program at the end of the feasibility study phase.

Prior to each call, the Consortium will define specific challenges in consultation with the Selection Committee to define the specific approach of each call for proposal.
The activation of the low tech SMEs potentially interested in submitting proposals, will be done through entities already connected to partners ecosystems (‘Supporting Partners’), through networks reaching out to SMEs from all around Europe, like the ‘Enterprise Europe Network’, the ‘NCP network’ and ‘FundingBox Community’ with more than 27.000 early adopters registered (as of June 2020), and taking advantage of the network of Digital Innovation Hubs (DIHs). Also, by organising several 6 InfoDays (online or offline), 3 participation as speakers in world-class events and 6 Webinars [WP7.2] 

All submitted proposals will go through an exhaustive sequential filtering process. All eligible proposals will be evaluated by external evaluators. The ‘Selection Committee’ will be responsible for ranking the proposals and selecting the final ones in the Jury Day.). All the selected proposals will undergo Ethics Review.

The proposals submission and selection procedure will be done through the ‘FundingBox Platform’2, a web-based system which allows FBA managing the whole Open Call cycle according to EC standards and in an efficient and rapid way that is transparent, free of conflicts of interest, confidential and non-discriminatory, in order to ensure equal treatment of all participants. Moreover, by using the network set up in the framework of LEDGER project (H2020 GA 780477)3, this Open Call Management System will use blockchain to authenticate online submission of proposals within deadline (Time Stamping), which will prevent server overload failures during online submission in Open Calls, as well as for gathering the data from external evaluation of proposals (Voting), reducing the possibility of collusion and eliminating the possibility of hacking selection results. 

In parallel to the call for selecting the Low-Tech SMEs a call to validate external providers will be launched (WP7.4). The goal of this call is to select the HW resources providers (cloud computing, HPC) and the AI experts with which the Low-Tech SMEs will work with. This call will be just to validate the providers and have a pool of providers to do the matchmaking between them and the Low-Tech SMEs during of the program. The beneficiaries of the FSTP will be the Low-Tech SMEs that will pay the HW resources providers and the AI experts from their budget. 

Phase III FSTP Projects execution for the adoption of AI by low tech SMEs (Support Program) and test of the service layer

The bottom-up projects will then become part of the Support Program, led by UBO, composed of 2 main stages for the Pilot instrument and 1 stage for the Call for Adopters Instrument. All the actions to be implemented in the Support Program are based on the following values:
? ‘Challenge-driven’ Challenges are the best way to ask for help. It allows us to focus the energy on finding solutions that will ultimately be relevant to overcome a specific problem. Moreover, it helps to define KPIs, and assign valuation criteria before investing additional valuable time and resources.
? ‘Transparency’, we work with Open Models where all the processes and results are visible in order to build a reputation and high credibility (respecting privacy where appropriate).
? ‘High performance’, by means of high demands on delivering quality towards ourselves, our mentors, and, also towards the bottom-up projects urging them to pursue strategic and daring growth,
? ‘Continuous improvement’ by measuring all the activities and services provided, we have direct feedback from bottom-up teams and we can identify “lessons learned” to improve our daily performance.


The Support Program is articulated as a totally customised service, where high level experts in the AI domain that are present on the platform, HW resource providers, top researchers and top business professionals are specifically validated to fit the needs of low tech sectors SMEs and guide them along the whole Program. The experts in AI, HW resources, top researchers and top business professionals will apply through a call and the ones validated will be published in theStairwAI website, and a matchmaking between the SMEs needs and them will be done.

Once selected, the low tech sectors SMEs will go through an exhaustive and customized support program. 


The first step of the program is engaging the talent and building up the best mentoring set up. Selected teams will meet during and intensive Welcome Event ending with a matching of the team with their Mentors. The teams will work intensively over a 4-week period to define their Individual Mentoring Plan (IMP). This document establishes the KPIs and Deliverables4 that will be taken into account when evaluating the bottom-up Projects’ performance. 

The expected results for each stage are: 
? Stage 1. Feasibility Study [WP7.5] (Two months). At the end of this stage a matchmaking between the SMEs and the AI resources that might be used by the beneficiaries, AI experts to support the pilots and HW resources both selected in the validation call will be done. The matchmaking will be done using the service layer developed in WP4 and WP5. As a result of this stage a feasibility plan for the adoption of AI will be defined including technical plan and business model for the adoption of AI. In the case of the call for adopters there is only this stage and there will not be Welcome Event at the beginning of this phase.
? Stage 2 – Pilot [WP7.5] (Four months). During this stage, each team will focus on developing the Pilot and performing a Market analysis to complement the Business Model.

In all this process, the mentorship services defined, to share and transfer the knowledge to the bottom-up projects, are the cornerstone of the Support Program and one of its most vital parts.
? The Technical Mentor will be a PhD. or a Senior Researcher provided by the Research & Technical Partners or with a wide background in AI and with an overall vision of all the Technologies/abilities offered within the project. This mentor will be allocated to a given bottom-up project to match its' needs with what is technologically feasible supporting them in the development of the Pilot. 
? The Business Mentor provided by FBA will support the teams in defining the Business Model, by applying the lean startup methodology, and the Exploitation Plan for market deployment. 

The Technical Mentor, together with the Business Mentor, will act as a kind of ‘Advisory Board’ for the selected bottom-up projects, complementing the capabilities of the team members. They will coordinate with each other since the beginning of the Program5 (a ‘Train the Trainers’ session will be held in WP7.5 to be sure that those profiles work in a cooperative and integrated way) and will do the follow up of the bottom-up Projects, all along the support program, acting as the ‘Mentoring Committee’ which will evaluate the performance of the bottom-up projects after each stage. Webinars may be organised along the project to complement this individual support with group training sessions.

Finally, an exhaustive ‘Review Process’ will be implemented in order to do a proper Follow Up of the bottom-up projects. There will be a ‘Milestone Review’ at the end of each stage (or whenever a payment milestone is scheduled). The review will be carried out by the ‘Mentoring Committee’ and the ‘Selection Committee’ .

In order to control in an integrated way the evolution of the bottom-up projects, based on the evaluations carried out by the different mentors, and also the mentor evaluation done by the teams (as the teams will be also requested to evaluate mentors), we will use a mentoring follow-up tools to monitor progress at every step and make corrections when needed.


Phase IV Sustainability of the project

StairwAI addresses the sustainability issue by means of two specific strategies, as described in Section 2.2.(a).1 and detailed in WP8 (coordinated by UCC):
(1). Ensuring the continuity of StairwAI model in AI4EU platform, the service layer will allow to do an automated matchmaking between low tech SMEs and AI experts, HW resources and AI resources. This service will be tested and integrated in the AI4EU platform to ensure its use after the StairwAI project ends. Tilde will continue to provide support on the service once the project ends.
(2). Adoption of the StairwAI support model by Digital Innovation Hubs. As part of the WP8 we will connect with DIH to engage them with the project, support to attract applicants to the open calls and also to present them the possibilities of the matchmaking layer. The goal is that the DIHs promote the use of the AI4EU platform and of the matchmaking service among its SMEs, and that the DIHs replicate the open calls model defined by StairwAI.

(b).2 Gender analysis
Female entrepreneurial potential in the Open Calls. According to Eurostat data6, female creativity and entrepreneurial potential are an under-exploited: while women constitute 52% of the total European population but only 34.4% of self-employed workforce and 30% of start-up entrepreneurs. Women hold less than 10% of patent applications. The picture is even worse in digital sector. In 2018, 23% of workers of European tech start-ups and 19% of its founders are women7. The Open Calls will be run as a fair and open competition and as such we will select all project based on the excellence of the ideas proposed. However, we are keen to promoting the participation of females in project subgrantees taking advantage of the following: when Scouting low tech SMEs mobilising female talent will be one of the priorities, in case of a tie the projects with a female in the position of management will be prioritised. (Open Call tie-break criteria, Section 4.3)

Project Management. StairwAI is committed to gender equality and will promote a diversity action plan which includes gender, functional and cultural diversity. Different cultures, awareness and approaches to gender issues will be integrated, including the following guidelines to be taken into account during the project execution:
? Management structures: StairwAI will take all measures to promote equal gender opportunities in the implementation of the action and will aim for a gender balance at all levels of personnel assigned to the action, including at supervisory and managerial level. Concretely, StairwAI aims to get a gender balance of 40% of the underrepresented gender in decision-making structures and 50% for the Advisory Group.

Evaluation process: when selecting evaluators out of the pool, StairwAI will try to reach also a 40% target of the underrepresented sex, taking into account the situation in the specific field of expertise. The evaluation process will include a mechanism to avoid biased decisions based on gender and multicultural issues. External evaluators will not have access to the information related with gender and cultural filiation of the applicant before the Jury Day, so the ranking will be done without access to such information.

1.3  Ambition
a. Advancements beyond the state of the art 
StairwAI provides a number of advancements to the platform that will make it an accessible central hub for AI assets for all domains. These advancements encompass a number of domains listed below

Knowledge organization:
The AI4EU platform aims to become the one-stop-shop for anyone looking for AI knowledge, technology, services, software, and experts. To be capable of offering these kinds of services, it is necessary to integrate heterogeneous information, not only from the AI domain but also from interrelated socioeconomic domains such as agri-food, energy, transport, healthcare, industry, environmental sciences, economics or culture. The information within those domains may also be heterogeneous in nature, as it includes: AI assets and tools, academic resources, domain experts’ descriptions, professional profiles or industry problem definitions. Those information resources are commonly exposed using different standard data exchange formats and protocols. 


To make the AI4EU domain cohesive, AI-related ontologies and vocabularies take relevance in order to offer a solution for (i) the lack of connection between AI assets, (ii) lack of knowledge exchange between AI stakeholders and society; and (iii) lack of common knowledge exchange formats to enable a matchmaking framework for AI assets, resources and professionals. Furthermore, one of the challenges from the AI4EU platform is to boost the embracing of its services by societal stakeholders. In this regard, bringing semantic interoperability within the AI domain and societal needs to the AI4EU platform is the main challenge that StairwAI aims to achieve with the planned work in WP3 and WP5. 

Applications that handle world-knowledge in a specific domain require detailed representation of the entities in this domain, as well as the relations among them. This is typically achieved using knowledge representation systems, which may range from a basic semantic network to a full-fledged ontology. 

Future AI semantic vocabularies need to be cohesive with the related domains being able to combine information from energy, environment, economy, politic, agriculture and security domains to mention a few facilitating the matchmaking of AI assets, professionals and resources with application fields and needs. Moreover, the adoption of these semantic models and vocabularies, in the near future, needs to be pushed up from the different industries and citizens instead of scientific bodies to ensure their wide adoption. 

In WP3, StairwAI will advance towards the generation of a cross-domain semantic vocabulary through building, on top of existing AI4EU reference data exchange models and vocabularies, a consistent reference ontology that enables efficient knowledge integration. This project aims to develop a mid-range knowledge representation able to provide the functionalities required by the matchmaking final applications. Given the required detail level, it may not be possible to specify a complete ontology, and therefore we will explore semiautomatic methods to extend the knowledge repositories already existing in AI4EU.

Ambition for the multi-lingual natural language interface:

The objectives of WP4 will be achieved through a complex interdisciplinary approach, leveraging a unique collaboration between computer scientists, in particular artificial intelligence (deep learning) specialists, and linguists (natural language analysis, processing and modelling). The first problem – a lack of adequate language resources – will be addressed (1) by exploring methods for knowledge extraction from multi-lingual text data and (2) providing the means for multilingual access to AI (WP3 and WP4).

First, there will be an investigation of methods for knowledge identification in text data, extraction of the knowledge from text data, automated knowledge base formation (also knowledge representation) and filling, and querying of knowledge bases for widely used languages in the knowledge extraction pipeline. To identify the knowledge that is embedded in the provided text, NLP processing methods that perform semantic analysis of the text are used. 

These methods may involve a complex workflow of NLP tools including syntactic parsing, semantic role labelling, semantic parsing and others. Typically, only very small amount of training data is available for virtual agents, moreover user inputs may contain grammatical errors and the sentences can be poorly structured. 

Although deep learning methods require a large amount of training data, we will leverage the power of deep learning by using pretrained word embeddings that are pretrained in an unsupervised manner on large corpora. The suitability of recent improvements in generating multilingual word embeddings and multilingual language models8 will be assessed.

The identified knowledge can then be added to an existing knowledge base (e.g., WikiData, DBPedia, and others) using knowledge base population (KBP) methods or an ontology can be learned from the data itself. In KBP, slot filling and entity linking methods are used to complete a knowledge base (e.g., Ji et al., 2011). 

The ontology learning task is much more difficult as it entails also the identification of relations that form the structure of a knowledge base (Buitelaar, 2005; Drumond & Girardi, 2008). In order to provide an answer, a query for the KB has to be constructed from the user's text. Traditionally, queries are generated using a workflow of NLP components (e.g., Yahya et al., 2012; Abujabal et al., 2017, etc.) that involve complex natural language understanding methods. A simpler workflow based on keyword spotting was proposed by Shekarpour et al. (2011). Research has also been focused on query generation that cover multiple knowledge bases simultaneously (Zhang et al., 2016). The rise of neural network-based methods has sparked research on end-to-end solutions for query generation (e.g., Sun et al., 2018; Hosu et al., 2018, etc.) as well as KB incorporation in end-to-end models (Madotto et al., 2018). 

Second, the project will research and apply methods for enabling multilingual access to virtual agents that have capabilities of communicating in foreign languages through (i) adapted machine translation (MT) solutions and (ii) cross-lingual structured query generation. In this task we will focus on neural machine translation (NMT)-based systems that are capable of dealing with (i) ungrammatical inputs and (ii) specialized translation domains for queries and utterances that a general-domain MT system may not be capable of handling. 

An alternative approach, which we will explore in the project, is to enable access to virtual agents developed for foreign languages by parsing the source language queries in a structured form, which then could be reassembled in the target language. The focus will be on methods  to leverage NLU methods (for structured information extraction from user utterances in the source language)(WP4), multilingual knowledge bases (for cross language knowledge transfer)(WP3), and natural language generation methods (for query generation in the target language) in order to enable cross-lingual access to AI.  

A key problem is transferring the structured information from the source language into the target language. There has been research in the area of cross-lingual information transfer, particularly from resource-rich to resource-poor languages for multi-lingual dependency parsing9, universal dependency parsing10,11,12 and cross-lingual representation learning13. It is also possible to induce and make use of cross-lingual word embeddings, which can then be used in cross-lingual text classification14. 

NLP needs to be available across users’ devices to improve efficiency and support data privacy. This NLP interaction will run across users’ smartphones as well as computer input devices.  HUA is a global leader in smartphones and associated AI technology. HUA will contribute an open Da Vinci architecture-based mobile NPU and software platform to support local NLP on smartphones. Additionally, the project will leverage innovative and open technologies, such as HUA’s open and industry-leading Da Vinci-powered Ascend NPU-based Atlas systems and MindSpore open source AI framework that enable significantly more energy-efficient and green NLP training in clouds and edge datacentres. HUA will support local, on-device training and model updates for NLP to improve user data privacy.

Ambition for Horizontal matchmaking:
For the recommendation of datasets and certain types of tools, such as machine learning models, we wish to use machine learning. Since this amounts to applying machine learning over the results of machine learning models, this is also called meta-learning. First, to measure how similar two datasets are, we can represent them as a vector or numbers (i.e. an embedding) that expresses the inner structure of the dataset, and use this to measure distances between datasets. This is useful to recommend datasets that are similar to a dataset that the user may already be interested in. Moreover, when recommending AI tools, we want to make sure that we recommend tools that actually work: for instance, when the user needs a classification model, we should recommend models or machine learning algorithms that are likely to work well for the type of data that the user has.

There exists extensive literature on characterizing machine learning tasks. One can efficiently extract so-called meta-features15 that measure statistical properties of the data. More recently, task embeddings based on deep learning are prominently used in industrial applications16. To also recommend the best AI tools for those tasks, one can benchmark many tools on many datasets to collect performance meta-data, and build meta-models that then predict the best tool for any new dataset17. More recently, meta-learning has also been used in combination with automated machine learning (AutoML)18. In this scenario, a range of candidate models is proposed by a recommender, and the user evaluates these candidates on their dataset, usually with an intelligent optimization technique so that the best model is found as quickly as possible.

Our ambition in StairwAI is to leverage these techniques in a much more general way to recommend datasets (tasks) and AI tools to end users given a task description. There are many additional challenges here, such as the likely incompleteness of the task description, a much wider range of tasks than typically considered in earlier literature, and additional constraints such as having very limited time to return a recommendation.

Vertical Matchmaking and interoperability with the Bonseyes platform:
The key expected collaboration points of Bonseyes Market Place (BMP) within StairwAI include:

* Integration of base community layer (create, discover, search, deliver);
* BMP services accessible through AI4EU platform (experimentation page);
* Develop new services on BMP visible on AI4EU;
* Analysing possible strong integration / merger between the two platforms.

Constant interaction and integration work will be examined during the StairwAI project to expose these new services layer to the AI4EU community.

The project will ensure alignment between its effort and the AI4EU roadmap. The expected impacts of the BMP project are to: Enrich AI-on-demand-platform, and to boost the deployment of AI-based solutions and services. It is expected that the StairwAI project will define and develop new services to be exposed and accessed by the AI4EU community with a focus on industry real-world application services necessary to resolve industry challenges. One of this is with respect to edge intelligence solution deployed on low-power edge cyber systems where BCA has its strong focus. Thus, the BCA will favour on the expertise of expanding its community and platform to the targets of the AI4EU project on broader scope beyond machine learning applications. Furthermore, the StairwAI and AI4EU projects will benefit from the experience and software and methodology already proven within the frame of Bonseyes project.

Vertical matchmaking requires the profiling of new hardware and software and also support open choices for users to select the best solution for their challenges in an automated way. HUA will provide state-of-the-art hardware and software in this space, including its Ascend-based Atlas hardware platforms and open source MindSpore AI framework. HUA will support the profiling and configuration of these solutions for vertical matchmaking, alongside other hardware and software.

AI hardware and algorithms are rapidly evolving, hence it will be needed a hardware marketplace that spans a wide variety of platforms across cloud, edge, and devices. Additionally, this marketplace will need to support a variety of AI algorithms across these different hardware platforms. To date, there is no single marketplace that provides access to such variety of solutions. 

HUA has developed state-of-the-art industry leading AI and computing platforms and will contribute these into the open marketplace. For example, HUA will provide its Ascend NPU-based Atlas hardware platforms and open source MindSpore AI framework.HUA will support any alternative AI framework and algorithm to run on its Atlas platforms. MindSpore will run efficiently across the different hardware platforms available in the marketplace so that there will be an open choice of hardware and software in this marketplace. Additionally, HUA will provide supporting cloud, edge and HPC platforms in the context of the open hardware marketplace.

The EGI e-Infrastructure is the largest distributed computing infrastructure of such kind in the world, and brings together hundreds of data centres worldwide and also includes the largest community cloud federation in Europe with tends of cloud providers across most of the European countries offering IaaS cloud and storage services. The current federated resources represent altogether more than 500 Petabytes of online storage and 500 Petabytes of archival storage supported by approximately 1M Cores. EGI expanded the federation of its facilities with other non-European digital infrastructures in North America, South America, Africa-Arabia and the Asia-Pacific region, as such EGI fully realise the “Open to the World” vision. EGI will contribute to the creation of StairwAI hardware marketplace, and provide requested Cloud resources to support AI4EU community.

Integration in the AI-on-demand platform
The StairwAI project will ensure alignment between its effort and the AI4EU roadmap. The ambitions of the StairwAI project are to enrich AI-on-demand-platform boosting the deployment of AI-based solutions and services on different hardware infrastructures: edge, cloud and HPC.

It is expected that the StairwAI project will define and develop new services to be exposed and accessed by the AI4EU community with a focus on industry real-world application services necessary to resolve industry challenges, in particular with respect to edge intelligence solution deployed on low-power edge cyber systems. The AI4EU project is focused on a broader scope beyond machine learning applications; also it is limited to the community layer functionalities rather than the industry facing value-added services, which are the focus of StairwAI.

The key expected collaboration points have been discussed and will be on the roadmap for StairwAI. StairwAI will define and develop new dedicated services for the AI community and intends to provide the necessary resources for the validation and integration of these new services in the AI4EU platform. This will include developing services around Matchmaking for talent (horizontal matchmaking) and between challenges, models and deployment platform/hardware (Vertical matchmaking). 

The interoperability with AI4EU will be sought across the various functions: 
* Profile and identity management (single sign on)
* Search function (bidirectional)
* Access to asset/resources catalogue
* Exchanging components with the other platforms such as the Bonseyes platform which will also integrate the new services developed by StairwAI.

API definition and proposal
In order to enable interoperability and add new services on the AI4EU platform, StairwAI will define and propose a dedicated API to this effect. Such an API will be developed by the project and will ensure security and fairness of resources allocation, while preserving platform integrity. This managed API will be described and documented in the work package for architecture (WP2, T2.4 and T2.5). An API could also be provided to allow new services developed by StairwAI to store and retrieve data on AI4EU related to these new services.

Constant interaction and integration work will be examined during the StairwAI project to expose these new services layer to the AI4EU community. In order to ensure alignment with AI4EU in terms of interoperability and integration, StairwAI will nominate and propose representatives to existing AI4EU bodies including: 

* Architecture committee: A body tasked with helping define the technical and functional architecture of the AI4EU platform and to assist in achieving its high-level goals and evolution.
* Industrial AI Alliance: A group of European companies that are developing a community that are engaging with the platform on the needs and concerns of industry as they relate investment, legislation, and technical development.
* Foundation: The organization that will examine and propose the long-term sustainability of the AI4EU Platform and its community.
 
b. Innovation potential
The StairwAI project has a potential to innovate along four lines:

AI technology development: By creating an AI-based service layer to ease the access to AI and push its adoption. These services will be integrated into the AI-on-demand platform but can be also exploited as stand-alone components. 

Low tech sector: StairwAI will provide innovation potential to strengthen low-tech SMEs by enabling the AI adoption by 60 low tech SMEs through the open calls and by providing success stories and good practices for applying AI solutions to business cases. Many companies nowadays foresee some innovation potential but do not have neither resources nor competencies to put them in place. StairwAI will provide a Stairway to AI for these actors and push the uptake of AI solutions to generate value.

Hardware providers: StairwAI will connect a flourishing ecosystem of hardware providers to low-tech SMEs for enabling their AI uptake, for dimensioning the needed resources for running a given algorithm on specific applications. This development has a tremendous innovation potential as it will open up new possibilities of finding new customers and for efficiently managing resources and infrastructures through the Bonseyes marketplace and business models.

Replicability of results. StairwAI fosters the replicability of its solution in SMEs even if they are not funded with FSTP due to the automated matchmaking and the sustainability through DIHs. DIH could adopt the StairwAI model and use its services to connect user needs with the AI-on-demand platform and to ease the AI adoption. 

2. Impact
2.1 Expected impacts
Contribution of StairwAI towards the expected impacts listed in the call topic
From the work-program: Enriching and optimising the AI on-demand platform service offer and reinforcing its sustainability

To support and reinforce the sustainability of the AI on-demand Platform, the StairwAI software stack proposes an innovative and potentially disruptive set of services. These services – NLP service, enhanced semantic AI assets representation, Vertical Matchmaking and the Horizontal Matchmaking, as described in Section 1, enrich and optimise the AI on-demand service offer by their own. Next, a list of expected impacts as a direct outcome from these services are listed and detailed according to the sustainability and enhanced use of the AI on-demand Platform:

Impact #1: An increase of the Quality of Experience for end users through an easy interaction with the AI on demand Platform 

Impact related to the Objectives TO1, TO4: Natural Language Processing service offers an opportunity to end users and companies that are not familiar with the AI technologies – or technology in general – to uptake the AI on demand Platform. These users have an excellent opportunity to employ AI technologies to improve their business. Some of the main technological gaps in which AI could help them in their business are in areas of interest, consultants finding, potential employees, tools, training, software applications, and expertise. To facilitate the enrichment and to attract more users, we recognize that language barriers can be an issue for many companies, mostly for those that do not have IT expertise. 

This means that an interaction in the native language of the user is extremely helpful for overtaking the digital divide for end users (including low tech ones). During the course of the project StairwAI will target at least 5 language with higher impact from a low tech users perspective – in different sectors-, with the ambition to extend to all European languages after the end of the project with the support of other actions, or projects during or after the end of the project. The platform resources are an enormous asset for boosting the European industrial ecosystem, but they need to be accessible and communicate to all end users.

Impact #2: An increase on AI-on-demand Platform users’ interaction and AI-asset discovery
Impact related to the Objectives TO2, TO4, SO1: The Horizontal Matchmaking will increase the number of experts within  the platform and will enhance the interaction between them, improving cooperation, and to showcase their interests and monetise their free time as consultants . Therefore, the horizontal matchmaking mechanism enables users to access a smart search engine for AI assets discovery, to enable cooperation with experts and consultants where they can discover on-line course material, and software component. Mechanisms for avoiding fraud or bias in these environments is crucial and StairwAI will enable mechanisms to satisfy this objective. A reputation mechanism, including end user feedback will ensure the high-quality assets and experts through a positive reinforcement mechanism to become a referent on European high-quality standards for AI. 

Impact #3: An increase on AI assets deployment – AI software services, algorithms, tools 
Impact related to the Objectives TO3, TO4: A main reason why generally SMEs and companies do not use new algorithms or AI services  is as a result of a  lack of expertise in  resource management to carry out, deploy/test them, or due to the lack of hardware resources. Current resources required to deploy the AI tools are quite heterogeneous, switching from Cloud, HPC, to edge and embedded systems. 
StairwAI will construct an ecosystem enriched with resource providers underlying the AI on Demand Platform by supporting the end users on their hardware needs Hence, the Vertical Matchmaking will help end users on resource planning to access hardware resources adequately dimensioned. This will increase the AI assets consumption as they will be easily deployed and within the boundaries – preferences of time, costs, others - defined by each end user. 

Impact #4: A self-sustained mechanisms for long term support the AI-on-demand platform – community and infrastructure 
Impact related to the Objectives TO3, TO4, EO1, EO2, SO2 To enrich the community of the AI on demand Platform and therefore to strength the AI market in Europe, SMEs and low tech SMEs, independently on the sector, must turn into AI consumers. Thus, it is essential that the underlying technology need to be access easily, widely understood, and largely uptaken. For that reason, the StairwAI services will provide a stairway to reach AI – resources and expertise -, for all these users lacking expertise or time, lowering the technological barriers for them. This means having them as the new AI-on-demand community, to understand their needs (TO1/TO2), to identify the adequate technologies and tools that can boost their business, to support them through top AI experts and consultants, but most importantly, to make the AI community to expand and grow richer in Europe. 
Furthermore, (EO1/EO2) a hardware marketplace will support several hardware providers to expose their resources, also favouring the monetisation and expansion of the companies and SMEs in Europe in this sector. (SO2) Similar approach will be taken here than in the Horizontal matchmaking with the experts, in which mechanisms of reputation and bias will be applied to avoid negative impact and allow all participants to have a fair contribution to the system. Thence, providing hardware support to ease the deployments of AI tools, a self-sustainability plan for the AI on demand Platform is enriched, with multiple resource providers and heterogeneous hardware resources. 
Easing proper access to well-dimensioned hardware will activate commercial scale up and sustain the AI on demand Platform infrastructure in mid and long term. This, in turns, (SO2) will incorporate to the arena hardware/resource providers, giving fair opportunities to SME in the sector though fair mechanisms, making them easily accessible, and reinforcing Europe´s IT competitiveness

From the work-program: Boosting the deployment of AI-based solutions and services, enabling a larger user community to recap the economic benefits of AI, especially SMEs and non-technology sectors 

StairwAI design makes its services and benefits suitable for any European Stakeholder, however, special focus has been made in the design and actions in order to incorporate low-tech users to access to the AI on demand Platform in similar conditions as experts users, with independence to their market segment. The SMEs need to ease their access to the system, but also need to have a seamless perception and knowledge on the different European Initiatives on AI and to find the channels for a better development. Following this reasoning, the next Impacts are depicted as consequence of both, widening the fair incorporation of low-tech-SMEs without disfavour any segment, and the efforts from StairwAI on having an alignment with the European AI Initiatives and the AI-on-demand Platform


Impact #5: An enrichment of the AI on demand platform with SMEs and low-tech SMEs from diverse market segments

Impact related to the Objective EO1: StairwAI targets SMEs in any sector - manufacturing, energy, healthcare, agriculture, food and services, education – to make of AI techniques a great improvement on their production processes. Foremost among these are: the maintenance of plants and facilities, the logistics, human resource management, and planning, training, dimensioning, and getting knowledge of their customers, clients, or businesses. In general, the added value to companies and SMEs is broad in any market vertical, but especially, for those small and medium companies that need to digitally develop.

Impact #6: A seamless view for SMEs and low-tech SMEs on European initiatives, Digital Innovation Hubs and AI-on-demand Platform Ecosystem

Impact related to the Objective EO1, EO2, EO3: The work and interaction planned with DIHs and the open calls; managing cascade funding will guarantee a proper connection with low-tech users, especially SMEs and their guidance toward AI technologies, thus creating best practices and use cases that can serve as lighthouse for future platform use and uptake. Furthermore, hardware providers will be involved in the cascade funding open calls to provide dimension and the proper resources for running the AI assets. 
Moreover, StairwAI will strength the European activities on AI by linking and synchronising with the action plans in tandem with the ones developed in the AI4EU ecosystem and the European AI panorama. The AI4EU ecosystem, which encompasses: the ICT 48 RIA’s linked by the ICT 48 – CSA Vision- where Thales, coordinator of AI4EU, and University College of Cork, are involved along with the ICT 49 projects. The AI PPP, BDVA, EurAI, Eurobotics, among others. StairwAI will cooperate with all these, aligning the activities described in this proposal to maximise the impact of the objectives. 

Other impacts outside the work program


StairwAI’s vision is a European economy that thrives and leads the world in the AI adoption of tools. Our mission is to advance this adoption by building a service on top of AI4EU, based on the principles of transparency and ease of use. Our goal is to demonstrate that this approach will lead to significantly enlargement of the AI community users namely specialised experts on the field, large companies, SMEs in diverse sectors – tech and low-tech sectors, and public bodies; beyond to provide training and mentoring to a limited number of companies. 

The self-sustained model of StairwAI from experts mentoring to the resource providers (hardware) marketplace will attract most of European Communities and sectors towards the ecosystem build by the European Commission on top of AI4EU, ICT 48 and ICT 49, among others. StairwAI will address not just technological concerns, but business, economic and societal on a robust technological foundation. The remainder of this section address these impacts, linking them to the objectives, detailing how the impact will be address by StairwAI, and the performance indicators to measure success.

In addition, StairwAI will favour to unite the marketplace of AI4EU with other initiatives like EOSC-hub, H-CLOUD or GAIA-X, by creating an interoperable hardware support for the companies to provide Cloud, edge, HPC and cyber-physical systems. 

The need for the StairwAI is clearly evident. It can be seen how companies, like the German Supermarket Chain Lidl - Schwartz IT19 - is creating their own services as rival to Amazon Web Services for the European market. There exists a clear gap in the market, also demonstrated by the emergence of the French-German initiative GAIA-X and from the European Commission’s EOSC programme.  

On the demand side, other projects and initiatives like FIWARE in Smart Cities, the AI European on demand platform (AI4EU), in cyberphysical systems (Bonseyes), or  OpenML for Machine Learning, have created catalogues of shared services but lack resource support or are limited in terms of sustainability. 

Furthermore, it is evident that in the case of some Digital Innovation Hubs support is required on expertise and resources to make regional SMEs and to low tech SMEs to develop in the digital single market across European Regions. 
StairwAI has clear those aspects and will make clear contributions to them. Next a market analysis and a comparison of the needs with key aspects developed in StairwAI is described. 

Strengthening industrial competitiveness, growth and sustainability of European stakeholders
 
The utilization of AI in most of the market segments drives the European computing industry into a very competitive world-wide ecosystem, while at the same time it is arguably one of the biggest technology shifts; AI technologies are a very important enabler for economic growth, providing companies with unique business opportunities and skilled workers many new job opportunities. 

The vision of the European Commission is to strengthen the position of Europe in Artificial Intelligence. It is difficult to say the amount of revenues that AI techniques will produce in the upcoming years, but it is expected to reach a value by 2025 of $51.9bn in computing infrastructure (Edge AI)20, $13bn in global healthcare21, $24bn in Security market22, or $10bn in Supply Chain market23. 

However, in order to address these Innovation triggers at the speed required to position Europe as a market leader, it is necessary adopt solutions such as those presented in StairwAI. Thus, companies and SMEs – even the ones lacking the requisite skills – will have easy access to expertise, AI assets, and computing support from the European AI on-demand Platform.


Trend #1: Artificial intelligence: Automation using AI will be involved in any aspects within I&O (regardless of what the I&O data are themselves). AI is the only path forward to enable IT growth at required speed. Europe has relevant players that cover the vertical chain of value from hardware resource providers to application. The integration of AI Assets with Cloud, HPC, and Cyber-physical resources demand for advanced artificial intelligence techniques is becoming challenging as no longer is it an effort that can be tackled within a single industrial organization. StairwAI will provide innovation across companies covering hardware, hardware - software interaction, software, and AI. 

Trend #2: Death of the data centre: Gartner predicts that by 2025, 80% of enterprises will shut down their traditional data centres. In fact, 10% of organizations already have. An enabler to this is data privacy and increasing security, as underlined by the EU laws: The General Data Protection Regulation (GDPR) and the Directive on Security of Network and Information Systems (NIS Directive), both of which came into effect in May 2018. StairwAI will support across enterprise, industry, and national borders the implementation of challenging policies that enforce data ownership, privacy, and security since regulations that govern the use and movement of data may change when the data moves. In addition, StairwAI software stack will increase the number of companies and SMEs using cloud services by constructing secure mechanisms compliant with laws and also with the Trustworthy AI Guidelines, in which one of the partners involved in the proposal Prof. Barry O’Sullivan (UCC) is the vice chair of the High-Level Expert Group on AI.  


Trend #3: Digital diversity management on assets to strength the European Market. This can be seen in European initiatives and projects such as: AI4EU, FIWARE, Bonseyes, or OpenML, having marketplaces available to enterprises, public bodies and researchers. However, a key challenge of these platforms is to have a robust sustainability and physical resources for SMEs and researchers to execute the AI assets there located. StairwAI will enable a sustainability plan for these European Initiatives, acting as integrator, and providing mechanisms to the platforms supporting assets and marketplaces. 


Trend #4: SaaS Denial as most professionals focus on IaaS and PaaS. StairwAI will provide a service linked to a hardware support that will enable a SaaS environment for the assets and algorithms allocated in the AI4EU platform, promoting to SMEs and companies to gain more trust and confidence on SaaS.


The StairwAI project comprises numerous entities that are involved at different levels across the project organisational hierarchy, namely:
1. Consortium level and partners involved in open calls: actors with activities in the project whose continuous collaboration and full commitment is expected;
2. Network of stakeholders: organisations that, without being partners in the project, have expressed their deep interest in this initiative and will participate to key events and General Assembly meetings;
3. Interested audience: relevant audience that StairwAI partners will keep aware during the project and beyond.

To ensure seamless collaboration across these three levels and to maximise the impact of the project, a strategic dissemination plan and an exploitation plan will be developed and updated as the project progresses, being key deliverables of WP8. The actions of these plans span across several WPs and finalise with a post-project plan in order to ensure the continuation of the StairwAI services and their integration in the AI4EU platform. In the following, both plans are described in detail.

The main project results to be disseminated and exploited, during and beyond the project, to maximise StairwAI impact are:
 
* Service layer in the AI-on-demand platform. The main exploitation result is the service layer that will be developed and integrated in the AI-on-demand platform (WP2) and will be used to accelerate and ease the adoption and uptake of the AI technology. All services can be also exploited as stand-alone components along with their corresponding methodology.


* Multi-lingual chatbot: This component can be exploited as a stand-alone component for interacting with low-tech users in their own language (WP4) and suggest them proper AI techniques and expertise that are needed in their business.


* Horizontal matchmaking and AI asset organization and discovery This component can be exploited also as a stand-alone component. It provides an organization of AI techniques in an ontology (WP3) so helping categorization and a matchmaking engine (WP5) for the classification of user needs into the proper classes. 


* Application benchmarking and hardware dimensioning This component is very general and can be applied to any application and hardware platform, provided we have data on application profiling on specific hardware. The vertical matchmaking (WP6) enables the definition of a proper hardware platform given an application to be run or the selection of a proper algorithm given the hardware and temporal constraints. 


* StairwAI low-tech sector showcases This is the project output that encapsulates the value of the ecosystem of practitioners from DIHs and low-tech SMEs that will be reached in WP7 and WP8 that will provide low-tech sector use-cases. These will be published on the platform as success stories and examples on how AI can be beneficial for different verticals to push its adoption further. The project partners will explore ways to keep this alive beyond the project, mainly through third party funding, an outline of the Business Model foreseen has been included in Section 2.2.(a).2.


Stakeholders and main audience

1. Low Tech SMEs: the StairwAI primary focus is to enable easy access to AI tools for low-tech companies. These SMEs will be approached not only through events, but also through SMEs incubators/accelerators and EC specific platforms such as I4MS (ICT Innovation for Manufacturing SMEs, http://i4ms.eu/) and the DIH ecosystem. SMEs and companies in manufacturing, energy, food, agriculture, health will be approached during StairwAI in fairs/exhibitions and will be the target of events, ad-hoc meetings, press releases, newsletters, etc.


2. Distributed edge/cloud, and HPC actors: including all types of organisations, but with a specific focus on SMEs and research centres/universities providing hardware resources and services, through the following channels:
a. Events on ICT at a national level/EU level;
b. Industrial sector associations’ events;
c. Scientific congresses and publications in scientific journals;


3. Open innovation initiatives: these stakeholders will be approached in events, such as congresses, open days, fairs, etc. the following dissemination tools will be used:
a. Participation in scientific and AI-related conferences introducing the new services and the European AI-on-demand platform;
b. Publications in scientific journals;
c. Participation in fairs and open days related to open innovation;


4. Broader scientific community (including universities and R&D centres, R&D teams, PhDs/post-doc candidates, etc.). The most important tools to create awareness will be seminars/webinars within Universities and research centres involved in the consortium. Ph.D. and post doc will be enrolled during the project;


5. European platforms and agencies: StairwAI results will be disseminated through participation in key events (conferences, brokerage events etc.), encouragement of knowledge sharing in the platforms’ communities and communication through partners already members of such platforms. Events related to the following organisations could be selected:
a. Big Data PPP (BDVA) and the subsequent AI, Data and Robotics PPP: with the following StairwAI partners as members: UNIBO, EGI, HUA
b. ETP4HPC: with the following StairwAI partners as members: UNIBO 
c. EGI European Grid Infrastructure is a partner of the project and connected to a network of hardware providers
d. ELG European Language Grid with the StairwAI partners TILDE as consortium member.
e. European Open Science Cloud projects, EOSC-hub and its follow-up project (under the call INFRAEOSC 03): with the following partners as members: EGI.


6. Policy makers and EC directorates: direct contact and networking with relevant departments of the European Commission, such as DG ENER, DG DIGIT, DG RTD, DG MOVE, DG CLIMA, DG ENV, etc.
7. Standardisation bodies: CEN and CENELEC will be kept aware in related events;
8. Regional/national authorities and the general public: approached through national press releases, pilots’ visits/workshops, summer schools, etc.

The StairwAI website will be a key tool to create awareness at all levels, including project’s news, newsletters, PhD positions, job offers, etc. (see specific activities in the next section). It will be tightly linked to the AI-on-demand platform.

The IPR strategy will follow three main aspects to maximize impact and protection of the knowledge created within StairwAI: 
* Proper assessment of the background knowledge and contribution to the innovative results;
* Evaluation of the new knowledge created during the development of StairwAI, assessing the state of the art and continuously conducting technology monitoring and patent searches;
* Overall IPR strategy and assessment of the best route for the protection of the knowledge created.

To support the assessment of knowledge development throughout the project, FBA will conduct a background knowledge assessment at the mid-point of the project life cycle. A further assessment to monitor achieved results will be conducted at month 36.

Based on the results, the most suitable solutions to protect the results will be analysed. Such solutions will depend on the nature of the result, as well as its TRL, and can include patents, trademarks, design rights, copyright, and trade secrets. This strategy will allow to better define exploitation routes and dissemination activities, leading to a shorter time-to-market of project results. Companies’, Universities’ and technology institutes’ transfer offices will assist each partner in ensuring that IP is appropriately protected and exploited. The knowledge management and protection policy for StairwAI’s results will be laid down in the Consortium Agreement.

 The StairwAI Consortium will ensure open access to all peer-reviewed scientific publications resulting from the project. Open access for scientific publishing will be granted for all publications resulting from StairwAI including on-line access to scientific information, free-of-charge to the end user and, as far as possible, re-usable. Open access includes the right to read, download, print, copy, distribute, link, crawl, and mine. StairwAI partners will reserve funding on their budgets for accomplishing open access, thus shifting the publication payment costs from readers to authors. 

All scientific publications will be stored in an online archive and a list of publications will be made available on the public project website. StairwAI partners will primarily target in publishing in journals with gold open access, providing articles immediately in an open access mode. Authors will target to retain their copyright. All consortium partners will reserve funding in their budgets to achieve open access publications.

The project will ensure open access not only to all peer reviewed scientific publications regarding the project results but also to the research data. Therefore, the project will deliver a Data Management Plan (DMP) that will cover the actions to be taken to make the project research data FAIR. 

The DMP will explain which methodologies and standards will be used in data creation and processing; how the research data will be handled and shared during the project and how they will be deposited, preserved and accessed after it is completed; which metadata, licenses and documentation will be associated to the data sets; how and when the dataset will be made openly available for re-use; the costs and the allocation of resources; the ethical issues to be addressed. In case parts or versions of the project data cannot be openly shared, the DMP will provide proper motivations and data availability statements.

The DMP will be a dynamic document delivered under the leadership of the Coordinator in cooperation with WP leaders and the research partners. It will be progressively updated in order to cover the entire life cycle of all the data sets of the project. The project will deliver a first DMP within the first six months of the project (M6). Updated DMP versions will be delivered at mid-term and at the end of the project in time for the periodic reviews of the project, unless otherwise agreed with the EC project officer.

 Moreover, new versions of the DMP will be produced whenever important changes in the data or data management policy may occur. The project will use both primary data generated in the course of the project and secondary data from existing sources. It will use qualitative and quantitative methods. The data will be formalised in structured data sets for the purposes of elaborations to be carried out in the project. They will be used for achieving project objectives and related scientific dissemination activities, including follow up scientific activities.

What types of data will the project generate/collect? There will be different data collected: data coming from the FSTP, data collected for training the multi-language chat-bot in Wp4, data collected for training the machine learning models for the horizontal and vertical matchmaking in WP5 and WP6.

What standards will be used? No specific disciplinary-driven standards can be identified for the type of data used and produced in the project. The project will develop a common protocol to collect data to ensure data quality, and guarantee comparability and homogeneity. Standards will be used for descriptive metadata and will be applied in the archiving procedure of the data in the chosen repositories.

How will this data be exploited and/or shared/made accessible for verification and re-use? The project data will be deposited in public or institutional data repositories and made available in open access at the publication of results. At the latest at the publisher’s acceptance of the manuscript, the underlying data will be deposited in an Open AIRE-compliant data repository to facilitate the cross-linking between the data set and the scientific publications and guarantee maximum visibility to the project results. The partners that generate the data will be responsible for their quality and compliance with H2020 Open Research Data requirements (FAIR data).

How will this data be curated and preserved? Every partner will be responsible, in the course of the project, for the secure storage of the generated data in reliable facilities that perform regular back-ups, permit recovery and do not compromise their integrity. At the project end or at the publication of the project results all research data will be archived in trustworthy public or institutional data repositories that ensure data security and long-term preservation. The DMP will include details for data maintenance and curation after the project completion.

How will the costs for data curation and preservation be covered? Costs for long-term preservation and availability of the project data are covered by the institutions providing the repository infrastructure.

Ethical issues. The data collected will never include information raising ethical issues (e.g. health related), but may include personal data related to expertise and skills. For this reason, the minimization of data will be guaranteed, and the sharing of data within the consortium will take place with non-disclosure of individual data (thanks to anonymization or pseudonymization). Anyways, data collection and management will be carried out, by each partner data controller, according with the related laws in place in the countries where data are collected. Additional cases will be regulated, during the project lifetime, according to the Grant agreement and the Consortium agreement.

StairwAI will deploy a 360 degrees Communication strategy combining online and offline channels, content marketing strategies, online marketing tools, growth hacking techniques, analytics tools, media relations, advertising campaigns, PR 2.0, agreements with top events and SMEs institutions and work with stakeholders and influencers among others efforts.  


In order to reach the communication objectives, at qualitative and quantitative level, the project will follow the "growth hacking funnel" approach 

"Growth hacking" can be defined as a process of rapid experimentation across promotion channels and product development to identify the most effective, efficient ways to grow a business. 

Growth hackers are marketers, engineers and product managers that specifically focus on building and engaging the user base of a business. Growth hackers often focus on low-cost alternatives to traditional marketing, e.g. using social media, viral marketing or targeted advertising instead of buying advertising through more traditional media. 

In order to reach the stakeholders StairwAI will use an impressive combination of channels:
* Partners’ ecosystems (meaning by that SMEs communities directly linked to partners and Cluster, Industrial Associations and other SMEs communities which usually cooperate with partners). Spreading then, the project, all across Europe thanks to  Partners' Networks and their area of influence (most of them National or European Networks aimed to promote, directly or indirectly, the CE approach).
 
* Assistance to events (fairs, exhibitions, scientific congresses) StairwAI will also be represented in Top Trending events and in events attended by our partners. 
* Public website: AI4EU will be used as public website having the specific space envisioned to the ICT 48 and ICT 49 for detailing project objectives, highlighting results as well as providing a public repository of open access publications. Other multimedia materials (photographs, PowerPoint presentations, videos) will be integrated with the AI4EU repositories generated during the project;

* Social web-networking through dedicated accounts and groups in social media like Facebook, Twitter and LinkedIn in order to promote StairwAI objectives and results across the audience already identified; 
* Paid Ads advertising in Google AdWords, Adsense and Social Media like Facebook will be used to attract applicants to the open calls.

* Organisation of European thematic workshops for the dissemination of the knowledge and the exploitation of the project results in major European events aligned to the Networks of Excellence of ICT 48 and in cooperation with other projects or initiatives linked within the AI4EU related projects. These workshops will be thematic and will be aligned with main interests of the StairwAI stakeholders target.
* Scientific publications in peer-reviewed journals and presentations in relative thematic conferences and seminars. Special attention will be given to events related to AI and its applications.

All this will weave a spider web, as detailed hereafter, which guarantees that StairwAI  results will reach to all around Europe, although the biggest effort will be initially done in the Regions participating in the project:


StairwAI will be implemented over 36 Months and will be articulated into 8 work packages. One Project Management WP, a requirement collection WP covering the collection of technical requirements, integration and coordination with the AI4EU platform requirements, three WPs concerning the development of the three services described in the concept and methodology section, namely multi-language interaction, horizontal and vertical matchmaking, , one WP dedicated to exploitation and innovation management including cascade funding and one dissemination and communication WP, all described as following:

WP1 – Project Management (leader: UNIBO): this work package has the goal of coordinating and managing the activities of the project partners, making decisions about the development of the project.

WP2 – Technical Requirements, service layer design and integration with the platform (leader: EGI): the main objective of this work package is the collection of technical and integration requirements and the design of the service layer.

WP3 – Competences and knowledge organisation (leader: UPC): this work package is devoted to the knowledge infrastructure organizing the platform content into categories, namely the design of the ontology and the organization of AI assets in the ontology classes. It also organizes the collection of training data for WP4 and WP5, and the collection of Open calls data from WP7.

WP4 – Multi-lingual interaction with the platform (leader: TIL): this work package addresses the development of the multi-lingual interaction service that will provide access to the AI assets by using natural language and in the user own language. The service will extract from the text proposed, relevant user needs, in terms of AI-assets, people and expertise.

WP5 – Horizontal Matchmaking - reputation and fairness (leader: UNIBO): this work package addresses the development of the horizontal matchmaking service that will match user needs with AI assets, experts, consultants. The service will develop also a reputation enforcement mechanism ranking AI assets according to the user feedback. Also, fairness should be taken into account in the matchmaking so to avoid biased choices. Integration of horizontal matchmaking services in the AI-on-demand platform is also considered in this work package.

WP6 – Vertical Matchmaking and hardware marketplace (leader: BCA): this work package addresses the development of the vertical matchmaking service that will match AI algorithms with hardware resources given user defined constraints on time and cost bounds. Hardware resources that are managed are embedded and IoT devices, cloud computing and HPC resources. Integration of vertical matchmaking services in the AI-on-demand platform is also considered in this work-package.

WP7 – Open call management (leader: FBA): this work package manages the exploitation of the services developed from early adopters and from hardware resource providers. Cascade funding and open calls will be managed inside this work-package that will consider both low-tech users providing test cases for the services developed. Innovation and long-term sustainability are addressed in this WP.

WP8 – Outreach activities and long-term sustainability – dissemination and communication (leader: UCC) Communication and dissemination of project results are managed and carried out in WP8. Industrial and scientific/technological dissemination activities (videos) and events are foreseen such as technical workshops, open days in partners premises. 

The StairwAI implementation plan is divided in 5 phases:
* Phase 1 – M1 to M6: Initial requirements - During this phase, all initial requirements are collected for the development of the different components. In addition, the Exploitation and dissemination plan will be ready aligned to the requirements and main stakeholders targets and objectives, the data management plan, and the preliminary evaluation on trustworthy AI use in the components of the project.

* Phase 2 – M7 to M18: Development - During this phase, the development of most of the components will be executed until reaching a prototype able to support the early adopters call. During this phase in M10 – if it is not extended due to pandemic delay – the AI4EU will end. Thus, by that time, all integration plans will be already designed and aligned to service layer design, and platform integration in WP2.


* Phase 3 – M19 to M24: Early adopters – In this phase the first set of open calls will be launched and starting to be engaged with the project. These SMEs and low tech SMEs will provide necessary input to refine the development of StairwAI. In addition the outreaching activities will be in peak with the dissemination of the open calls and with the engagement with other communities and projects.

* Phase 4 – M25 to M30: First Pilot – In this phase the second adopters of StairwAI will be providing more inputs for finalization of refinement of development. WPs in charge of development WP3, WP4, WP5 and WP6 will finalize mainly all their activities. 


* Phase 5 – M31 to M36: Second Pilot and Sustainability – This last phase will close the project by providing a seamless engagement with the last open call. The engagement and dissemination activities will be in peak, and the integration with the AI4EU platform will closing the last operational activities.

The Consortium is aware that management activities are extremely important for the successful realisation of the project as well as for a transparent accountability of the European contribution. Management activities are needed to ensure that the overall project will work as a whole and will be greater than the sum of its parts. The following paragraph details the planned management structure, the distribution of responsibilities, communication flow, decision-making procedures and conflict management.

The management organisation is composed by the following bodies:
a) The Coordinator, Michela Milano (UNIBO), will be ultimately responsible for the overall project coordination and acts as intermediary between the Consortium and the European Commission;
b) The General Assembly (GA), chaired by the Coordinator, composed by one member from each partner institution, will be the steering and decision-making body;
c) The Executive Board (EB), chaired by the Coordinator and composed by WP leaders,is in charge of supervising the activities carried out in the respective WPs, will be the executive body;
d) The Project Manager will be appointed within the European Research & Innovation Office of UNIBO, He/she will assist the General Assembly and the Coordinator in all administrative and financial duties;
e) The Technical Manager, Gabriel Gonzalez-Castañé, will assist in coordinating the technical and scientific work and provide guidance to the consortium and Coordinator.
f) The Risk Manager, Jean-Marc Bonnefous (BCA), will assist the General Assembly and the Coordinator in the assessment of risks and in the definition of proposals for remedial actions; 
g) The Communication and Dissemination Manager, Barry O’Sullivan (UCC), will assist the General Assembly and the Coordinator in the communication and dissemination activities;

h) The Innovation Manager, Anna Dymowska (FBA), will assist the General Assembly and the Coordinator in the innovation related activities (exploitation);
i) The Advisory Board (AB), external body comprised of relevant stakeholders, will provide feedback and advice as required. A major input will be about the usability of project results. 
The project will be coordinated by the University of Bologna, Italy, which has considerable experience in the management of National, European and International research projects.

The Coordinator will be ultimately responsible for the overall coordination of the Project, especially but not limited to the scientific point of view. The Coordinator shall:
* monitor that the action is implemented properly
* act as an intermediary between the Beneficiaries and the European Commission in order to keep constantly informed both the EC about any aspect that may affect the work progress and the Consortium on EC indications;
* request and review any documents or information required and verify their completeness and correctness
* receive the financial contribution from the Commission and ensure the timely transfer of shares to the Beneficiaries;

* supervise the scientific, technical, financial and administrative progress of the Project;
* submit to the Commission reports, Project Deliverables and financial statements prepared and duly certified by the Beneficiaries;
* keep accurate records identifying the budget share allocated to each Beneficiary and inform the EC of the distribution of funds and the date of transfer to the partners on an annual basis;
* organize and ensure appropriate communication among the partners;
* Chair the meetings of the General Assembly and the Executive Board.

The Coordinator will be supported by the Project Manager, the Risk Manager the Innovation Manager and the Communication and Dissemination Manager and, as for matters of general relevance, by the General Assembly.


The General Assembly (GA) consists of one representative from each partner and is chaired by the Coordinator. It is responsible for discussing the general innovation direction of the project and for ensuring the completion of the work plan within the scheduled time frame.
The GA will meet at least 6 times during the project duration: at the kick-off meeting and at least twice a year thereafter. 

In particular, the GA shall be responsible for:
* decisions concerning the work plan and its major changes;
* allocation of the budget to the work plan and any financial and budget-related matters;
* decisions with regard to any amendment of the terms of the EC contract and Consortium Agreement which should prove necessary;
* decisions concerning possible premature completion/termination of the project; 
* settling any disputes arising from project implementation;
* IPR-related matters;
* press releases and joint publications by the partners with regard to the project.
Executive Board
As far as the work plan implementation is concerned, the Executive Board (EB) will support operatively the GA. The EB will be chaired by the Coordinator and gather all WP Leaders, specified in the table below. For the tasks that require particular attention, a task leader will be also nominated by the respective WP Leader at the start of the WP. More in depth, each WP the EB shall be responsible for:
* Coordination and monitoring of the progress of the tasks included in each WP;
* Organization, collection and quality control of the deliverables;
* Coordination with WP leaders through the GA in order to ensure exchange of information;
* Informing the GA, the Coordinator and the Expert Groups of any event within their WP that may affect work schedule.

The Communication & Dissemination Manager (CDM) will be Barry O’Sullivan, UCC, and will ensure high quality communication and dissemination for the project.
The CDM will be working in close cooperation with the Coordinator and GA and will be responsible:
* to track project results and to propose to the GA the strategies for its dissemination 
* to support networking and dissemination activities, by increasing the visibility of project results within the international scientific community, European research and innovation initiatives, target users, media (Magazines, web portals, newspapers, radio and TV interviews), etc.
* to prepare the communication material and the project newsletter
* to constantly update the project website.

The Innovation manager (IM) will be Anna Dymowska, FBA and will ensure high visibility of the innovation aspects of StairwAI results. The IM will provide technical, legal and economic expertise in technology transfer and supporting guidance on IPR and innovation Management. The IM will be working in close collaboration with the Coordinator and Partners and will be responsible: 
* track innovation aspects of StairwAI results;  
* to support partners in exploring opportunities for patents.
 
To strengthen the continuous communication and feed-back with key stakeholders, the General Assembly will be aided by an external Advisory Board (AB). The AB is mainly in charge of:
* ensuring that project results are directly usable in response to the policy issue addressed and to social needs, by providing feedback on the technology developed in the project and addressing user-friendly and feasibility of the method;
* serving as the first contact point in communicating and applying the results of the project, strengthening the dissemination part of the project;

All the project partners will meet at least twice a year for project general meetings, where to sum up the activities carried out during the past year and plan activities for the coming year. General meetings shall be convened by the Coordinator with at least 15 calendar days prior notice, accompanied by an agenda. The agenda shall be deemed to be accepted unless one of the members notifies the coordinator and the other members in writing of additional points to the agenda, at the latest two working days before the date of the meeting. Minutes of the meetings shall be transmitted to the members within 30 calendar days after the date of the meeting. The minutes shall be considered as accepted by the other members if, within fifteen calendar days from receipt, no member has objected in a traceable form to the Coordinator. 
Specific meetings of the EB will be organised within project general meetings.

For further co-ordination meetings, the partners have agreed to make use, whenever possible, of the current technologies for tele-video conferencing, in order to increase cost effectiveness, while still keeping some occasions for meeting personally (a permanent conference room will be reserved for the project in the UNIBO web conference service). In addition, the consortium plan to organize technical integration meetings (among technical partners) to integrate together the different parts of the technologies developed within the project. 
A final meeting, concomitant with the Final Conference, will be held in Brussels to facilitate participation from relevant stakeholder and target audience representative thus increasing the project impacts.

The decision mechanisms in the project are planned to suit the size of the project, which last 36 months and involves 11 partners. In principle, the voting system will be kept as simple and as direct as possible. First of all, any decision requiring a vote at a meeting must be identified as such on the pre-meeting agenda. The General Assembly shall not deliberate and decide validly unless the 75% of its member are present or represented (quorum). The decisions of the GA are expected to be taken by consensus. If such a consensus cannot be reached, decisions will be taken by a majority vote, each present partner having one vote. In the event of tie, the vote of the coordinator will decide. 


StairwAI consortium consists of 11 partners and it is built to ensure the multidisciplinary skills needed to hit its outcomes maximizing impacts and stakeholders’ involvement at EU level. The consortium includes 5 higher education and research institutions (UNIBO, UCC, UPC, TUE and INFN), 1 SME (TIL) 3 no-profit organisations (BCA, FBA and EGI.eu) and 2 companies (HUA and THA) to cover the wide range of knowledge needed to pursue the challenging objectives of StairwAI. 


StairwAI aims to ease and automatize the process of supporting low-tech users in accessing the AI-on-demand platform, through a service layer that enables natural language interaction and performs horizontal matchmaking, namely an automatic mapping between user requirements into assets of the AI-on-demand platform to meet their business needs.

The research activity envisaged in the project StairwAI will be conducted applying fundamental ethical principles and relevant national, EU and international legislation, including the Charter of Fundamental Rights of the European Union and the European Convention on Human Rights, the General Data Protection Regulation (GDPR - Regulation UE 2016/679).

A fundamental principle underpinning the whole research activity is respect for the welfare (health and safety) of the participants who take part in the work, and this principle will over-ride all other considerations when the work is executed. 
Other principles are:
- participation only after informed consent
- personal data protection, anonymity and confidentiality
- fairness
- equity
- justice
- social responsibility

All research is conducted ensuring respect for the participants and their dignity, protecting their values, rights and interests and fair distribution of research benefits and burden. As a general principle, benefits are maximized, and harm/risks minimized. 

All beneficiaries accessing to the StairwAI Grant Agreement and Consortium Agreement will be responsible for monitoring and implementing the necessary measures to ensure compliance with National and EU legislation concerning the above-mentioned ethical issues. All lead investigators will be responsible to ensure that ethical standards compatible with, and equivalent to, those of H2020 will be applied, regardless of the country in which the research is carried out. The participating research groups and individual principal investigators in StairwAI are supported by experienced team in the ethics management of research studies, data protection officer (DPO) and legal staff in complying with relevant national and international laws and regulations concerning research involving and personal data protection.

Ethics Procedures are detailed as follows, with the specification that regular updates on ethical issues will be provided in project reports for the project reviews.







1. Executive Summary

This document delivers the first version of the Benchmark software framework proposed within WP6 Task 6.1 of StairwAI project. The benchmark software framework represents one of the main blocks of the vertical matchmaking service as it produces benchmarks across several HW platforms (Task 6.2) that are then used to train the vertical matchmaking engine (Task 6.3).

Section 2 provides a general introduction of the WP and summarize the scope and contributions of this deliverable.

Section 3 introduces LPDNN: the inference framework that has been used and extended to create a benchmark framework for heterogeneous HW platforms. 

First, LPDNN’s architecture is detailed, explaining the different components that make it an interoperable framework. Next, we introduce the different inference engines that LPDNN supports, which increase the portability and optimisation of neural networks across HW platforms. Then, we describe the support for HW platforms and those platforms that have been already integrated. 

Section 4 starts by explaining the elements that are used to deploy AI Applications on HW platforms. Then, the benchmark flow is explained by showcasing the execution of an AI Application and the obtention of a benchmark. Finally, the creation of a Benchmark as a Service and its integration into AI4EU is discussed.

Lastly, Section 5 elaborates the conclusions and future work on Task 6.1.

2. Introduction

WP6 has the main objective of building a Vertical Matchmaking engine that matches AI algorithms and HW resources to optimise the deployment of services and increase their efficiency. 

As such, one of the main objectives of WP6 is to develop benchmark software framework/service suitable for selected machine learning models and hardware, including CPU (Intel x86, Arm Cortex-A5x, Risc-V), GPGPU (NVIDIA, etc), and NPU (HUA, etc) accelerated platforms. The benchmark software framework will be used in Task 6.2 to produce the benchmarks and create a profiling dataset that can be used in Task 6.3 to train the Vertical Matchmaking engine’s algorithms. 

In this document, we propose LPDNN framework as the benchmark software framework. LPDNN framework was developed during the H2020 Bonseyes project (Prado, Miguel De, et al) and has been largely extended to provide a structed benchmarking layer to analyze the execution of AI Applications on a variety of heterogeneous platforms. 

This layer, on top of LPDNN, adheres to the following design principles:
* Industry-driven Research:
> Input requirements from SMEs wanting to use AI solutions
> Able to deploy AI solutions on edge devices (low-power, low-carbon footprint)

*  Structured benchmarking workflow:
> Availability of anchors within the deployment framework to evaluate the metrics truthfully
> Optimised deployment (value added to research and industry)
> Extensive documentation & Support (user friendly for SMEs)
> Defined interfaces (standarization)
> Easy to replicate (reproducibility)
> Create trust & community

Taking the previous design principles, the main contributions of this deliverable are the following:
> Introduction of LPDNN as an inference framework.
> Integration into LPDNN of three inference engines to benchmark CPU, GPU and NPU platforms.
> Integration into LPDNN of three HW platforms and update of four available platforms to the latest SW release.
> Creation of the benchmarking layer within LPDNN, containing a variety of static and dynamic metrics.
> Evaluation of the inference/benchmarking framework as Service that can be used by external users and integrated into the AI-on-demand platform.

3. LPDNN

LPDNN, which stands for Low-Power Deep Neural Network, is a deployment framework that provides the tools and capabilities to generate portable and efficient implementations of DNNs. The main goal of LPDNN is to provide a set of AI applications for deep learning tasks, e.g., object detection, image classification, speech recognition, which can be deployed and optimised across heterogeneous platforms, e.g., CPU, GPU, FPGA, NPU (ASIC).

3.1. LPDNN architecture

One of the main issues of deep learning systems is the hardship to replicate results across different systems. To solve this issue, LPDNN features a full development flow for deep learning solutions on embedded devices by providing platform support, sample models, optimisation tools, integration of external libraries and benchmarking. LPDNN’s full development flow makes the AI solution reliable and easy to replicate across systems.

AI applications are the result of LPDNN’s optimisation process and the higher level of abstraction for the deployment of DNNs[GG1][mi2][mi3][m4] on a target platform. They contain all the necessary elements or modules for the

execution of a DNN. An AI application is the optimised representation of a DNN model to be efficiently executed on a target embedded device.  An AI application contains all the necessary elements or modules for the execution of a DNN:
* Pre-processing: Step to prepare, normalize or convert the input data into the required input that is expected by the DNN.
* DNN inference: Forward-pass of the neural network. The execution is taken care of by an inference engine.
* Post-processing: Conversion of the neural network’s output into structured and human-readable information.

Next, we detail LPDNN’s architecture by further describing the concept of LPDNN ‘s inference engines and the support for heterogeneous platforms.

3.2. Inference engines

AI applications contain a hierarchical but flexible architecture that allows new modules to be integrated within the LPDNN framework through an extendable and straightforward API. For instance, LPDNN supports the integration of 3rd-party self-contained inference engines to perform DNN inference. Initially, LPDNN only supported:
* LNE: LPDNN Native Engine (LNE) allows the execution of DNNs across arm-based and x86 CPUs as well as on Nvidia-based GPUs.
During the development of the Stairwai project, the following inference engines have been integrated:
* NCNN: NCNN ports the execution of DNNs on GPU through the Vulkan driver.
* TensorRT: TensorRT accelerates the DNN inference on Nvidia-based GPUs and NPUs.
* ONNXruntime: ONNXruntime enables the direct execution of ONNX models on CPUs and GPUs.

The inclusion of external engines also benefits LPDNN as certain embedded platforms provide their own specific and optimised framework to deploy DNNs on their hardware.

More information about how to add a new inference engine in LPDNN can be found at:
* Developer guides: https://bonseyes.gitlab.io/bonseyes-cli/pages/dev_guides/ai_app_index.html

3.3. Supported HW platforms

One of the main factors for LPDNN’s adoption is performance portability across the wide span of hardware platforms. LPDNN’s flexible architecture allows the main core to remain small and dependency-free while additional 3rd party libraries or inference engines are only included when needed and for specific platforms, notably increasing the portability across systems. Besides, cross-compilation and specific tools are added to support a wide range of heterogeneous computing platforms such as CPUs, GPUs, NPUs. 

One of the objectives of LPDNN is to provide full support for reference platforms by providing:
* Developer Platform Environments (DPEs) to help the user employ a developer platform, including OS images, drivers, and cross-compilation toolchains for several heterogeneous platforms.
* A dockerised and stable environment, which increases the reliability by encouraging the replication of results across platforms and environments.
* Optimisation tools and computing libraries for a variety of computing embedded platforms that can be used by LPDNN’s inference engines to accelerate the execution of neural networks.

Originally, the range of embedded platforms that were supported within LPDNN were the following:
* Raspberry Pi 3b+: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz
* Raspberry Pi 4b: Quad-core ARM Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz
* Nvidia Jetson Nano: Quad-core ARM Cortex -A57 @ 1.43 GHz & 128-core Nvidia Maxwell GPU
* Nvidia Jetson Xavier: Octa-core ARM v8.2 @ 2.03 GHz & 512-core Nvidia Volta GPU with Tensor Cores

During the development of the Stairwai project, we have employed Bonseyes LPDNN’s platform workflows to integrate new HW platforms and provide a more heterogeneous set of benchmarks on variety of HW processing cores. The following HW platforms have been fully integrated:
* Intel NUC: Intel quad-core (TM) i5-7260U CPU @ 3.4 GHz
* iMX8m Nano: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz
* STM32 MP1: Dual-core ARM Cortex-A7 cores up to @ 800 MHz

Besides, the following platforms have been upgraded: 
* Nvidia Jetson Nano: Update to latest jetpack release JP6.4
* Nvidia Jetson Xavier: Update to latest jetpack release JP6.4
* Raspberry Pi 3b+: Update to latest Ubuntu20 packages
* Raspberry Pi 4b: Update to latest Ubuntu20 packages 

Furthermore, BonsAPPs project is currently performing the integration of extra-low-power platforms containing Micro-controller Units, which can then be leveraged in Stairwai for benchmarking:
* STM32 H7A3:  Arm® Cortex®-M7 core @ 480 MHz & 1.4 Mbytes of SRAM
* Greenwaves GAP8: RISC-V core & Octa-core RISC-V (cluster) & 512kB of L2 memory

More information about the HW platforms can be found at:
* User guide: https://bonseyes.gitlab.io/bonseyes-cli/pages/user_guides.html#platform
* Developer guide: https://bonseyes.gitlab.io/bonseyes-cli/pages/developer_guides.html#platform

Developer platforms can be accessed upon request at https://gitlab.com/bonseyes/platforms/.

3.4. Benchmark framework

LPDNN provides a structed benchmarking layer to analyze the execution of AI Applications on HW platforms. LPDNN integrates a first analysis of static metrics during the offline compilation of the AI Applications. Metrics such as FLOPS, model parameters and model storage are obtained. 

Besides, LPDNN integrates two methods to benchmark the execution of AI Apps:
* Built-in anchors:  C++ anchors are included in LPDNN to measure the latency and memory consumption of the AI Apps during their executions. Several anchors are included:
o Pre-processing: This anchor measures the latency and memory of a given pre-processor:
* Image: cropping, normalization, resize, filtering
* Audio: mfcc feature computation
* Signal: resizing, filtering
o Inference: This anchor measures the latency and memory spent during the forward pass of the neural network for a given inference engine.
o Post-processing: This anchor measures the latency and memory spent during the post-processing of the outputs of the forward pass, e.g., decoding, non-max suppression, bounding box forming.
o Total execution: This anchor measures the latency and memory spent during the total execution of the AI App.

* HW resources monitoring: Python script that monitors the HW resources of the system while the execution of the AI App. Metrics such as CPU/GPU/NPU processor load, peak/average memory usage, memory bandwidth, temperature, etc.

Overall, these are the following metrics are LPDNN collects:
* Static metrics (not measured on device): 
o FLOPs
o Parameters
o Storage
* Dynamic metrics (measured on device): 
o Latency
o Throughput
o Average Memory (CPU/GPU)
o Peak Memory (CPU/GPU)
o Memory Bandwidth (CPU/GPU)
o Processor load (CPU/GPU/NPU)
o Power consumption
o Temperature

More details about the benchmark process will be given in D6.2.

4. Deployment of AI Apps and Benchmark

In this section, we showcase what elements are needed for deployment and how to execute a benchmark.

4.1. Deployment of AI Apps

To be able to execute AI applications on a hardware platform, two elements are required:

A. An LPDNN AI application: 
It defines the class and structure of the AI application, the DNN models’ architecture and weights, its deployment configuration and the pre- and post-processing that it takes. LPDNN AI applications are platform-specific, although the same AI application can be executed on different HW platforms as long as the selected inference engine or backends are supported by the HW platform.

B. An LPDNN Deployment Package: 
Collection of tools, executables, libraries, inference engines and backends that allows the actual execution of the LPDNN AI application. The collection of libraries and binaries that are copied on the target platform for the execution of the DNN is called a runtime. The runtime dynamically loads an AI application and executes it based on its defined configuration. LPDNN’s deployment packages are platform specific as they contain the inferences engines and backends supported by the HW platform.

4.1.1 LPDNN AI Application (AI App)

An LPDNN AI App is composed of the following files:

* ai_app_config.json: This file is the main descriptor of an AI App. It defines the AI App’s components and their type, e.g., image_classification, object_clasification, face_recognition, audio_classification andsignal_processing, the type of pre- and post-processing as well as the inference engine to use to execute the DNN model. It also points to the model architecture and weights file.

* ai_app.yml: This file defines the AI App metadata and license type. It also This describes the platform, runtime and challenge that the AI App was initially compiled for. This file is not used by the runtime, but by other deployment tools.

* DNN model: A DNN model describes the model architecture and the trained weights. A DNN model may come on different forms based on the selected inference engine, e.g., model.json, model.param, model.bin or model.onnx.



4.1.2 LPDNN Deployment Package

An LPDNN runtime, contained within a deployment package, is the collection of binaries and libraries that are copied to the target platform for the execution of DNNs. Runtimes are composed of the following files:

* Binaries: This folder contains the set of executables to start an AI App manually through different interfaces, e.g., ai-app-cli, ai-app-cli.py, http-worker.

* Libraries: This folder contains the set of dynamic and static libraries that are included in LPDNN for a target developer platform. It includes the inference engines, the pre- and post-processing routines, backends, etc.

* Solutions: This folder contains the bash scripts to start up an AI App automatically or remotely.

* package.yml: This file defines the runtime name.

* runtime.yml: This file contains metadata for the runtime and it is used to automatically start an AI App using the Bonseyes tools.

* engines.yml: This file describes the available engines for within the runtime.



4.2. Benchmark of AI Apps

Fig. 4 illustrate the process a user needs to go through to benchmark an AI Application using LPDNN. The process is divided into the following steps:
1. Choose AI App and target HW platform from the Catalog.
2. Follow the requirements step to be compliant with the process and the tools.
3. Download an AI App and a deployment package.
4. Benchmark your AI App on your target HW.

Next, each of these steps will be explained in detail.

4.2.1. Choose AI App and target HW platform

The user needs to choose a target platform and AI App from the Marketplace’s Catalog. 

If the user would like to create its own application, detailed documentation is provided to generate a new one at Bonseyes docs for AI-App generation.

4.2.2. Requirements

To be able to execute AI applications on a hardware platform, the following steps need to have been performed:

1. Setup the local environment as explained in Bonseyes Prerequisites doc.
2. Set up the target hardware as explained in Setup platform section of Bonseyes’ doc and have the ${platformName_src} ${platformName_build} and ${platformName_config} folders in your machine.
3. Install python packages in the target board as explained in the Packages’s section.

Once those steps are completed, change directory to the folder where you built your target platform during the Setup platform section, e.g., my-bonseyes-platform.

4.2.3. Download AI App and deployment package

New AI App can be generated by following the Bonseyes docs for AI-App generation. Already built AI Apps can also be obtained from the Bonseyes Marketplace’s catalogue. To download an AI-App from the Marketplace’s catalogue execute the following command (replace ${ai_app} by the name you would like to give to your AI-App):

       bonseyes marketplace download-ai-app --output-dir ${ai_app}



A dialog will prompts asking you to choose the AI-App you would like to download and he ai app will be downloaded in the directory ${ai_app} as shown in Figure 6. The AI App will contain a series of files, as explained in Section 4.1.1.

LPDNN’s deployment packages are currently available at https://gitlab.com/bonseyes/artifacts/deployment-packages/-/packages and can be accessed under request. Once you obtained a platform-specific deployment package, copy your LPDNN’s deployment package in the same folder as the target platform and the AI-App. Next, decompress it (change ${deployment_package} by the name of your package):

       cp ${deployment_package}.tar.gz my-bonseyes-platform

       tar -xvf ${deployment_package}.tar.gz



4.2.4. Benchmark your AI-App on your target HW

To benchmark an AI-App, your folder should contain the following elements:

1. ${platformName_config}: target configuration coming from the DPE setup

2. ${deployment_package}: downloaded from Gitlab

3. ${ai_app}: downloaded from the BMP

To benchmark an AI App, you need to execute the following command:

bonseyes ai-app benchmark-analyzer --target-config ${platformName_config} \

                                   --ai-app ${ai_app} \

                                   --deployment-package ${deployment_package} \

                                   --dataset PATH/TO/DATASET_FOLDER \

                                   [--number 20] \

                                   [--filename ${FileName}]

For instance, to benchmark a Face Detection Retinaface AI-App for Imagenet on the Jetson Xavier platform looks like:

	bonseyes ai-app benchmark-analyzer 

			--target-config jetson_xav_config \

                       --ai-app my-aiapp \

                       --deployment-package xav_deployment_package \

                       --dataset ../samples \

                       --number 10 \

                       [--filename benchmark.json]

This call will perform then inferences for Face Detection Retinaface AI-App on the Jetson Xavier platform with Jetpack 4.6 as follows:

To store the benchmark in a file, add the –filename option and the metrics will be dumped in the named file in JSON format. If the file name option is enabled, prediction results will be automatically dump into results.txt*.

4.3. Benchmark as a Service

The StairwAI project aims to enhance the AI-on-demand platform services by means of a service layer that provides, among others, Vertical Matchmaking. The benchmark, being a building block of the Vertical Matchmaking, was not considered to be a service on its own in the proposed Grant Agreement. Nonetheless, given its maturity, the project is exploring the feasibility of a more closely integration of the Benchmark as a Service (aaS) into the AI-on-demand platform. 

In that direction, LPDNN has been released in the form of deployment packages that can be used within the Benchmark aaS workflow to ease the usage by external users. Benchmark tools as well as HW platforms and AI Apps can be accessed from the Bonseyes ecosystem, as explained in Section 4.2.

AI-on-demand experimentation platform (Acumos) allows the creation of AI pipelines based on dockerized elements (Assets) that are interconnected by an orchestrator. Such containers may have any sort of artifact inside, e.g., dataset, AI model, or algorithm, and provide interfaces that define the API to communicate or transfer te content from one container to another. Users can create docker components and experiment with them in the form of an AI pipeline by defining an orchestrator to connect them up.

Partners of WP6 have held coordination meetings with AI4EU partners to understand the requirments and dependencies that are requered to integrate the Benchmark aaS into Acumus. Fig. 8 describes a potential interconnection between the Acumos and the benchmark aaS:

The understanding that has been reached so far indicates that a dockerized workflow (within the blue cloud in Fig.8) should be put in place to to integrate the Benchmark aaS into Acumus. 

The dockerized workflow that is proposed is the Bonseyes AI Asset workflow that should allow the following functions:
* Align docker dependencies and interfaces between Acumus and LPDNN’s docker environments. 
* Allow the transfer of AI models from Acumus’s containers (Assets) to the Bonseyes’s ecosystem by providing interfaces that allow Acumus’s Assets to be “benchmarkable” by the Benchmark aaS.
* Transform the AI models into LDPNN AI Applications that can be executed by the benchmark layer. 

More details and efforts will be put into the integration/compatibility of the benchmark as a Service with the AI-on-demand platform during the second half of the StairwAI project, as part of T2.5.  However, to go beyond the initial proposal in the Grant agreement, appropriate resources should be mobilised to this effect.

5. Conclusion and future work

This document has presented the first version of the Benchmark software framework proposed within WP6 Task 6.1 of StairwAI project. The document describes how LPDNN inference framework has been used and extended to accommodate a benchmark layer that allows to profile AI Applications on heterogeneous HW platforms. The deployment and benchmark of AI Applications has been showcased, illustrating the flow and commands that a user need to execute to benchmark an AI Application. 

As future work for the 2nd release of the Benchmark software framework, more efforts will be made to integrate Huawei’s embedded platform and inference engine into LPDNN, increasing the capabilities for profiling AI Algorithms on a wide range of platforms, 

In addition, more effort will be put into the integration/interoperability of the benchmark as a Service with the AI4EU platform, providing the Bonseyes AI Asset workflows as a bridge to allow the benchmark of a variety of AI4EU’s Assets. 

This is a heavy limitation, as it doesn´t consider any other application. Can we have a reasoning on why it was not?[GG1][GG1]

This should be clearly stated and reflected if it is not the case, however I am at the beginning of the document. It could be later explained

Neural networks cover a great majority of use cases using AI nowadays. LPDNN was developed to be portable and yet efficient (c++) across many platforms, rather than portable across many algorithm types[mi2R1][mi2R1]

We are further developing the "Bonseyes AI Assets", which provide more flexibility for the algorithm type and can also benchmark as LPDNN. This will be introduced in the 2nd version of the software, in D6.2

This section must present the alternatives as discussed in the Plenary with the options and how they will support both, the AI on demand and Bonsapps , if required. But the ultimate goal is to provide the services into the AI on demand.[GG5]

Furthermore the services on AI4Experiments is ready to incorporate the LPDNN. This is a key question on why it has not been considered. Please include that reasoning and the alternatives.

1. Executive Summary

In M20, the 1st version of the Benchmark software framework was released within WP6 Task 6.1 of StairwAI project. The 1st version of benchmark software framework represented one of the main blocks of the vertical matchmaking service as it produced the benchmarks across several HW platforms (Task 6.2) that are then used to train the vertical matchmaking engine (Task 6.3).

This document provides the 2nd version of the Benchmark software framework, including the integration of a new SW inference engine and HW platform from Huawei into LPDNN. Besides, this deliverable also provides a large step towards achieving MS10 for the integration of the Benchmark as a Service into the AI-on-demand platform.

This document is built incrementally on top of 1st version of the Benchmark software framework deliverable, making the document self-contained. We add and highlight the new features and content of the Benchmark software framework 2nd that have been added with respect to the 1st.

Section 2 provides a general introduction of the WP and summarize the scope and contributions of this deliverable.

Section 3 introduces LPDNN: the inference framework that has been used and extended to create a benchmark framework for heterogeneous HW platforms. First, LPDNN’s architecture is detailed, explaining the different components that make it an interoperable framework. Next, we introduce the different inference engines that LPDNN supports, which increase the portability and optimisation of neural networks across HW platforms. Then, we describe the support for HW platforms and those platforms that have been already integrated. 

Section 4 starts by explaining the elements that are used to deploy AI Applications on HW platforms. Then, the benchmark flow is explained by showcasing the execution of an AI Application and the obtention of a benchmark. 

Section 5 introduces the design of the Benchmark as a Service (WP2) out of the Benchmarking software framework, increasing its scope and usability as well as its integration into the AI-on-demand platform.

Lastly, Section 6 elaborates the conclusions.

2. Introduction

WP6 has the main objective of building a Vertical Matchmaking engine that matches AI algorithms and HW resources to optimise the deployment of services and increase their efficiency. 

As such, one of the main objectives of WP6 is to develop benchmark software framework/service suitable for selected machine learning models and hardware, including CPU (Intel x86, Arm Cortex-A5x, Risc-V), GPGPU (NVIDIA, etc), and NPU (HUA, etc) accelerated platforms. The benchmark software framework will be used in Task 6.2 to produce the benchmarks and create a profiling dataset that can be used in Task 6.3 to train the Vertical Matchmaking engine’s algorithms. 

In this document, we propose LPDNN framework as the benchmark software framework. LPDNN framework was developed during the H2020 Bonseyes project (Prado, Miguel De, et al) and has been largely extended to provide a structed benchmarking layer to analyze the execution of AI Applications, particularly DNNs, on a variety of heterogeneous platforms. This layer, on top of LPDNN, adheres to the following design principles:

* Industry-driven Research:
> Input requirements from SMEs wanting to use AI solutions
> Able to deploy AI solutions on edge devices (low-power, low-carbon footprint)

*  Structured benchmarking workflow:
> Availability of anchors within the deployment framework to evaluate the metrics truthfully
> Optimised deployment (value added to research and industry)
> Extensive documentation & Support (user friendly for SMEs)
> Defined interfaces (standarization)
> Easy to replicate (reproducibility)
> Create trust & community



Taking the previous design principles, the main contributions of the benchmark software framework are the following:
> Introduction of LPDNN as an inference framework.
> Integration into LPDNN of four inference engines to benchmark CPU, GPU and NPU platforms.
> Integration into LPDNN of four HW platforms and update of another four available platforms to the latest SW release.
> Creation of the benchmarking layer within LPDNN, containing a variety of static and dynamic metrics.
> Design of the benchmarking framework as Service that will be used by external users and interoperable with the AI-on-demand platform.


2.1. Purpose and Scope of the document

Deliverable D6.2 is a Demonstrator, i.e., it introduces the second and final version of the Benchmark software framework.

3. LPDNN

LPDNN, which stands for Low-Power Deep Neural Network, is a deployment framework that provides the tools and capabilities to generate portable and efficient implementations of DNNs. The main goal of LPDNN is to provide a set of AI applications for deep learning tasks, e.g., object detection, image classification, speech recognition, which can be deployed and optimised across heterogeneous platforms, e.g., CPU, GPU, FPGA, NPU.

3.1. LPDNN architecture

One of the main issues of deep learning systems is the hardship to replicate results across different systems. To solve this issue, LPDNN features a full development flow for deep learning solutions on embedded devices by providing platform support, sample models, optimisation tools, integration of external libraries and benchmarking. LPDNN’s full development flow makes the AI solution reliable and easy to replicate across systems.

AI applications are the result of LPDNN’s optimisation process and the higher level of abstraction for the deployment of DNNs on a target platform. They contain all the necessary elements or modules for the execution of a DNN. An AI application is the optimised representation of a DNN model to be efficiently executed on a target embedded device.  

An AI application contains all the necessary elements or modules for the execution of a DNN:
* Pre-processing: Step to prepare, normalize or convert the input data into the required input that is expected by the DNN.
* DNN inference: Forward-pass of the neural network. The execution is taken care of by an inference engine.
* Post-processing: Conversion of the neural network’s output into structured and human-readable information.

Next, we detail LPDNN’s architecture by further describing the concept of LPDNN ‘s inference engines and the support for heterogeneous platforms.

3.2. Inference engines

AI applications contain a hierarchical but flexible architecture that allows new modules to be integrated within the LPDNN framework through an extendable and straightforward API. For instance, LPDNN supports the integration of 3rd-party self-contained inference engines to perform DNN inference. 

Initially, LPDNN only supported:
* LNE: LPDNN Native Engine (LNE) allows the execution of DNNs across arm-based and x86 CPUs as well as on Nvidia-based GPUs.

During the development of the 1st version of the Benchmark software framework in the Stairwai project, the following inference engines have been integrated into LPDNN:
* NCNN: NCNN ports the execution of DNNs on GPU through the Vulkan driver.
* TensorRT: TensorRT accelerates the DNN inference on Nvidia-based GPUs and NPUs.
* ONNXruntime: ONNXruntime enables the direct execution of ONNX models on CPUs and GPUs.

The inclusion of external engines also benefits LPDNN as certain embedded platforms provide their own specific and optimised framework to deploy DNNs on their hardware.

As part of the 2nd release of the benchmarking software framework, Huawei’s inference engine has also been integrated. Huawei’s computing hardware fall into the category of platforms with a dedicated inference engine, called Ascend Computing Language (ACL). ACL provides a collection of C language APIs for users to develop deep neural network apps ranging from device management and context management to model/operator loading and execution. 

ACL APIs can be called through a third-party framework such as LPDNN to utilize the compute capability of the Ascend AI Processor.  The integration of ACL into LPDNN provides a method for benchmarking AI apps on Huawei’s platforms against other platforms such as GPUs and CPUs and other inference engines such as NCNN, ONNXRuntime, etc. with a common framework. Further information can be found here.

More information about how to add a new inference engine in LPDNN can be found at:
* Developer guides: https://bonseyes.gitlab.io/bonseyes-cli/pages/dev_guides/ai_app_index.html

3.3. Supported HW platforms

One of the main factors for LPDNN’s adoption is performance portability across the wide span of hardware platforms. LPDNN’s flexible architecture allows the main core to remain small and dependency-free while additional 3rd party libraries or inference engines are only included when needed and for specific platforms, notably increasing the portability across systems. Besides, cross-compilation and specific tools are added to support a wide range of heterogeneous computing platforms such as CPUs, GPUs, NPUs. 

One of the objectives of LPDNN is to provide full support for reference platforms by providing:
* Developer Platform Environments (DPEs) to help the user employ a developer platform, including OS images, drivers, and cross-compilation toolchains for several heterogeneous platforms.
* A dockerised and stable environment, which increases the reliability by encouraging the replication of results across platforms and environments.
* Optimisation tools and computing libraries for a variety of computing embedded platforms that can be used by LPDNN’s inference engines to accelerate the execution of neural networks.

Originally, the range of embedded platforms that were supported within LPDNN were the following:
* Raspberry Pi 3b+: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz
* Raspberry Pi 4b: Quad-core ARM Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz
* Nvidia Jetson Nano: Quad-core ARM Cortex -A57 @ 1.43 GHz & 128-core Nvidia Maxwell GPU
* Nvidia Jetson Xavier: Octa-core ARM v8.2 @ 2.03 GHz & 512-core Nvidia Volta GPU with Tensor Cores

During the development of the 1st version of the Benchmark software framework in the Stairwai project, we have employed Bonseyes LPDNN’s platform workflows to integrate new HW platforms and provide a more heterogeneous set of benchmarks on variety of HW processing cores. 

The following HW platforms have been fully integrated:
* Intel NUC: Intel quad-core (TM) i5-7260U CPU @ 3.4 GHz
* iMX8m Nano: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz
* STM32 MP1: Dual-core ARM Cortex-A7 cores up to @ 800 MHz

Besides, the following platforms have been upgraded: 
* Nvidia Jetson Nano: Update to latest jetpack release JP6.4
* Nvidia Jetson Xavier: Update to latest jetpack release JP6.4
* Raspberry Pi 3b+: Update to latest Ubuntu20 packages
* Raspberry Pi 4b: Update to latest Ubuntu20 packages 

As part of the 2nd release of the benchmarking software framework, two new platforms from Huawei have also been used for deployment:
* Atlas 200 DK (model: 3000): The Atlas 200 DK is a high-performance AI application developer board that integrates the Ascend 310 AI processor to facilitate quick development and verification. It has been widely used in scenarios such as developer solution verification, higher education, and scientific research.
* Atlas 800 Training Server (Model: 9010): The Atlas 800 training server is an AI training server based on the Intel processors and Huawei Ascend 910 processors. It features ultra-high computing density and high network bandwidth. The server is widely used in deep learning model development and training scenarios and is an ideal option for computing-intensive industries, such as smart city, intelligent healthcare, astronomical exploration, and oil exploration.

Further information can be found here. 

Furthermore, BonsAPPs project is currently performing the integration of extra-low-power platforms containing Micro-controller Units, which can then be leveraged in Stairwai for benchmarking:
* STM32 H7A3:  Arm® Cortex®-M7 core @ 480 MHz & 1.4 Mbytes of SRAM
* Greenwaves GAP8: RISC-V core & Octa-core RISC-V (cluster) & 512kB of L2 memory

More information about the HW platforms can be found at:
* User guide: https://bonseyes.gitlab.io/bonseyes-cli/pages/user_guides.html#platform
* Developer guide: https://bonseyes.gitlab.io/bonseyes-cli/pages/developer_guides.html#platform

Developer platforms can be accessed upon request at https://gitlab.com/bonseyes/platforms/.

3.4. Benchmark framework

LPDNN provides a structed benchmarking layer to analyze the execution of AI Applications on HW platforms. LPDNN integrates a first analysis of static metrics during the offline compilation of the AI Applications. Metrics such as FLOPS, model parameters and model storage are obtained. 

Besides, LPDNN integrates two methods to benchmark the execution of AI Apps:
* Built-in anchors:  C++ anchors are included in LPDNN to measure the latency and memory consumption of the AI Apps during their executions. Several anchors are included:
o Pre-processing: This anchor measures the latency and memory of a given pre-processor:
* Image: cropping, normalization, resize, filtering
* Audio: mfcc feature computation
* Signal: resizing, filtering
o Inference: This anchor measures the latency and memory spent during the forward pass of the neural network for a given inference engine.
o Post-processing: This anchor measures the latency and memory spent during the post-processing of the outputs of the forward pass, e.g., decoding, non-max suppression, bounding box forming.
o Total execution: This anchor measures the latency and memory spent during the total execution of the AI App.
* HW resources monitoring: Python script that monitors the HW resources of the system while the execution of the AI App. Metrics such as CPU/GPU/NPU processor load, peak/average memory usage, memory bandwidth, temperature, etc.

Overall, these are the following metrics are LPDNN collects:
* Static metrics (not measured on device): 
o FLOPs
o Parameters
o Storage
* Dynamic metrics (measured on device): 
o Latency
o Throughput
o Average Memory (CPU/GPU)
o Peak Memory (CPU/GPU)
o Memory Bandwidth (CPU/GPU)
o Processor load (CPU/GPU/NPU)
o Power consumption
o Temperature

For more details, please refer to D6.3 submitted in M25.

4. Deployment of AI Apps and Benchmark

In this section, we showcase what elements are needed for deployment and how to execute a benchmark.

4.1. Deployment of AI Apps

To be able to execute AI applications on a hardware platform, two elements are required:

A. An LPDNN AI application: 

It defines the class and structure of the AI application, the DNN models’ architecture and weights, its deployment configuration and the pre- and post-processing that it takes. LPDNN AI applications are platform-specific, although the same AI application can be executed on different HW platforms as long as the selected inference engine or backends are supported by the HW platform.

B. An LPDNN Deployment Package: 

Collection of tools, executables, libraries, inference engines and backends that allows the actual execution of the LPDNN AI application. The collection of libraries and binaries that are copied on the target platform for the execution of the DNN is called a runtime. The runtime dynamically loads an AI application and executes it based on its defined configuration. LPDNN’s deployment packages are platform specific as they contain the inferences engines and backends supported by the HW platform.

4.1.1 LPDNN AI Application (AI App)

An LPDNN AI App is composed of the following files:
* ai_app_config.json: This file is the main descriptor of an AI App. It defines the AI App’s components and their type, e.g., image_classification, object_clasification, face_recognition, audio_classification andsignal_processing, the type of pre- and post-processing as well as the inference engine to use to execute the DNN model. It also points to the model architecture and weights file.
* ai_app.yml: This file defines the AI App metadata and license type. It also This describes the platform, runtime and challenge that the AI App was initially compiled for. This file is not used by the runtime, but by other deployment tools.
* DNN model: A DNN model describes the model architecture and the trained weights. A DNN model may come on different forms based on the selected inference engine, e.g., model.json, model.param, model.bin or model.onnx.



4.1.2 LPDNN Deployment Package

An LPDNN runtime, contained within a deployment package, is the collection of binaries and libraries that are copied to the target platform for the execution of DNNs. 
Runtimes are composed of the following files:
* Binaries: This folder contains the set of executables to start an AI App manually through different interfaces, e.g., ai-app-cli, ai-app-cli.py, http-worker.
* Libraries: This folder contains the set of dynamic and static libraries that are included in LPDNN for a target developer platform. It includes the inference engines, the pre- and post-processing routines, backends, etc.
* Solutions: This folder contains the bash scripts to start up an AI App automatically or remotely.

* package.yml: This file defines the runtime name.

* runtime.yml: This file contains metadata for the runtime and it is used to automatically start an AI App using the Bonseyes tools.

* engines.yml: This file describes the available engines for within the runtime.



4.2. Benchmark of AI Apps

The process is divided into the following steps:
1. Choose AI App and target HW platform from the Catalog.
2. Follow the requirements step to be compliant with the process and the tools.
3. Download an AI App and a deployment package.
4. Benchmark your AI App on your target HW.

Next, each of these steps will be explained in detail.

4.2.1. Choose AI App and target HW platform

The user needs to choose a target platform and AI App from the Marketplace’s Catalog. 

If the user would like to create its own application, detailed documentation is provided to generate a new one at Bonseyes docs for AI-App generation.

4.2.2. Requirements

To be able to execute AI applications on a hardware platform, the following steps need to have been performed:
1. Setup the local environment as explained in Bonseyes Prerequisites doc.
2. Set up the target hardware as explained in Setup platform section of Bonseyes’ doc and have the ${platformName_src} ${platformName_build} and ${platformName_config} folders in your machine.
3. Install python packages in the target board as explained in the Packages’s section.

Once those steps are completed, change directory to the folder where you built your target platform during the Setup platform section, e.g., my-bonseyes-platform.

4.2.3. Download AI App and deployment package

New AI App can be generated by following the Bonseyes docs for AI-App generation. Already built AI Apps can also be obtained from the Bonseyes Marketplace’s catalogue. 

To download an AI-App from the Marketplace’s catalogue execute the following command (replace ${ai_app} by the name you would like to give to your AI-App):
       bonseyes marketplace download-ai-app --output-dir ${ai_app}



A dialog will prompts asking you to choose the AI-App you would like to download and he ai app will be downloaded in the directory ${ai_app} as shown in Figure 7. The AI App will contain a series of files, as explained in Section 4.1.1.

LPDNN’s deployment packages are currently available at https://gitlab.com/bonseyes/artifacts/deployment-packages/-/packages and can be accessed under request. Once you obtained a platform-specific deployment package, copy your LPDNN’s deployment package in the same folder as the target platform and the AI-App. Next, decompress it (change ${deployment_package} by the name of your package):

       cp ${deployment_package}.tar.gz my-bonseyes-platform

       tar -xvf ${deployment_package}.tar.gz



4.2.4. Benchmark your AI-App on your target HW

To benchmark an AI-App, your folder should contain the following elements:
1. ${platformName_config}: target configuration coming from the DPE setup
2. ${deployment_package}: downloaded from Gitlab
3. ${ai_app}: downloaded from the BMP

To benchmark an AI App, you need to execute the following command:
bonseyes ai-app benchmark-analyzer --target-config ${platformName_config} \
                                   --ai-app ${ai_app} \
                                   --deployment-package ${deployment_package} \
                                   --dataset PATH/TO/DATASET_FOLDER \
                                   [--number 20] \
                                   [--filename ${FileName}]

For instance, to benchmark a Face Detection Retinaface AI-App for Imagenet on the Jetson Xavier platform looks like:
	bonseyes ai-app benchmark-analyzer 
			--target-config jetson_xav_config \
                       --ai-app my-aiapp \
                       --deployment-package xav_deployment_package \
                       --dataset ../samples \
                       --number 10 \
                       [--filename benchmark.json]

This call will perform then inferences for Face Detection Retinaface AI-App on the Jetson Xavier platform with Jetpack 4.6 as follows:

To store the benchmark in a file, add the –filename option and the metrics will be dumped in the named file in JSON format. If the file name option is enabled, prediction results will be automatically dump into results.txt*.

5. Benchmark as a Service

As part of the 2nd release of the benchmarking software framework, we also introduce the Benchmark as-a-Service (BaaS) design in collaboration with WP2 for broader interoperability with the AI-on-demand platform. StairwAI aims to enhance the AI-on-demand platform services by employing a service layer that provides, among others, Vertical Matchmaking. The Benchmarker, a building block of the Vertical Matchmaking, was not initially considered as a service on its own. Nonetheless, given its maturity, the StairwAI has provided the means and effort for a closer integration of the BaaS into the AI-on-demand platform. 

Partners of WP6 have held coordination meetings with AI4Europe partners to understand the requirements and dependencies required to interoperate the BaaS into the AI-on-demand platform. Given that the AI-on-demand platform may contain multiple AI artifacts, e.g., dataset, AI model, or algorithm, the best solution found was to provide interfaces that define the API to communicate or transfer the content from one service/database to another. Thus, AI models from the AI-on-demand platform could be benchmarked through the BaaS from StairwAI. Fig. 9 illustrates the interfaces between the AI-on-demand Platform, the AI artifacts from AI4EU and the BaaS:

5.1. BaaS Design

To be able to put in place such a workflow, it was envisioned a service layer wrapping and also extending the Benchmarking Software framework (middle green block in Fig.9). This would also allow to alleviate some of the Benchmarking software framework dependencies and be able to benchmark any AI model and format while still benefiting from LPDNN optimisations. 

The dockerized service will allow the following functions:
* Transfer of AI models from the AI-on-demand Platform to the BaaS for benchmarking by providing CLI or HTTP interfaces. 
* Dispatch of requests from the central service to the operative docker containers to benchmark the AI model on the target platform. 
* Check the format and compatibility of the AI model with the available inference engine on the target platform when the model is received at the docker container. Note that some computing platforms do not support all inference engines, e.g. RPI does not support TensorRT as it does not have an Nvidia GPU.
* Benchmark of the AI model on the target platform and return of the results to the end user through the dispatcher.

In order to transform the Benchmarking software framework into the BaaS described in Fig. 10, LPDNN will be released in the form of deployment packages that can be employed within the BaaS workflow for AI model inference. LPDNN deployment packages will fit into a more abstract and general class for AI model inference, which also allows to add not supported engines such as TensorFlow Lite and broaden the range of AI model that can be benchmarked. LPDNN benchmarking layer, explained in Section 3.4, will be adapted to provide all the available metrics, i.e., static, and dynamic, in a coherent manner for all inference engines. Finally, to create the BaaS’s docker containers, LPDNN’s DPE, explained in Section 3.3, will be used to include all the environment, libraries, and dependencies to set up and deploy AI models on the platform.

More details and efforts will be put into the integration and deployment BaaS with the AI-on-demand platform as part of T2.5 during the last months of the StairwAI project.  

5.2. BaaS API

In this section, we introduce the API that will be available for the BaaS once it’s deployed. We provide two types of interfaces:

5.2.1. CLI

Command Line Interface (CLI) allows to a given user to query the BaaS locally through the command line, allowing for easy usage and testing. 

The usage of the BaaS would be as follows:

Usage: bonseyes_baas_cli
	--model-path <local_model_path>
	--engine {tf, tflite, onnx, tensorrt, lpdnn}
	--profile (specifies versions of dependencies etc.)
	--platform {x86, raspberrypi, jetson}
	[--device {cpu, gpu}]

The Bonseyes_baas_cli will connect to the dispatcher, which based on the platform and profile, will redirect the query to the right docker container. The docker container will then take the model-path and engine to perform the benchmark either on the CPU or GPU as specified by the device parameter.

5.2.2. HTTP

The HTTP interface allows to a given user to query the BaaS remotely without direct access to where the BaaS is deployed (only URL needed). 

The usage of the BaaS would be as follows:
<url>/bonseyes_baas/<platform>/<profile>/<engine>?device=<cpu|gpu>
	<platform>: {x86, raspberrypi, jetson}
	<engine>: {tf, tflite, onnx, tensorrt, lpdnn}

For a user to be able to post a query, the model should be in a binary body of the request as follows:
curl --request POST --data-binary @<path_to_model_file> <url>/bonseyes_baas/<platform>/<profile>/<engine>?device=<cpu|gpu>

The BaaS internal process will perform in the exact same way as explained for CLI.

6. Conclusion and future work

This document has presented the second and final release of the Benchmark software framework proposed within WP6 Task 6.1 of StairwAI project. The document describes how LPDNN inference framework has been used and extended to accommodate a benchmark layer that allows to profile AI Applications on a range of heterogeneous HW platforms. The deployment and benchmark of AI Applications has been showcased, illustrating the flow and commands that a user need to execute to benchmark an AI Application. 

In addition, we have presented the design of the Benchmark as-a-Service, extending the Benchmark software framework’s scope to increase its reusability and interoperability with the AI-on-demand platform and allow the benchmark of AI-on-demand’s assets. This represents a large step towards achieving MS10 for integrating the services into the AI-on-demand platform.

1. Executive Summary

This document delivers the first version of the Benchmark results proposed within WP6 Task 6.2 of StairwAI project. The benchmark results are the outcome of benchmark software framework executed across multiple heterogenous platforms, which are then used to train the vertical matchmaking engine (Task 6.3).

Other than the results obtained from the benchmark software framework (Section 2), this document also presents a summary of external benchmarks (Section 3) that are commonly used by researchers to push the limits of the fields. The combination of the external benchmarks with the ones provided by the benchmark software framework allows us to have a richer and more complete database to train the vertical matchmaking engine, which is discussed in Section 4. Lastly, Section 5 elaborates the conclusions and future work on Task 6.2.











































2. Introduction



As the computational capacity of embedded devices grows, they become more capable of performing sophisticated tasks, including those considered artificial intelligence (AI) applications. Since the advent of AI for computer vision and speech recognition tasks, training of AI models has been performed using machine learning algorithms, with a large number of open-source datasets created by researchers. Such datasets provide an opportunity to develop, train and test AI algorithms on devices that range from the Cloud to the Edge.

A significant concern for AI engineers during the deployment of AI Algorithms is maximizing the system's performance in terms of overall latency, quality of solution, power, and space. The evaluation of AI Algorithms is often based on how well they perform on restricted public benchmark problems. These benchmarks give an informative indication of how well a given algorithm will perform on some given tested systems. However, AI Algorithms are often deployed on systems with rather different characteristics. As a result, the performance can vary widely according to the system on which they are deployed. Therefore, it is vital to have a large variety of benchmarks across multiple heterogeneous platforms to understand an AI Algorithm’s behaviour comprehensively.

Taking these principles, the main contributions of this deliverable are the following:

> Presentation of benchmark results obtained by StairwAI’s benchmark frameworks across heterogeneous platforms, including CPUs, GPUs, and NPUs.

> Survey of external benchmarks, presenting their scope, metrics, pros, and cons.

> Discussion about the found results towards creating a Vertical Matchmaking Engine.



2.1. Purpose and Scope of the document

WP6 has the main objective of building a Vertical Matchmaking engine that matches AI algorithms and HW resources to optimise the deployment of services and increase their efficiency. In the deliverable D6.1 (Benchmarking software-1st version M20), the benchmark software framework was introduced, which had the main objectives of developing a software tool to benchmark machine learning models and hardware, including CPU (Intel x86, Arm Cortex-A5x, Risc-V), GPGPU (NVIDIA, etc), and NPU (HUA, etc) accelerated platforms. 

Deliverable D6.3 is a Report, i.e., it introduces the first version of the Benchmark results, which will be extended during the course of Task 6.2 and will conclude with D6.4 (Benchmarking results-2nd M30). In this deliverable, we present the benchmarking data set collected so far by the benchmark software framework. Further, we offer the results of a survey we conducted to investigate existing benchmarking data, especially to understand whether it could be useful for the Vertical Matchmaking Engine (D6.5 in M30). Finally, we summarize the key insights gathered and provide some observations. We will explicitly focus the discussion on the aspects most relevant for the downstream task in WP6, namely the vertical matchmaking engine – which will exploit the benchmarking data to reach its goals. In addition, we also provide some key insights about the alignment with external benchmark frameworks and data from the AI-on-demand platform.







3. Benchmark results (by StairwAI)

3.1. Benchmark Software Framework (LPDNN)

3.1.1. Scope

LPDNN is the deployment framework that was introduced in D6.1 as the benchmark software framework developed during T6.1. The LPDNN framework was initially developed during the H2020 Bonseyes project (Prado, Miguel De, et al) and has been largely extended in StarwAI to provide a structured benchmarking layer to analyze the execution of AI Applications on a variety of heterogeneous platforms. This layer, on top of LPDNN, adheres to the following design principles:

* Industry-driven Research:

> Input requirements from SMEs wanting to use AI solutions.

> Able to deploy AI solutions on edge devices (low-power, low-carbon footprint).

*  Structured benchmarking workflow:

> Availability of anchors within the deployment framework to evaluate the metrics truthfully.

> Optimised deployment (value added to research and industry).

> Extensive documentation & Support (user friendly for SMEs).

> Defined interfaces (standarization).

> Easy to replicate (reproducibility).

> Create trust & community.

LPDNN provides the tools and capabilities to generate portable and efficient AI applications, which can be deployed and optimised across heterogeneous platforms, e.g., CPU, GPU, FPGA, NPU (ASIC). LPDNN features a full development flow for AI solutions on hardware devices by providing platform support, sample models, optimisation tools, integration of external libraries, and benchmarking. 

LPDNN’s full development flow makes the AI solution reliable and easy to replicate, increasing the portability and fairness across the wide span of hardware platforms. LPDNN’s flexible architecture allows the main core to remain small and dependency-free. At the same time, additional 3rd party libraries or inference engines are only included when needed and for specific platforms, notably increasing the portability across systems. Thereby, different systems can be benchmarked with the same benchmarking code and fairly compared.

The collected dataset using LPDNN across heterogenous platforms has been open sourced at: https://gitlab.com/bonseyes/bonseyes-benchmarks/-/tree/master.

Almost 3000 implementations have been benchmarked across 8 different HW platforms, allowing the first version of the Vertical Matchmaking engine to be trained (T6.3, to be reported in D6.5 in M30).

For more details about LPDNN, please refer to D6.1.

3.1.2. Metrics

LPDNN provides a first analysis of static metrics during the offline compilation of the AI Applications. The offline metrics are the following:

* AI_TASK: Task or class that the AI Model does, e.g., image classification. 

* MODEL_VERSION: Specific version of the AI Model, e.g., different backbone. 

* INPUT_TILE: Input size that the neural network takes. 

* PLATFORM: HW platform where the AI Model is deployed. 

* ENGINE: Inference engine that executes the AI Model. 

* PROCESSOR: Processor where the AI Model is executed. 

* PRECISION: Precision, e.g., floating point, of the data. 

* GFLOP: Giga Floating Point OPerations that the AI Model’s inference contains 

* #PARAMS: Number of parameters (weights) that the AI Model contains. 

* STORAGE: Size on disk of #PARAMS. 

During the execution of the AI Applications, LPDNN measures the following metrics:

* QUALITY_METRIC: Quality metric, e.g., accuracy, mean square error. 

* QUALITY_VALUE: Value of the quality metric. 

* PREPROCESSING_TIME: Time spent on pre-processing the data. 

* INFERENCE_TIME: Time spent on the forward pass of the neural network (NN). 

* POSTPROCESSING_TIME: Time spent on post-processing the results of the NN. 

* LATENCY: Addition of pre/post-processing+ inference times. 

* THROUGHPUT: Number of executions per second. Inverse of latency. 

* DMIPS: Dhrystone Million Instructions per Second. 

* CPU_MEM: Average CPU memory allocated over the execution. 

* CPU_MEM_PEAK: Peak CPU memory allocated over the execution. 

* GPU_MEM: Average CPU memory allocated over the execution. 

* GPU_MEM_PEAK: Peak CPU memory allocated over the execution. 

* MEMORY_BANDWIDTH: Average Memory bandwidth taken. 

* MEMORY_BANDWIDTH_PEAK: Peak Memory bandwidth taken. 

* CPU_LOAD: Average load of the CPU over the execution. 

* GPU_LOAD: Average load of the GPU over the execution. 

* NPU_LOAD: Average load of the NPU over the execution. 

* CPU_TEMP: Average temperature of the CPU over the execution. 

* GPU_TEMP: Average temperature of the GPU over the execution. 

* POWER_CONSUMPTION: Power consumption over the execution. 

* ENERGY_EFFICIENCY: Energy efficiency over the execution.

For more detail about how these metrics are collected, please refer to D6.1.

Among these metrics, it is worth noting the different supported platforms, inference engines and AI models:

Platforms:

* Raspberry Pi 3b+: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz

* Raspberry Pi 4b: Quad-core ARM Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz

* Nvidia Jetson Nano: Quad-core ARM Cortex -A57 @ 1.43 GHz & 128-core Nvidia Maxwell GPU

* Nvidia Jetson Xavier: Octa-core ARM v8.2 @ 2.03 GHz & 512-core Nvidia Volta GPU with Tensor Cores

* Intel NUC: Intel quad-core (TM) i5-7260U CPU @ 3.4 GHz

* iMX8m Nano: Quad-core ARM Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz

* STM32 MP1: Dual-core ARM Cortex-A7 cores up to @ 800 MHz

* HPC: General x86 Intel or AMD CPU cores and Nvidia GPUs



Inference engines:

* LNE: LPDNN Native Engine (LNE) allows the execution of DNNs across arm-based and x86 CPUs as well as on Nvidia-based GPUs.

* NCNN: NCNN ports the execution of DNNs on GPU through the Vulkan driver.

* TensorRT: TensorRT accelerates the DNN inference on Nvidia-based GPUs and NPUs.

* ONNXruntime: ONNXruntime enables the direct execution of ONNX models on CPUs and GPUs.

AI Models:

* face-landmarks-detection-3ddfa

* body-pose-openpifpaf

* face-landmarks-detection-retinaface

* face-recognition-insightface-mobilefacenet

* incar-object-detection-nanodet

* age-gender-insightface

* emotion-classic-light-112x96

* eyegaze-rankgaze

* headpose-rankpose

* imagenet-alexnet

* imagenet-googlenet

* imagenet-efficientb0

* imagenet-squeezenet

* imagenet-mobilenet

* imagenet-mobilenet-v2

* imagenet-mobilenet-v3

* imagenet-resnet-resnet18

* imagenet-resnet-resnet32

* imagenet-resnet-resnet50

* coco-nanodet

3.1.3. Pros/Cons

PROS

* Fair comparison across SW/HW vendors:

o Integration of SW inference engines from several vendors on the same codebase

o Execution on heterogeneous systems (CPU, GPU, NPU)

* Variety on Model deployment: 

o Multiple flavours of models and backbones with several input sizes

o Deployment with different inference engines and data types (fp32, fp16, int8) 

* Real Industrial benchmark 

o Measures real latency of AI App with pre- & post-processing s

o Monitors system during benchmarking

o Extensive number of metrics

CONS

* Mostly focused on deep learning model for computer vision 

* Relatively small amount of data

3.2. Benchmark online algorithms (Energy domain)

3.2.1. Scope

Online algorithms configuration for energy systems is a complex domain area characterized by uncertainty, e.g., renewable energy production, demand fluctuation, tight Hardware (HW) and real-time constraints. An online algorithm should be able to calculate the amount of energy that must be produced by the energy system to meet the required load, minimizing the total energy cost over the daily time horizon and by taking into account the uncertainty. This is a typical real-world problem with real time constraints.

Selecting the optimal HW configuration for a given set of tight constraints on solution time and quality over multiple and diverse data instances (e.g., the specific details of the energy system) for a given online algorithm is a complex problem. Moreover, each algorithm is characterized by a configurable parameter that further complicates this task.

The collected dataset using online algorithms has been open sourced at https://zenodo.org/record/5838437

For more details about the dataset, please refer to https://doi.org/10.1016/j.knosys.2022.109199

3.2.2. Metrics

The benchmark describes the behaviour of two different online algorithms with different parameter configurations and with different input instances (e.g., the specific details of the energy system and renewable energy production and demand fluctuation).   In more details, both algorithms are characterized by a single changeable parameter that impacts the measured metrics: algorithm runtime, memory consumption and solution quality.

For each record of the dataset, we collected the following information: 

* nParameter: an integer that represents the number of scenario/traces used by the algorithm (the configurable parameter). 

* Load (kW): a vector of 96 values representing the load observations sampled in 96 stages (every 15 minutes over the course of a day). 

* RES (kW): a vector of 96 values representing the observations of available renewable production. 



During the execution of the different algorithm configurations, we measure the following metrics:

* solution quality (k\euro): a real number representing the value of the solution, in practice a measure of the daily total energy cost. It is obtained as the sum over the entire time horizon (over 96 stages) of all partial solutions. 

* runtime (sec): the time required for finding the solution with the algorithm. It is obtained as sum over the entire time horizon of all 96 partial running times. 

* memory consumption (MB): represents the RAM used by the algorithm on the machine where it was performed. It is an average of the memory used by the algorithm in 96 partial runs.

The dataset is generated by executing both the algorithms on different problem instances and with different parameter configurations. 

Platform:

All the runs for dataset creation have been performed on Intel Core i5 (3,1 GHz) machines with 16 GB of RAM.

3.2.3. Pros/Cons

CONS

The limitation of this dataset is mainly that we focused on a single HW architecture for analyzing the performance of both the algorithms with different parameter’s configurations.

PROS

This dataset represents the first prototype to test the vertical MatchMaking (MM) engine: 

* exhaustive grid-search exploration of the hyperparameter space 

* exhaustive validation of the vertical MM engine on different configurations.



4. Survey of external benchmarks

4.1. AI Benchmark

4.1.1. Scope

The AI Benchmark is an Android application designed to check the performance and the memory limitations associated with running AI and deep learning algorithms on mobile platforms. It consists of several computer vision tasks performed by neural networks that are running directly on Android devices. The considered networks represent the most popular and commonly used architectures that can be currently deployed on smartphones.

Besides the Android version, a separate open-source AI Benchmark build for desktops was released in June 2019. It is targeted at evaluating AI performance of the common hardware platforms, including CPUs, GPUs and TPUs, and measures the inference and training speed for several key deep learning models. The benchmark is relying on the TensorFlow machine learning library and is distributed as a Python pip package that can be installed on any system running Windows, Linux or macOS.

Reference Paper: https://arxiv.org/pdf/1810.01109.pdf, https://arxiv.org/pdf/1910.06663.pdf

Website: https://ai-benchmark.com/index.html

4.1.2. Metrics

   Supported Mobile Architectures:

* Qualcomm: Snapdragon 845 (Hex. 685 + Adreno 630); Snapdragon 710 (Hexagon 685); Snapdragon 670 (Hexagon 685); Snapdragon 855+ (Hex. 690 + Adreno 640); Snapdragon 855 (Hex. 690 + Adreno 640); Snapdragon 730 (Hex. 688 + Adreno 618); Snapdragon 675 (Hex. 685 + Adreno 612); Snapdragon 665 (Hex. 686 + Adreno 610).

* HiSilicon: Kirin 970 (NPU, Cambricon); Kirin 980 (NPU×2, Cambricon).

* Samsung: Exynos 9810 (Mali-G72 MP18); Exynos 9610 (Mali-G72 MP3); Exynos 9609 (Mali-G72 MP3); Exynos 9825 (NPU + Mali-G76 MP12); Exynos 9820 (NPU + Mali-G76 MP12).

* MediaTek: Helio P70 (APU 1.0 + Mali-G72 MP3); Helio P60 (APU 1.0 + Mali-G72 MP3); Helio P65 (Mali-G52 MP2; Helio P90 (APU 2.0); Helio G90 (APU 1.0 + Mali-G76 MP4).

Frameworks:

* TensorFlow Mobile

* TensorFlow Lite

* Caffe2

Deep Learning Tests:

* Test Section 1: Image Classification, Model: MobileNet-V2, Inference modes: CPU (FP16/32) and NNAPI (INT8 + FP16), Image resolution: 224×224 px, Test time limit: 20 seconds.

* Test Section 2: Image Classification, Model: Inception-V3, Inference modes: CPU (FP16/32) and NNAPI (INT8 + FP16), Image resolution: 346X346 px, Test time limit: 30 seconds.

* Test Section 3: Face Recognition, Model: Inception-ResNet-V1, Inference modes: CPU (INT8) and NNAPI (INT8 + FP16), Image resolution: 512X512 px, Test time limit: 30 seconds.

* Test Section 4: Playing Atari, Model: LSTM, Inference modes: CPU (FP16/32), Image resolution: 84X84 px, Test time limit: 20 seconds.

* Test Section 5: Image Deblurring, Model: SRCNN 9-5-5, Inference modes: NNAPI (INT8 + FP16), Image resolution: 384X384 px, Test time limit: 30 seconds.

* Test Section 6: Image Super-Resolution, Model: VGG-19 (VDSR), Inference modes: NNAPI (INT8 + FP16), Image resolution: 256X256 px, Test time limit: 30 seconds.

* Test Section 7: Image Super-Resolution, Model: SRGAN, Inference modes: CPU (INT8 + FP16/32), Image resolution: 512X512 px, Test time limit: 40 seconds.

* Test Section 8: Bokeh Simulation, Model: U-Net, Inference modes: CPU (FP16/32), Image resolution: 128X128 px, Test time limit: 20 seconds.

* Test Section 9: Image Segmentation, Model: ICNet, Inference modes: NNAPI (2 X FP32 models in parallel), Image resolution: 768X1152 px, Test time limit: 20 seconds.

* Test Section 10: Image Enhancement, Model: DPED-ResNet, Inference modes: NNAPI (FP16 + FP32), Image resolution: 128X192 px, Test time limit: 20 seconds.

* Test Section 11: Memory Test, Model: SRCNN 9-5-5, Inference modes: NNAPI (FP16), Image resolution: from 200X200 px to 2000X2000 px.

   Desktop GPU and CPUs architectures:

> Tesla V100 SXM2 32Gb

> Tesla V100 PCIE 32Gb

> NVIDIA Quadro GV100

> NVIDIA Quadro RTX 8000

> GeForce RTX 2070 SUPER

> NVIDIA TITAN Xp CE

> AMD Radeon VII

> GeForce RTX 2080 Max-Q

> GeForce RTX 2060 Laptop

> NVIDIA Tesla T4

> Intel Xeon Gold 6148

> Intel Xeon Gold 6248

> AMD EPYC 7451

Desktop GPU and CPUs datasets and tasks

* MobileNet-V2 [classification] 

* Inception-V3 [classification]

* Inception-V4 [classification] 

* Inception-ResNet-V2 [classification] 

* ResNet-V2-50 [classification] 

* ResNet-V2-152 [classification] 

* VGG-16 [classification] 

* SRCNN9-5-5 [image-to-image mapping] 

* VGG-19 [image-to-image mapping]

* ResNet-SRGAN [image-to-image mapping] 

* ResNet-DPED [image-to-image mapping] 

* U-Net [image-to-image mapping] 

* Nvidia-SPADE [image-to-image mapping] 

* ICNet [image segmentation]

Scoring System 

AI Benchmark is measuring the performance of several test categories, including int-8, float-16, float-32, parallel, CPU (int-8 and float-16/32), memory tests, and tests measuring model initialization time.

 The contribution of the test categories is as follows: 

* 48%-float-16 tests; 

* 24%-int-8 tests; 

* 12%-CPU,float-16/32 tests; 

* 6%-CPU,int-8 tests; 

* 4%-float-32 tests; 

* 3%-parallel execution of the models; 

* 2%-initialization time, float models; 

* 1%-initialization time, quantized models; 

The scores of each category are computed as a geometric mean of the test results belonging to this category. The computed L1 error is used to penalize the runtime of the corresponding networks running with NNAPI (an exponential penalty with exponent 1.5 is applied).

4.1.3. Pros/Cons

   PROS

• Heavy focus on computer vision

• Multiple ML architecture:

- MobileNet, Inception

• Multiple precision for the same ML architecture:

- FP16, FP8

• State of Art Platforms/hardware:

- AMD processors, Nvidia accelerators, etc

• Industry standard and unbiased evaluations:

- Recognised by major companies and universities.

• Regularly updated with new platforms

   CONS

• Mobile Oriented: CPU and GPU results are not easy to compare with Mobile Inference. Still under development

• Relies on TensorFlow Lite: the number of critical bugs and issues introduced in its new versions prevents from recommending it for any commercial projects or projects dealing with non-standard AI models

4.2. MLPerf

4.2.1. Scope

MLPerf aims to create a representative benchmark suite for ML that evaluates system performance to meet five high-level goals: 

1. Enable fair comparison of competing systems while still encouraging ML innovation. 

2. Accelerate ML progress through fair and useful measurement. 

3. Enforce reproducibility to ensure reliable results. 

4. Serve both the commercial and research communities. 

5. Keep benchmarking effort affordable so all can participate.

   Reference Paper: https://arxiv.org/abs/1911.02549

   Website: https://mlcommons.org/en/, section “Benchmarks”, subsections “Training” and “Inference”

   

   

   

   

   Training

MLPerf Training does the following: 

1. Establish a comprehensive benchmark suite that covers diverse applications, DNN models, and optimizers. 

2. Create reference implementations of each benchmark to precisely define models and training procedures. 

3. Establish rules that ensure submissions are equivalent to these reference implementations and use equivalent hyperparameters. 

4. Establish timing rules to minimize the effects of stochasticity when comparing results.

5. Make submission code open source so that the ML and systems communities can study and replicate the results.

6. Form working groups to keep the benchmark suite up to date.

	ML areas, including vision, language, recommendation, and reinforcement learning set of seven benchmarks.

   Inference

ML inference systems range from deeply embedded devices to smartphones to data centers. They have a variety of real-world applications and many figures of merit, each requiring multiple performance metrics. The right metrics, reflecting production use cases, allow not just MLPerf but also publications to show how a practical ML system would perform. MLPerf Inference consists of four evaluation scenarios: single-stream, multistream, server, and offline.

These scenarios represent many critical inference applications. MLPerf Inference provides a way to simulate the realistic behavior of the inference system under test.

4.2.2. Metrics

   Training

Area

Benchmark

Dataset

Quality Threshold

Model

Vision

Image classification

ImageNet

75.90% classification

ResNet-50 v1.5

Vision

Image segmentation (medical)

KiTS19

0.908 Mean DICE score

3D U-Net

Vision

Object detection (light weight)

Open Images

34.0% mAP

RetinaNet

Vision

Object detection (heavy weight)

COCO

0.377 Box min AP and 0.339 Mask min AP

Mask R-CNN

Language

Speech recognition

LibriSpeech

0.058 Word Error Rate

RNN-T

Language

NLP

Wikipedia 2020/01/01

0.72 Mask-LM accuracy

BERT-large

Commerce

Recommendation

1TB Click Logs

0.8025 AUC

DLRM

Research

Reinforcement learning

Go

50% win rate vs. checkpoint

Mini Go (based on Alpha Go paper)



Training Supported Hardware

* NC96ads_A100_v4

* ND96amsr_A100_v4_n16

* ND96amsr_A100_v4_n8

* ESCN4A-E11

* ESC8000A-E11-8xA100-PCIE-80GB-NVBridge        

* 8_node_64_A100_PaddlePaddle        

* R750xax4A100-PCIE-80GB        

* PRIMERGY-RX2540M6-mxnet        

* 8xR750xax4A100-PCIE-80GB        

* G492-ZD2        

* HPE-ProLiant-XL675d-Gen10-Plus_A100-SXM-80GB_hugectr        

* HLS-Gaudi2-PT        

* Dell Precision 7920 Tower with 2x A5000 using MxNet 22.04        

* Lenovo ThinkSystem SR670 V2 Server with 4x 40GB SXM4 A100        

* Lenovo ThinkSystem SR670 V2 Server with 8x 80GB PCIe A100        

* dgxa100_ngc22.04_merlin_hugectr        

* AS-4124GS-TNR        

* G5500V6x8xA30

* 1-node-SPR-pytorch        

* 16-nodes-SPR-pytorch        

* dgxh100_n4_preview        

* NF5468M6J



Training HPC

Area

Benchmark

Dataset

Quality Threshold

Model

Scientific

Climate segmentation

CAM5+TECA simulation

IOU 0.82

DeepCAM

Scientific

Cosmology parameter prediction

CosmoFlow N-body simulation

Mean average error 0.124

CosmoFlow

Scientific

Quantum molecular modeling

Open Catalyst 2020 (OC20)

Forces mean absolute error 0.036

DimeNet++

Time-to-Train Performance Metric

To address the ML-benchmarking challenges of system optimization and scale, MLPerf performance metric is the time to train to a defined quality target. 

It incorporates both system speed and accuracy and is most relevant to ML practitioners. As an end-to end metric, it also captures the auxiliary operations necessary for training such models, including data-pipeline and accuracy calculations. The metric’s generality enables application to reinforcement learning, unsupervised learning, generative adversarial networks, and other training schemes.

Each benchmark measures the wall clock time required to train a model on the specified dataset to achieve the specified quality target. To account for the substantial variance in ML training times, final results are obtained by measuring the benchmark a benchmark-specific number of times, discarding the lowest and highest results, and averaging the remaining results. Even the multiple result average is not sufficient to eliminate all variance. Imaging benchmark results are very roughly +/- 2.5% and other benchmarks are very roughly +/- 5%.

For non-HPC training, results that converged in fewer epochs than the reference implementation run with the same hyperparameters were normalized to the expected number of epochs.

   Inference

MLPerf defines model-quality targets. We established per-model and scenario targets for inference latency and model quality. The latency bounds and target qualities are based on input gathered from ML-system end users and ML practitioners. As MLPerf improves these parameters in accordance with industry needs, the broader research community can track them to stay relevant.

   Inference Datacenter

Area

Benchmark

Dataset

Quality Threshold

Model

Vision

Image classification

ImageNet (224x224)

99% of FP32 (76.46%)

Resnet50-v1.5

Vision

Object detection

OpenImages (800x800)

99% of FP32 (0.20 mAP)

Retinanet

Vision

Medical image segmentation

KITS 2019 (602x512x512)

99% of FP32 and 99.9% of FP32 (0.86330 mean DICE score)

3D UNET

Speech

Speech-to-text

Librispeech dev-clean (samples < 15 seconds)

99% of FP32 (1 - WER, where WER=7.452253714852645%)

RNNT

Language

Language processing

SQuAD v1.1 (max_seq_len=384)

99% of FP32 and 99.9% of FP32 (f1_score=90.874%)

BERT-large

Commerce

Recommendation

1TB Click Logs

99% of FP32 and 99.9% of FP32 (AUC=80.25%)

DLRM

Metrics:

A. queries/s

B. samples/s

C. Accuracy

D. System Power (W)

   Inference Mobile

Area

Benchmark

Dataset

Quality Threshold

Model

Vision

Image classification

ImageNet

98% of FP32 (Top1: 76.19%)

MobileNetEdgeTPU

Vision

Object detection

MS-COCO 2017

95% of FP32 (mAp: 0.285)

MobileDETs

Vision

Segmentation

ADE20K (32 classes, 512x512)

97% of FP32 (32-class mIOU: 54.8)

DeepLabV3+ (MobileNetV2)

Vision

Segmentation, MOSAIC

ADE20K (32 classes, 512x512)

96% of FP32 (32-class mIOU: 59.8)

MOSAIC

Language

Language processing

SQUAD 1.1

93% of FP32 (F1 score: 90.5)

Mobile-BERT



Metrics:

A. frames/s

B. latency in ms



4.2.3. Pros/Cons

PROS

The framework offers different sorts of benchmarks for comparison/innovation:

* Close: Same original model on HW or SW framework 

* Open: - Allow to innovate with networks and frameworks 

* Power: - Allows to obtain system power measurements 

* Scenarios: - Different streams: single, multi, server, offline 

* Platforms: - Many HW platforms and SW frameworks

CONS

* Only few tasks 

* Only few models

* No memory usage 

* No system usage

* No pre- or post-processing

4.3. AIBench

4.3.1. Scope

AIBench provides a scalable and comprehensive data-center AI benchmark suite. In total, it includes 12 micro benchmarks, 16 component benchmarks, covering 16 AI problem domains: image classification, image generation, text-to-text translation, image-to-text, image-to-image, speech-to-text, face embedding, 3D face recognition, object detection, video prediction, image compression, recommendation, 3D object reconstruction, text summarization, spatial transformer, learning to rank, and two end-to-end application. AI benchmarks: 

- DCMix —a datacenter AI application combination mixed with AI workloads 

- E-commerce AI—an end-to-end business AI benchmark.

Reference Paper: https://arxiv.org/pdf/2004.14690, http://www.benchcouncil.org/aibench/file/AIBench-Bench18.pdf, https://www.benchcouncil.org/file/Cluster_2021_hpcai_camera.pdf 

Website: https://www.benchcouncil.org/aibench/index.html, section “AIBench Training” and “AIBench Inference”

4.3.2. Metrics

AI Tasks

• Image generation uses WGAN algorithms and uses LSUN dataset as data input to generate image data. 

• Text-to-Text Translation uses recurrent neural networks and takes WMTEnglish-German as data input to translate text data. 

• Image-to-Text uses Neural Image Caption model and takes Microsoft COCO dataset as input to describe image using text. 

• Image-to-Image uses the cycleGAN algorithm and takes Cityscapes dataset as input to transform the image to another image. 

• Speech-to-Text uses the DeepSpeech2 algorithm and takes Librispeech dataset as input to recognize the speech data. 

• Face embedding uses the FaceNet algorithm and takes the LFW (Labeled Faces in the Wild) dataset or VGGFace2 as input to convert image to an embedding vector. 

• 3Dface recognition uses 3D face modes to recognize 3D information within images. The input data includes 77,715 samples from 253 face IDs, which is published on the BenchCouncil web site. 

• Object detection uses the Faster R-CNN algorithm and takes Microsoft COCO dataset as input to detect objects in images. 

• Recommendation uses collaborative filtering algorithm and takes MovieLens dataset as input to provide recommendations. 

• Video prediction uses motion-focused predictive models and takes Robot pushing dataset as input to predict video frames. 

• Image compression uses recurrent neural networks and takes ImageNet dataset as input to compression images.

• 3D object reconstruction uses a convolutional encoder-decoder network and takes ShapeNet Dataset as input to reconstruct 3D object. 

• Text summarization uses sequence-to-sequence model and takes Gigaword dataset as input to generate summary description for text. 

• Spatial transformer uses spatial transformer networks and takes MNIST dataset as input to make spatial transformations. 

• Learning to Rank uses ranking distillation algorithm and uses Gowalla dataset to generate ranking scores.

Metrics

- Wall Clock Time

- Energy consumption of running a benchmark

- Accuracy

Provides both training and inference benchmarks. 

The training metrics are the wall clock time to train the specific epochs, the wall clock time to train a model achieving a target accuracy, and the energy consumption to train a model achieving a target accuracy. 

The inference metrics are the wall clock time, accuracy, and energy consumption. 

Additionally, the performance numbers are reported on the BenchCouncil web site (http://www.benchcouncil.org/numbers.html), to measure the training and inference speeds of different hardware platforms, including multiple types of NIVDIA GPUs, Intel CPUs, AI accelerator chips, and to measure the performance of different software stacks, including TensorFlow, PyTorch, and etc.

4.3.3. Pros/Cons

PROS

- Wide variety of data types and data sources are covered, including text, images, street scenes, audios, videos, etc. 

- Not only based on mainstream deep learning frameworks like TensorFlow and PyTorch, but also based on traditional programming model like Pthreads.

- Provides Training, Inference, Micro and Synthetics Benchmarks across Datacenter, HPC, IoT, and Edge.

CONS

- Last update in 2021

- Lacking benchmark results on latest Mobile and CPU/GPUs architectures











4.4. OpenML CC-18

4.4.1. Scope

Seamlessly integrated into the OpenML platform, this benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier:

* All datasets are uniformly formatted in standardized data formats.

* They can be easily downloaded programmatically through APIs and client libraries.

* They come with machine-readable meta-information, such as the occurrence of missing values, to train algorithms correctly.

* Standardized train-test splits are provided to ensure that results can be objectively compared.

* Results can be shared in a reproducible way through the APIs.

* Results from other users can be easily downloaded and reused.



Reference Paper: https://arxiv.org/pdf/1708.03731.pdf 

Website: https://www.openml.org/search?type=benchmark&sort=tasks_included&study_type=task,  Benchmarking Doc: https://docs.openml.org/benchmark/ 

4.4.2. Metrics

The OpenML-CC18 contains all verified and publicly licenced OpenML datasets until mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking: 

* The number of observations is between 500 and 100000 to focus on medium-sized datasets that can be used to train models on almost any computing hardware. 

* The dataset has less than 5000 features, counted after one-hot-encoding categorical features (which is the most frequent way to deal with categorical variables), to avoid most memory issues.

* The target attribute has at least two classes, with no class of less than 20 observations. This ensures sufficient samples per class per fold when running 10-fold cross-validation experiments. 

* The ratio of the minority and majority class is above 0.05 (to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures). 

* The dataset is not sparse because not all machine learning models can handle them gracefully, this constraint facilitates our goal of wide applicability. 

* The dataset does not require taking time dependency between samples into account, e.g., time series or data streams, as this is often not implemented in standard machine learning libraries. Removed datasets where each sample constitutes a single data stream. 

* The dataset does not require grouped sampling. Such datasets would contain multiple data points for one subject and require that all data points for a subject are put into the same data split for evaluation.







4.4.3. Pros/Cons

PROS

• Easy creation of benchmarks 

• Permanence and provenance: Because benchmarking suites are its own entity on OpenML, it is clear who created them (provenance). 

• Community of practice: Curated benchmark suites allow scientists to thoroughly benchmark their machine learning methods without having to worry about finding and selecting datasets for their benchmarks. 

• Building on existing suites: Scientists can extend, subset, or adapt existing benchmarking suites to correct issues, raise the bar, or run personalized benchmarks.

• Reproducibility of benchmarks: Based on machine-readable OpenML tasks, with detailed instructions for evaluation procedures and train-test splits, shared results are comparable and reproducible. 

CONS

* Overfitting: overfitting on fixed suites is increasingly likely. 

* Computational issues: focused on mid-size datasets, some larger ones still incurred too high computational load, so some researchers have used subsets of the OpenML-CC18 in their work

* Breadth of current benchmarking suites: researchers are interested in benchmarking larger (deep learning) models on larger datasets from many domains (including language and vision). 

* Specification of resource constraints: the task and suite specifications do not yet allow for constraints on resources, e.g., memory or time limits. 

4.5. DataPerf

4.5.1. Scope

DataPerf has the following goals:

* Focus research and development on improving ML dataset quality.

* Improve ML training datasets to increase accuracy and/or reduce data required to train.

* Improve ML test datasets to drive ML solution fidelity and reliability.

* Motivate datasets that increase representation and decrease bias.

* Drive development of better techniques and tools for creating and optimizing datasets.

* Provide consistent metrics for researchers and commercial developers.

* Enforce replicability to ensure reliable results.

* Keep benchmarking effort affordable so all can participate.



Reference Paper: https://arxiv.org/pdf/2207.10062 

Website: https://dataperf.org/ 





4.5.2. Metrics

The DataPerf suite includes the benchmark types listed below. Each benchmark type uses a different metric, though all in principle either maximize the efficacy of a training set or the breadth/difficulty of a test set.

* Training dataset: create a novel training dataset that maximizes the accuracy of a standard set of models trained on it.

* Test dataset: identify novel test data which is incorrectly labeled by the maximum percentage of a standard set of trained models yet is correctly labeled by humans.

* Selection algorithm: select a subset of a larger dataset for use as a training set to maximize the accuracy of a standard set of models trained on it.

* Debugging algorithm: identify mislabeled or unlabeled data that, when corrected, maximizes the accuracy of a standard set of models trained on it.

* Slicing algorithm: identify semantically consistent slices of a dataset for which a trained model underperforms and maximize top-K precision of such identification vs. ideal choices.

* Valuation: estimate the increase in accuracy from supplementing a known training dataset with new data, presently unlabeled, and minimize the error with respect to the actual value.

4.5.3. Pros/Cons

PROS

* Data parsing 

* Data augmentation 

* Representation selection 

* Data quality assessment 

* Data acquisition 

* Data cleaning

CONS

* Under Development, not publicly available



5. Discussion and alignment with AI-on-demand platform

5.1. Benchmarks discussion

Multiple insights could be gained from the survey of available datasets and benchmarks presented in the previous sections. In particular, we will consider the relation of the benchmarks with the final goal of the WP6, which is to build a vertical matchmaking component capable of matching available hardware resources and AI algorithms while respecting user-specified constraints. One of the critical components of the vertical engine is a Machine Learning model, which learns the relation between the behaviour of the algorithm (for instance, represented as time-to-solution, runtime, memory consumption, power consumption, and other similar metrics) and the HW platform used to run the algorithm.

From the point of view of the vertical matchmaking engine, most of the value of the benchmarking data lies in the possibility of characterizing the behaviour of AI algorithms running across different hardware platforms and under different configurations. The main key to be drawn is to understand whether the deployment of the algorithm can be maximized by being deployed on a specific platform or configuration. The vertical matchmaker will then use this information to support the user in selecting the optimal hardware resources for a given task, i.e., a specific algorithm, and possibly to find the optimal hyperparameters’ configuration.

The benchmarks provided by StairwAI, e.g., LPDNN's dataset, have into consideration the support of multiple platforms and configurations, e.g., inference engine, model version, model size, data type, etc., addressing those vertical matchmaking engine’s requirements explained above. Thus, the first version of vertical matchmaking has already been trained on this dataset, which will be further described in D6.5.

However, this information is not always explicitly reflected in the external benchmarking datasets, which are not always created to characterize the algorithms’ behaviour over many different HW platforms, i.e., they tend to collect information about runs executed on a single device. This does not necessarily make these data worthless, as helpful information can be extracted, especially for transfer learning purposes.

It can also be noticed that there is no general unified format employed to collect data and perform the benchmarks. This lack of a standard hinders the development of a vertical matchmaking engine over different domains, as an ad-hoc technique to process the benchmarking data would need to be implemented for each case. A standard and unified format could be highly beneficial in this regard – it could be as simple as deciding a common format by which the collected data should be organized. For instance, the first step could consist in creating a companion meta-data descriptor containing the information about the benchmarking data and easily readable by a machine for automated processing.

Another observation that can be made is the fact that existing benchmarking data sets cover a wide range of different target metrics, that is, the metrics used to characterize the algorithms' behaviour and measured during the execution on a specific HW platform. On the one hand, the variety of target metrics is a boon as it allows for studying an algorithm's behaviour under different aspects. On the other hand, this might limit the possibility of comparing different algorithms and/or different HW devices, as the comparison is difficult when the benchmarks measure different things. Further, fairness can be at stake if the benchmarking process across the various frameworks is not performed homogeneously.

In conclusion, we have generated benchmark datasets using StairwAI’s benchmark software, which addresses the heterogeneity of HW devices and have been validated by training the first version of the vertical matchmaking engine. Besides, we have identified existing external benchmark datasets which should be explored more in detail. Further efforts will be put into understanding whether a common format for the external benchmarks dataset can make them useful to extend StairwAI’s own datasets and further extend the vertical matchmaking engine’s capabilities. If this is the case, other points, such as fairness across the data, should be addressed and validated.

5.2. Alignment with AI-on-demand platform

During the course of this tasks, StairwAI has also participated in an AI4Europe workshop about benchmarking. The main goal of this meeting was to align the efforts for benchmarking across multiple ICT49 projects and the AI-on-demand platform. 

During the workshop, multiple benchmarking frameworks and benchmark results were presented. The alignment of benchmark frameworks seemed to be less important as they tend to focus on different AI problems, e.g., training or inference, cloud or edge. Nonetheless, and as concluded in Section 5.1, it happens to be fairly important to align the benchmark results on some established standard format to have a clear common understanding and smooth transition across different AI Tasks. 

More benchmark workshops are planned to happen during the extension of StairwAI project to further work on this direction.

6. Conclusion and future work

This document has presented the first version of the Benchmark results proposed within WP6 Task 6.2 of StairwAI project. The document has described the benchmark results obtained using Stairwai’s benchmark framework from D6.1 across multiple heterogeneous platforms. Besides, we have provided a survey of external benchmarks on the domain of AI to analyse further benefits by combining the two sources of data. 

As future work for the 2nd release of the Benchmark results, we will include the results of benchmarking Huawei’s embedded platform, which will be integrated into LPDNN in T6.1, increasing the capabilities for profiling AI Algorithms on a wide range of platforms.

In addition, we will give more details on the feasibility and results of combining both StairwAI and external benchmarks for the training of the Vertical Matchmaking Engine in T6.3.









Executive Summary



The benchmark results are the outcome of benchmark software framework executed across multiple heterogenous platforms, which are then used to train the vertical matchmaking engine (Task 6.3).

In M25, the 1st version of the Benchmark Results was released within WP6 Task 6.2 of StairwAI project. The 1st version of benchmark results represented one of the main contributions from WP6 to train the 1st version of the vertical matchmaking engine (Task 6.3). 



This document provides the 2nd version of the Benchmark Results, including the integration of Huawei’s benchmarks that led the integration of Huawei’s HW and SW into LPDNN (Benchmark Software Framework) within D6.2. Besides, this deliverable also provides a large step towards achieving MS10 for the integration of the Benchmark Results into the AI-on-demand platform by providing a database that can be accessed through the AI-on-demand API (Section 5).



This document is built incrementally on top of 1st version of the Benchmark Results deliverable, making the document self-contained. We add and highlight the new features and content of the Benchmark Results 2nd that have been added with respect to the 1st. 



Section 2 introduces the deliverable, providing the context and the contributions. 

Section 3 presents the results obtained from the benchmark software framework while Section 4 gives a summary of external benchmarks that are commonly used by researchers to push the limits of the fields. 

In Section 5, we provide an analysis of these benchmarks and compare them against the ones collected within the project. We also propose a series of operational steps to integrate the benchmarking framework with StairwAI platform and with the AI-on-demand platform as well. 

Lastly, Section 6 elaborates the conclusions.

























2. Introduction

As the computational capacity of embedded devices grows, they become more capable of performing sophisticated tasks, including those considere
Stairw_all.txt
Tiek attēlots vienums Stairw_all.txt.
